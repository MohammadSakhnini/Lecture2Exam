
Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.
Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.

History
The history of natural language processing (NLP) generally started in the 1950s, although work can be found from earlier periods.
In 1950, Alan Turing published an article titled "Computing Machinery and Intelligence" which proposed what is now called the Turing test as a criterion of intelligence.
The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.  However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced.  Little further research in machine translation was conducted until the late 1980s when the first statistical machine translation systems were developed.
Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted "blocks worlds" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966.  Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the "patient" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to "My head hurts" with "Why do you say your head hurts?".
During the 1970s, many programmers began to write "conceptual ontologies", which structured real-world information into computer-understandable data.  Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981).  During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.
Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.  This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing. Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules.  However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models.  Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.
Many of the notable early successes occurred in the field of machine translation, due especially to work at IBM Research, where successively more complicated statistical models were developed.  These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government.  However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.
Recent research has increasingly focused on unsupervised and semi-supervised learning algorithms.  Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data.  Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data.  However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results if the algorithm used has a low enough time complexity to be practical.
In the 2010s, representation learning and deep neural network-style machine learning methods became widespread in natural language processing, due in part to a flurry of results showing that such techniques can achieve state-of-the-art results in many natural language tasks, for example in language modeling, parsing, and many others. Popular techniques include the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing). In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing. For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation (SMT).

Rule-based vs. statistical NLP
In the early days, many language-processing systems were designed by hand-coding a set of rules: such as by writing grammars or devising heuristic rules for stemming. 
Since the so-called "statistical revolution" in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora (the plural form of corpus, is a set of documents, possibly with human or computer annotations) of typical real-world examples.
Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks. These algorithms take as input a large set of "features" that are generated from the input data. Some of the earliest-used algorithms, such as decision trees, produced systems of hard if-then rules similar to the systems of handwritten rules that were then common. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature. Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.
Systems based on machine-learning algorithms have many advantages over hand-produced rules:

The learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not at all obvious where the effort should be directed.
Automatic learning procedures can make use of statistical inference algorithms to produce models that are robust to unfamiliar input (e.g. containing words or structures that have not been seen before) and to erroneous input (e.g. with misspelled words or words accidentally omitted). Generally, handling such input gracefully with handwritten rules, or, more generally, creating systems of handwritten rules that make soft decisions, is extremely difficult, error-prone and time-consuming.
Systems based on automatically learning the rules can be made more accurate simply by supplying more input data. However, systems based on handwritten rules can only be made more accurate by increasing the complexity of the rules, which is a much more difficult task. In particular, there is a limit to the complexity of systems based on handcrafted rules, beyond which the systems become more and more unmanageable. However, creating more data to input to machine-learning systems simply requires a corresponding increase in the number of man-hours worked, generally without significant increases in the complexity of the annotation process.

Major evaluations and tasks
The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
Though natural language processing tasks are closely intertwined, they are frequently subdivided into categories for convenience. A coarse division is given below.

Syntax
Grammar induction
Generate a formal grammar that describes a language's syntax.
Lemmatization
The task of removing inflectional endings only and to return the base dictionary form of a word which is also known as a lemma.
Morphological segmentation
Separate words into individual morphemes and identify the class of the morphemes. The difficulty of this task depends greatly on the complexity of the morphology (i.e., the structure of words) of the language being considered. English has fairly simple morphology, especially inflectional morphology, and thus it is often possible to ignore this task entirely and simply model all possible forms of a word (e.g., "open, opens, opened, opening") as separate words. In languages such as Turkish or Meitei, a highly agglutinated Indian language, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms.
Part-of-speech tagging
Given a sentence, determine the part of speech (POS) for each word. Many words, especially common ones, can serve as multiple parts of speech. For example, "book" can be a noun ("the book on the table") or verb ("to book a flight"); "set" can be a noun, verb or adjective; and "out" can be any of at least five different parts of speech. Some languages have more such ambiguity than others. Languages with little inflectional morphology, such as English, are particularly prone to such ambiguity. Chinese is prone to such ambiguity because it is a tonal language during verbalization. Such inflection is not readily conveyed via the entities employed within the orthography to convey the intended meaning.
Parsing
Determine the parse tree (grammatical analysis) of a given sentence. The grammar for natural languages is ambiguous and typical sentences have multiple possible analyses: perhaps surprisingly, for a typical sentence there may be thousands of potential parses (most of which will seem completely nonsensical to a human). There are two primary types of parsing: dependency parsing and constituency parsing. Dependency parsing focuses on the relationships between words in a sentence (marking things like primary objects and predicates), whereas constituency parsing focuses on building out the parse tree using a probabilistic context-free grammar (PCFG) (see also stochastic grammar).
Sentence breaking (also known as "sentence boundary disambiguation")
Given a chunk of text, find the sentence boundaries. Sentence boundaries are often marked by periods or other punctuation marks, but these same characters can serve other purposes (e.g., marking abbreviations).
Stemming
The process of reducing inflected (or sometimes derived) words to their root form. (e.g., "close" will be the root for "closed", "closing", "close", "closer" etc.).
Word segmentation
Separate a chunk of continuous text into separate words. For a language like English, this is fairly trivial, since words are usually separated by spaces. However, some written languages like Chinese, Japanese and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language. Sometimes this process is also used in cases like bag of words (BOW) creation in data mining.
Terminology extraction
The goal of terminology extraction is to automatically extract relevant terms from a given corpus.

Semantics
Lexical semantics
What is the computational meaning of individual words in context?
Distributional semantics
How can we learn semantic representations from data?
Machine translation
Automatically translate text from one human language to another.  This is one of the most difficult problems, and is a member of a class of problems colloquially termed "AI-complete", i.e. requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.) to solve properly.
Named entity recognition (NER)
Given a stream of text, determine which items in the text map to proper names, such as people or places, and what the type of each such name is (e.g. person, location, organization). Although capitalization can aid in recognizing named entities in languages such as English, this information cannot aid in determining the type of named entity, and in any case, is often inaccurate or insufficient.  For example, the first letter of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized.  Furthermore, many other languages in non-Western scripts (e.g. Chinese or Arabic) do not have any capitalization at all, and even languages with capitalization may not consistently use it to distinguish names. For example, German capitalizes all nouns, regardless of whether they are names, and French and Spanish do not capitalize names that serve as adjectives.
Natural language generation
Convert information from computer databases or semantic intents into readable human language.
Natural language understanding
Convert chunks of text into more formal representations such as first-order logic structures that are easier for computer programs to manipulate. Natural language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural language expression which usually takes the form of organized notations of natural language concepts. Introduction and creation of language metamodel and ontology are efficient however empirical solutions. An explicit formalization of natural language semantics without confusions with implicit assumptions such as closed-world assumption (CWA) vs. open-world assumption, or subjective Yes/No vs. objective True/False is expected for the construction of a basis of semantics formalization.
Optical character recognition (OCR)
Given an image representing printed text, determine the corresponding text.
Question answering
Given a human-language question, determine its answer.  Typical questions have a specific right answer (such as "What is the capital of Canada?"), but sometimes open-ended questions are also considered (such as "What is the meaning of life?"). Recent works have looked at even more complex questions.
Recognizing Textual entailment
Given two text fragments, determine if one being true entails the other, entails the other's negation, or allows the other to be either true or false.
Relationship extraction
Given a chunk of text, identify the relationships among named entities (e.g. who is married to whom).
Sentiment analysis (see also multimodal sentiment analysis)
Extract subjective information usually from a set of documents, often using online reviews to determine "polarity" about specific objects. It is especially useful for identifying trends of public opinion in social media, for marketing.
Topic segmentation and recognition
Given a chunk of text, separate it into segments each of which is devoted to a topic, and identify the topic of the segment.
Word sense disambiguation
Many words have more than one meaning; we have to select the meaning which makes the most sense in context.  For this problem, we are typically given a list of words and associated word senses, e.g. from a dictionary or an online resource such as WordNet.

Discourse
Automatic summarization
Produce a readable summary of a chunk of text.  Often used to provide summaries of the text of a known type, such as research papers, articles in the financial section of a newspaper.
Coreference resolution
Given a sentence or larger chunk of text, determine which words ("mentions") refer to the same objects ("entities"). Anaphora resolution is a specific example of this task, and is specifically concerned with matching up pronouns with the nouns or names to which they refer. The more general task of coreference resolution also includes identifying so-called "bridging relationships" involving referring expressions. For example, in a sentence such as "He entered John's house through the front door", "the front door" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of John's house (rather than of some other structure that might also be referred to).
Discourse analysis
This rubric includes several related tasks.  One task is identifying the discourse structure of a connected text, i.e. the nature of the discourse relationships between sentences (e.g. elaboration, explanation, contrast).  Another possible task is recognizing and classifying the speech acts in a chunk of text (e.g. yes-no question, content question, statement, assertion, etc.).

Speech
Speech recognition
Given a sound clip of a person or people speaking, determine the textual representation of the speech.  This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed "AI-complete" (see above).  In natural speech there are hardly any pauses between successive words, and thus speech segmentation is a necessary subtask of speech recognition (see below). In most spoken languages, the sounds representing successive letters blend into each other in a process termed coarticulation, so the conversion of the analog signal to discrete characters can be a very difficult process. Also, given that words in the same language are spoken by people with different accents, the speech recognition software must be able to recognize the wide variety of input as being identical to each other in terms of its textual equivalent.
Speech segmentation
Given a sound clip of a person or people speaking, separate it into words.  A subtask of speech recognition and typically grouped with it.
Text-to-speech
Given a text, transform those units and produce a spoken representation. Text-to-speech can be used to aid the visually impaired.

Dialogue
The first published work by an artificial intelligence was published in 2018, 1 the Road, marketed as a novel, contains sixty million words.

See also
References


== Further reading ==
The following outline is provided as an overview of and topical guide to natural language processing:
Natural language processing – computer activity in which computers are entailed to analyze, understand, alter, or generate natural language.  This includes the automation of any or all linguistic forms, activities, or methods of communication, such as conversation, correspondence, reading, written composition, dictation, publishing, translation, lip reading, and so on.  Natural language processing is also the name of the branch of computer science, artificial intelligence, and linguistics concerned with enabling computers to engage in communication using natural language(s) in all forms, including but not limited to speech, print, writing, and signing.

Natural language processing
Natural language processing can be described as all of the following:

A field of science – systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.An applied science – field that applies human knowledge to build or design useful things.
A field of computer science – scientific and practical approach to computation and its applications.
A branch of artificial intelligence –  intelligence of machines and robots and the branch of computer science that aims to create it.
A subfield of computational linguistics –  interdisciplinary field dealing with the statistical or rule-based modeling of natural language from a computational perspective.
An application of engineering – science, skill, and profession of acquiring and applying scientific, economic, social, and practical knowledge, in order to design and also build structures, machines, devices, systems, materials and processes.
An application of software engineering – application of a systematic, disciplined, quantifiable  approach to the design, development, operation, and maintenance of software, and the study of these approaches; that is, the application of engineering to software.A subfield of computer programming – process of designing, writing, testing, debugging, and maintaining the source code of computer programs. This source code is written in one or more programming languages (such as Java, C++, C#, Python, etc.). The purpose of programming is to create a set of instructions that computers use to perform specific operations or to exhibit desired behaviors.
A subfield of artificial intelligence programming –
A type of system – set of interacting or interdependent components forming an integrated whole or a set of elements (often called 'components' ) and relationships which are different from relationships of the set or its elements to other elements or sets.
A system that includes software –  software is a collection of computer programs and related data that provides the instructions for telling a computer what to do and how to do it. Software refers to one or more computer programs and data held in the storage of the computer. In other words, software is a set of programs, procedures, algorithms and its documentation concerned with the operation of a data processing system.
A type of technology – making, modification, usage, and knowledge of tools, machines, techniques, crafts, systems, methods of organization, in order to solve a problem, improve a preexisting solution to a problem, achieve a goal, handle an applied input/output relation or perform a specific function. It can also refer to the collection of such tools, machinery, modifications, arrangements and procedures. Technologies significantly affect human as well as other animal species' ability to control and adapt to their natural environments.
A form of computer technology – computers and their application. NLP makes use of computers, image scanners, microphones, and many types of software programs.
Language technology –  consists of natural language processing (NLP) and computational linguistics (CL) on the one hand, and speech technology on the other. It also includes many application oriented aspects of these. It is often called human language technology (HLT).

Prerequisite technologies
The following technologies make natural language processing possible:

Communication – the activity of a source sending a message to a receiver
Language –
Speech –
Writing –
Computing –
Computers –
Computer programming –
Information extraction –
User interface –
Software –
Text editing – program used to edit plain text files
Word processing – piece of software used for composing, editing, formatting, printing documents
Input devices – pieces of hardware for sending data to a computer to be processedComputer keyboard – typewriter style input device whose input is converted into various data depending on the circumstances
Image scanners –

Subfields of natural language processing
Information extraction (IE) – field concerned in general with the extraction of semantic information from text.  This covers tasks such as named entity recognition, coreference resolution, relationship extraction, etc.
Ontology engineering – field that studies the methods and methodologies for building ontologies, which are formal representations of a set of concepts within a domain and the relationships between those concepts.
Speech processing – field that covers speech recognition, text-to-speech and related tasks.
Statistical natural language processing –
Statistical semantics – a subfield of computational semantics that establishes semantic relations between words to examine their contexts.
Distributional semantics – a subfield of statistical semantics that examines the semantic relationship of words across a corpora or in large samples of data.

Related fields
Natural language processing contributes to, and makes use of (the theories, tools, and methodologies from), the following fields:

Automated reasoning – area of computer science and mathematical logic dedicated to understanding various aspects of reasoning, and producing software which allows computers to reason completely, or nearly completely, automatically. A sub-field of artificial intelligence, automatic reasoning is also grounded in theoretical computer science and philosophy of mind.
Linguistics – scientific study of human language. Natural language processing requires understanding of the structure and application of language, and therefore it draws heavily from linguistics.
Applied linguistics – interdisciplinary field of study that identifies, investigates, and offers solutions to language-related real-life problems. Some of the academic fields related to applied linguistics are education, linguistics, psychology, computer science, anthropology, and sociology. Some of the subfields of applied linguistics relevant to natural language processing are:
Bilingualism / Multilingualism –
Computer-mediated communication (CMC) – any communicative transaction that occurs through the use of two or more networked computers. Research on CMC focuses largely on the social effects of different computer-supported communication technologies. Many recent studies involve Internet-based social networking supported by social software.
Contrastive linguistics – practice-oriented linguistic approach that seeks to describe the differences and similarities between a pair of languages.
Conversation analysis (CA) – approach to the study of social interaction, embracing both verbal and non-verbal conduct, in situations of everyday life. Turn-taking is one aspect of language use that is studied by CA.
Discourse analysis – various approaches to analyzing written, vocal, or sign language use or any significant semiotic event.
Forensic linguistics – application of linguistic knowledge, methods and insights to the forensic context of law, language, crime investigation, trial, and judicial procedure.
Interlinguistics – study of improving communications between people of different first languages with the use of ethnic and auxiliary languages (lingua franca). For instance by use of intentional international auxiliary languages, such as Esperanto or Interlingua, or spontaneous interlanguages known as pidgin languages.
Language assessment – assessment of first, second or other language in the school, college, or university context; assessment of language use in the workplace; and assessment of language in the immigration, citizenship, and asylum contexts. The assessment may include analyses of listening, speaking, reading, writing or cultural understanding, with respect to understanding how the language works theoretically and the ability to use the language practically.
Language pedagogy – science and art of language education, including approaches and methods of language teaching and study. Natural language processing is used in programs designed to teach language, including first and second language training.
Language planning –
Language policy –
Lexicography –
Literacies –
Pragmatics –
Second language acquisition –
Stylistics –
Translation –
Computational linguistics –  interdisciplinary field dealing with the statistical or rule-based modeling of natural language from a computational perspective. The models and tools of computational linguistics are used extensively in the field of natural language processing, and vice versa.
Computational semantics –
Corpus linguistics – study of language as expressed in samples (corpora) of "real world" text.  Corpora is the plural of corpus, and a corpus is a specifically selected collection of texts (or speech segments) composed of natural language. After it is constructed (gathered or composed), a corpus is analyzed with the methods of computational linguistics to infer the meaning and context of its components (words, phrases, and sentences), and the relationships between them.  Optionally, a corpus can be annotated ("tagged") with data (manually or automatically) to make the corpus easier to understand (e.g., part-of-speech tagging).  This data is then applied to make sense of user input, for example, to make better (automated) guesses of what people are talking about or saying, perhaps to achieve more narrowly focused web searches, or for speech recognition.
Metalinguistics –
Sign linguistics – scientific study and analysis of natural sign languages, their features, their structure (phonology, morphology, syntax, and semantics), their acquisition (as a primary or secondary language), how they develop independently of other languages, their application in communication, their relationships to other languages (including spoken languages), and many other aspects.
Human–computer interaction – the intersection of computer science and behavioral sciences, this field involves the study, planning, and design of the interaction between people (users) and computers.  Attention to human-machine interaction is important, because poorly designed human-machine interfaces can lead to many unexpected problems. A classic example of this is the Three Mile Island accident where investigations concluded that the design of the human–machine interface was at least partially responsible for the disaster.
Information retrieval (IR) – field concerned with storing, searching and retrieving information. It is a separate field within computer science (closer to databases), but IR relies on some NLP methods (for example, stemming). Some current research and applications seek to bridge the gap between IR and NLP.
Knowledge representation (KR) – area of artificial intelligence research aimed at representing knowledge in symbols to facilitate inferencing from those knowledge elements, creating new elements of knowledge. Knowledge Representation research involves analysis of how to reason accurately and effectively and how best to use a set of symbols to represent a set of facts within a knowledge domain.
Semantic network – study of semantic relations between concepts.
Semantic Web –
Machine learning – subfield of computer science that examines pattern recognition and computational learning theory in artificial intelligence. There are three broad approaches to machine learning.  Supervised learning occurs when the machine is given example inputs and outputs by a teacher so that it can learn a rule that maps inputs to outputs. Unsupervised learning occurs when the machine determines the inputs structure without being provided example inputs or outputs. Reinforcement learning occurs when a machine must perform a goal without teacher feedback.
Pattern recognition – branch of machine learning that examines how machines recognize regularities in data. As with machine learning, teachers can train machines to recognize patterns by providing them with example inputs and outputs (i.e. Supervised Learning), or the machines can recognize patterns without being trained on any example inputs or outputs (i.e. Unsupervised Learning).
Statistical classification –

Structures used in natural language processing
Anaphora – type of expression whose reference depends upon another referential element. E.g., in the sentence 'Sally preferred the company of herself', 'herself' is an anaphoric expression in that it is coreferential with 'Sally', the sentence's subject.
Context-free language –
Controlled natural language – a natural language with a restriction introduced on its grammar and vocabulary in order to eliminate ambiguity and complexity
Corpus – body of data, optionally tagged (for example, through part-of-speech tagging), providing real world samples for analysis and comparison.
Text corpus – large and structured set of texts, nowadays usually electronically stored and processed. They are used to do statistical analysis and hypothesis testing, checking occurrences or validating linguistic rules within a specific subject (or domain).
Speech corpus – database of speech audio files and text transcriptions. In Speech technology, speech corpora are used, among other things, to create acoustic models (which can then be used with a speech recognition engine). In Linguistics, spoken corpora are used to do research into phonetic, conversation analysis, dialectology and other fields.
Grammar –
Context-free grammar (CFG) –
Constraint grammar (CG) –
Definite clause grammar (DCG) –
Functional unification grammar (FUG) –
Generalized phrase structure grammar (GPSG) –
Head-driven phrase structure grammar (HPSG) –
Lexical functional grammar (LFG) –
Probabilistic context-free grammar (PCFG) – another name for stochastic context-free grammar.
Stochastic context-free grammar (SCFG) –
Systemic functional grammar (SFG) –
Tree-adjoining grammar (TAG) –
Natural language –
n-gram – sequence of n number of tokens, where a "token" is a character, syllable, or word. The n is replaced by a number. Therefore, a 5-gram is an n-gram of 5 letters, syllables, or words. "Eat this" is a 2-gram (also known as a bigram).
Bigram – n-gram of 2 tokens. Every sequence of 2 adjacent elements in a string of tokens is a bigram. Bigrams are used for speech recognition, they can be used to solve cryptograms, and bigram frequency is one approach to statistical language identification.
Trigram –  special case of the n-gram, where n is 3.
Ontology – formal representation of a set of concepts within a domain and the relationships between those concepts.
Taxonomy – practice and science of classification, including the principles underlying classification, and the methods of classifying things or concepts.
Hyponymy and hypernymy – the linguistics of hyponyms and hypernyms.  A hyponym shares a type-of relationship with its hypernym. For example, pigeon, crow, eagle and seagull are all hyponyms of bird (their hypernym); which, in turn, is a hyponym of animal.
Taxonomy for search engines – typically called a "taxonomy of entities". It is a tree in which nodes are labelled with entities which are expected to occur in a web search query. These trees are used to match keywords from a search query with the keywords from relevant answers (or snippets).
Textual entailment – directional relation between text fragments. The relation holds whenever the truth of one text fragment follows from another text. In the TE framework, the entailing and entailed texts are termed text (t) and hypothesis (h), respectively. The relation is directional because even if "t entails h", the reverse "h entails t" is much less certain.
Triphone – sequence of three phonemes. Triphones are useful in models of natural language processing where they are used to establish the various contexts in which a phoneme can occur in a particular natural language.

Processes of NLP
Applications
Automated essay scoring (AES) – the use of specialized computer programs to assign grades to essays written in an educational setting. It is a method of educational assessment and an application of natural language processing. Its objective is to classify a large set of textual entities into a small number of discrete categories, corresponding to the possible grades—for example, the numbers 1 to 6. Therefore, it can be considered a problem of statistical classification.
Automatic image annotation – process by which a computer system automatically assigns textual metadata in the form of captioning or keywords to a digital image. The annotations are used in image retrieval systems to organize and locate images of interest from a database.
Automatic summarization – process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document. Often used to provide summaries of text of a known type, such as articles in the financial section of a newspaper.
Types
Keyphrase extraction –
Document summarization –
Multi-document summarization –
Methods and techniques
Extraction-based summarization –
Abstraction-based summarization –
Maximum entropy-based summarization –
Sentence extraction –
Aided summarization –
Human aided machine summarization (HAMS) –
Machine aided human summarization (MAHS) –
Automatic taxonomy induction – automated construction of tree structures from a corpus. This may be applied to building taxonomical classification systems for reading by end users, such as web directories or subject outlines.
Coreference resolution – in order to derive the correct interpretation of text, or even to estimate the relative importance of various mentioned subjects, pronouns and other referring expressions need to be connected to the right individuals or objects. Given a sentence or larger chunk of text, coreference resolution determines which words ("mentions") refer to which objects ("entities") included in the text.
Anaphora resolution – concerned with matching up pronouns with the nouns or names that they refer to. For example, in a sentence such as "He entered John's house through the front door", "the front door" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of John's house (rather than of some other structure that might also be referred to).
Dialog system –
Foreign-language reading aid – computer program that assists a non-native language user to read properly in their target language. The proper reading means that the pronunciation should be correct and stress to different parts of the words should be proper.
Foreign language writing aid – computer program or any other instrument that assists a non-native language user (also referred to as a foreign language learner) in writing decently in their target language. Assistive operations can be classified into two categories: on-the-fly prompts and post-writing checks.
Grammar checking – the act of verifying the grammatical correctness of written text, especially if this act is performed by a computer program.
Information retrieval –
Cross-language information retrieval –
Machine translation (MT) – aims to automatically translate text from one human language to another.  This is one of the most difficult problems, and is a member of a class of problems colloquially termed "AI-complete", i.e. requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.) in order to solve properly.
Classical approach of machine translation – rules-based machine translation.
Computer-assisted translation –
Interactive machine translation –
Translation memory – database that stores so-called "segments", which can be sentences, paragraphs or sentence-like units (headings, titles or elements in a list) that have previously been translated, in order to aid human translators.
Example-based machine translation –
Rule-based machine translation –
Natural language programming – interpreting and compiling instructions communicated in natural language into computer instructions (machine code).
Natural language search –
Optical character recognition (OCR) – given an image representing printed text, determine the corresponding text.
Question answering – given a human-language question, determine its answer.  Typical questions have a specific right answer (such as "What is the capital of Canada?"), but sometimes open-ended questions are also considered (such as "What is the meaning of life?").
Open domain question answering –
Spam filtering –
Sentiment analysis – extracts subjective information usually from a set of documents, often using online reviews to determine "polarity" about specific objects. It is especially useful for identifying trends of public opinion in the social media, for the purpose of marketing.
Speech recognition – given a sound clip of a person or people speaking, determine the textual representation of the speech.  This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed "AI-complete" (see above).  In natural speech there are hardly any pauses between successive words, and thus speech segmentation is a necessary subtask of speech recognition (see below). In most spoken languages, the sounds representing successive letters blend into each other in a process termed coarticulation, so the conversion of the analog signal to discrete characters can be a very difficult process.
Speech synthesis (Text-to-speech) –
Text-proofing –
Text simplification – automated editing a document to include fewer words, or use easier words, while retaining its underlying meaning and information.

Component processes
Natural language understanding – converts chunks of text into more formal representations such as first-order logic structures that are easier for computer programs to manipulate. Natural language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural language expression which usually takes the form of organized notations of natural languages concepts. Introduction and creation of language metamodel and ontology are efficient however empirical solutions. An explicit formalization of natural languages semantics without confusions with implicit assumptions such as closed-world assumption (CWA) vs. open-world assumption, or subjective Yes/No vs. objective True/False is expected for the construction of a basis of semantics formalization.
Natural language generation – task of converting information from computer databases into readable  human language.

Component processes of natural language understanding
Automatic document classification (text categorization) –
Automatic language identification –
Compound term processing – category of techniques that identify compound terms and match them to their definitions. Compound terms are built by combining two (or more) simple terms, for example "triple" is a single word term but "triple heart bypass" is a compound term.
Automatic taxonomy induction –
Corpus processing –
Automatic acquisition of lexicon –
Text normalization –
Text simplification –
Deep linguistic processing –
Discourse analysis – includes a number of related tasks.  One task is identifying the discourse structure of connected text, i.e. the nature of the discourse relationships between sentences (e.g. elaboration, explanation, contrast).  Another possible task is recognizing and classifying the speech acts in a chunk of text (e.g. yes-no questions, content questions, statements, assertions, orders, suggestions, etc.).
Information extraction –
Text mining – process of deriving high-quality information from text. High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning.
Biomedical text mining –  (also known as BioNLP), this is text mining applied to texts and literature of the biomedical and molecular biology domain. It is a rather recent research field drawing elements from natural language processing, bioinformatics, medical informatics and computational linguistics. There is an increasing interest in text mining and information extraction strategies applied to the biomedical and molecular biology literature due to the increasing number of electronically available publications stored in databases such as PubMed.
Decision tree learning –
Sentence extraction –
Terminology extraction –
Latent semantic indexing –
Lemmatisation – groups together all like terms that share a same lemma such that they are classified as a single item.
Morphological segmentation – separates words into individual morphemes and identifies the class of the morphemes.  The difficulty of this task depends greatly on the complexity of the morphology (i.e. the structure of words) of the language being considered.  English has fairly simple morphology, especially inflectional morphology, and thus it is often possible to ignore this task entirely and simply model all possible forms of a word (e.g. "open, opens, opened, opening") as separate words.  In languages such as Turkish, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms.
Named entity recognition (NER) – given a stream of text, determines which items in the text map to proper names, such as people or places, and what the type of each such name is (e.g. person, location, organization).  Although capitalization can aid in recognizing named entities in languages such as English, this information cannot aid in determining the type of named entity, and in any case is often inaccurate or insufficient.  For example, the first word of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized.  Furthermore, many other languages in non-Western scripts (e.g. Chinese or Arabic) do not have any capitalization at all, and even languages with capitalization may not consistently use it to distinguish names.  For example, German capitalizes all nouns, regardless of whether they refer to names, and French and Spanish do not capitalize names that serve as adjectives.
Ontology learning –  automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms and the relationships between those concepts from a corpus of natural language text, and encoding them with an ontology language for easy retrieval. Also called "ontology extraction", "ontology generation", and "ontology acquisition".
Parsing – determines the parse tree (grammatical analysis) of a given sentence.  The grammar for natural languages is ambiguous and typical sentences have multiple possible analyses.  In fact, perhaps surprisingly, for a typical sentence there may be thousands of potential parses (most of which will seem completely nonsensical to a human).
Shallow parsing –
Part-of-speech tagging – given a sentence, determines the part of speech for each word.  Many words, especially common ones, can serve as multiple parts of speech.  For example, "book" can be a noun ("the book on the table") or verb ("to book a flight"); "set" can be a noun, verb or adjective; and "out" can be any of at least five different parts of speech. Some languages have more such ambiguity than others.  Languages with little inflectional morphology, such as English are particularly prone to such ambiguity. Chinese is prone to such ambiguity because it is a tonal language during verbalization. Such inflection is not readily conveyed via the entities employed within the orthography to convey intended meaning.
Query expansion –
Relationship extraction – given a chunk of text, identifies the relationships among named entities (e.g. who is the wife of whom).
Semantic analysis (computational) – formal analysis of meaning, and "computational" refers to approaches that in principle support effective implementation.
Explicit semantic analysis –
Latent semantic analysis –
Semantic analytics –
Sentence breaking (also known as sentence boundary disambiguation and sentence detection) – given a chunk of text, finds the sentence boundaries.  Sentence boundaries are often marked by periods or other punctuation marks, but these same characters can serve other purposes (e.g. marking abbreviations).
Speech segmentation – given a sound clip of a person or people speaking, separates it into words.  A subtask of speech recognition and typically grouped with it.
Stemming – reduces an inflected or derived word into its word stem, base, or root form.
Text chunking –
Tokenization – given a chunk of text, separates it into distinct words, symbols, sentences, or other units
Topic segmentation and recognition – given a chunk of text, separates it into segments each of which is devoted to a topic, and identifies the topic of the segment.
Truecasing –
Word segmentation – separates a chunk of continuous text into separate words.  For a language like English, this is fairly trivial, since words are usually separated by spaces.  However, some written languages like Chinese, Japanese and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language.
Word sense disambiguation (WSD) – because many words have more than one meaning, word sense disambiguation is used to select the meaning which makes the most sense in context.  For this problem, we are typically given a list of words and associated word senses, e.g. from a dictionary or from an online resource such as WordNet.
Word-sense induction – open problem of natural language processing, which concerns the automatic identification of the senses of a word (i.e. meanings). Given that the output of word-sense induction is a set of senses for the target word (sense inventory), this task is strictly related to that of word-sense disambiguation (WSD), which relies on a predefined sense inventory and aims to solve the ambiguity of words in context.
Automatic acquisition of sense-tagged corpora –
W-shingling – set of unique "shingles"—contiguous subsequences of tokens in a document—that can be used to gauge the similarity of two documents. The w denotes the number of tokens in each shingle in the set.

Component processes of natural language generation
Natural language generation – task of converting information from computer databases into readable  human language.

Automatic taxonomy induction (ATI) – automated building of tree structures from a corpus. While ATI is used to construct the core of ontologies (and doing so makes it a component process of natural language understanding), when the ontologies being constructed are end user readable (such as a subject outline), and these are used for the construction of further documentation (such as using an outline as the basis to construct a report or treatise) this also becomes a component process of natural language generation.
Document structuring –

History of natural language processing
History of natural language processing

History of machine translation
History of automated essay scoring
History of natural language user interface
History of natural language understanding
History of optical character recognition
History of question answering
History of speech synthesis
Turing test – test of a machine's ability to exhibit intelligent behavior, equivalent to or indistinguishable from, that of an actual human. In the original illustrative example, a human judge engages in a natural language conversation with a human and a machine designed to generate performance indistinguishable from that of a human being. All participants are separated from one another. If the judge cannot reliably tell the machine from the human, the machine is said to have passed the test. The test was introduced by Alan Turing in his 1950 paper "Computing Machinery and Intelligence," which opens with the words: "I propose to consider the question, 'Can machines think?'"
Universal grammar – theory in linguistics, usually credited to Noam Chomsky, proposing that the ability to learn grammar is hard-wired into the brain. The theory suggests that linguistic ability manifests itself without being taught (see poverty of the stimulus), and that there are properties that all natural human languages share. It is a matter of observation and experimentation to determine precisely what abilities are innate and what properties are shared by all languages.
ALPAC – was a committee of seven scientists led by John R. Pierce, established in 1964 by the U. S. Government in order to evaluate the progress in computational linguistics in general and machine translation in particular. Its report, issued in 1966, gained notoriety for being very skeptical of research done in machine translation so far, and emphasizing the need for basic research in computational linguistics; this eventually caused the U. S. Government to reduce its funding of the topic dramatically.
Conceptual dependency theory – a model of natural language understanding used in artificial intelligence systems.  Roger Schank at Stanford University introduced the model in 1969, in the early days of artificial intelligence. This model was extensively used by Schank's students at Yale University such as Robert Wilensky, Wendy Lehnert, and Janet Kolodner.
Augmented transition network – type of graph theoretic structure used in the operational definition of formal languages, used especially in parsing relatively complex natural languages, and having wide application in artificial intelligence.  Introduced by William A. Woods in 1970.
Distributed Language Translation (project) –

Timeline of NLP software
General natural language processing concepts
Sukhotin's algorithm – statistical classification algorithm for classifying characters in a text as vowels or consonants. It was initially created by Boris V. Sukhotin.
T9 (predictive text) – stands for "Text on 9 keys", is a USA-patented predictive text technology for mobile phones (specifically those that contain a 3x4 numeric keypad), originally developed by Tegic Communications, now part of Nuance Communications.
Tatoeba – free collaborative online database of example sentences geared towards foreign language learners.
Teragram Corporation –  fully owned subsidiary of SAS Institute, a major producer of statistical analysis software, headquartered in Cary, North Carolina, USA. Teragram is based in Cambridge, Massachusetts and specializes in the application of computational linguistics to multilingual natural language processing.
TipTop Technologies – company that developed TipTop Search, a real-time web, social search engine with a unique platform for semantic analysis of natural language. TipTop Search provides results capturing individual and group sentiment, opinions, and experiences from content of various sorts including real-time messages from Twitter or consumer product reviews on Amazon.com.
Transderivational search – when a search is being conducted for a fuzzy match across a broad field. In computing the equivalent function can be performed using content-addressable memory.
Vocabulary mismatch – common phenomenon in the usage of natural languages, occurring when different people name the same thing or concept differently.
LRE Map –
Reification (linguistics) –
Semantic Web –
Metadata –
Spoken dialogue system –
Affix grammar over a finite lattice –
Aggregation (linguistics) –
Bag-of-words model – model that represents a text as a bag (multiset) of its words that disregards grammar and word sequence, but maintains multiplicity. This model is a commonly used to train document classifiers
Brill tagger –
Cache language model –
ChaSen, MeCab – provide morphological analysis and word splitting for Japanese
Classic monolingual WSD –
ClearForest –
CMU Pronouncing Dictionary – also known as cmudict, is a public domain pronouncing dictionary designed for uses in speech technology, and was created by Carnegie Mellon University (CMU). It defines a mapping from English words to their North American pronunciations, and is commonly used in speech processing applications such as the Festival Speech Synthesis System and the CMU Sphinx speech recognition system.
Concept mining –
Content determination –
DATR –
DBpedia Spotlight –
Deep linguistic processing –
Discourse relation –
Document-term matrix –
Dragomir R. Radev –
ETBLAST –
Filtered-popping recursive transition network –
Robby Garner –
GeneRIF –
Gorn address –
Grammar induction –
Grammatik –
Hashing-Trick –
Hidden Markov model –
Human language technology –
Information extraction –
International Conference on Language Resources and Evaluation –
Kleene star –
Language Computer Corporation –
Language model –
Languageware –
Latent semantic mapping –
Legal information retrieval –
Lesk algorithm –
Lessac Technologies –
Lexalytics –
Lexical choice –
Lexical Markup Framework –
Lexical substitution –
LKB –
Logic form –
LRE Map –
Machine translation software usability –
MAREC –
Maximum entropy –
Message Understanding Conference –
METEOR –
Minimal recursion semantics –
Morphological pattern –
Multi-document summarization –
Multilingual notation –
Naive semantics –
Natural language –
Natural language interface –
Natural language user interface –
News analytics –
Nondeterministic polynomial –
Open domain question answering –
Optimality theory –
Paco Nathan –
Phrase structure grammar –
Powerset (company) –
Production (computer science) –
PropBank –
Question answering –
Realization (linguistics) –
Recursive transition network –
Referring expression generation –
Rewrite rule –
Semantic compression –
Semantic neural network –
SemEval –
SPL notation –
Stemming – reduces an inflected or derived word into its word stem, base, or root form.
String kernel –

Natural language processing tools
Google Ngram Viewer – graphs n-gram usage from a corpus of more than 5.2 million books

Corpora
Text corpus (see list) – large and structured set of texts (nowadays usually electronically stored and processed). They are used to do statistical analysis and hypothesis testing, checking occurrences or validating linguistic rules within a specific language territory.
Bank of English
British National Corpus
Corpus of Contemporary American English (COCA)
Oxford English Corpus

Natural language processing toolkits
The following natural language processing toolkits are notable collections of natural language processing software. They are suites of libraries, frameworks, and applications for symbolic, statistical natural language and speech processing.

Named entity recognizers
ABNER (A Biomedical Named Entity Recognizer) – open source text mining program that uses linear-chain conditional random field sequence models. It automatically tags genes, proteins and other entity names in text. Written by Burr Settles of the University of Wisconsin-Madison.
Stanford NER (Named Entity Recognizer) — Java implementation of a Named Entity Recognizer that uses linear-chain conditional random field sequence models. It automatically tags persons, organizations, and locations in text in English, German, Chinese, and Spanish languages. Written by Jenny Finkel and other members of the Stanford NLP Group at Stanford University.

Translation software
Comparison of machine translation applications
Machine translation applications
Google Translate
DeepL
Linguee – web service that provides an online dictionary for a number of language pairs. Unlike similar services, such as LEO, Linguee incorporates a search engine that provides access to large amounts of bilingual, translated sentence pairs, which come from the World Wide Web. As a translation aid, Linguee therefore differs from machine translation services like Babelfish and is more similar in function to a translation memory.
Hindi-to-Punjabi Machine Translation System
UNL Universal Networking Language
Yahoo! Babel Fish
Reverso

Other software
CTAKES – open-source natural language processing system for information extraction from electronic medical record clinical free-text. It processes clinical notes, identifying types of clinical named entities — drugs, diseases/disorders, signs/symptoms, anatomical sites and procedures. Each named entity has attributes for the text span, the ontology mapping code, context (family history of, current, unrelated to patient), and negated/not negated. Also known as Apache cTAKES.
DMAP –
ETAP-3 – proprietary linguistic processing system focusing on English and Russian. It is a rule-based system which uses the Meaning-Text Theory as its theoretical foundation.
JAPE – the Java Annotation Patterns Engine, a component of the open-source General Architecture for Text Engineering (GATE) platform. JAPE is a finite state transducer that operates over annotations based on regular expressions.
LOLITA – "Large-scale, Object-based, Linguistic Interactor, Translator and Analyzer". LOLITA was developed by Roberto Garigliano and colleagues between 1986 and 2000. It was designed as a general-purpose tool for processing unrestricted text that could be the basis of a wide variety of applications. At its core was a semantic network containing some 90,000 interlinked concepts.
Maluuba –  intelligent personal assistant for Android devices, that uses a contextual approach to search which takes into account the user's geographic location, contacts, and language.
METAL MT –  machine translation system developed in the 1980s at the University of Texas and at Siemens which ran on Lisp Machines.
Never-Ending Language Learning – semantic machine learning system developed by a research team at Carnegie Mellon University, and supported by grants from DARPA, Google, and the NSF, with portions of the system running on a supercomputing cluster provided by Yahoo!. NELL was programmed by its developers to be able to identify a basic set of fundamental semantic relationships between a few hundred predefined categories of data, such as cities, companies, emotions and sports teams. Since the beginning of 2010, the Carnegie Mellon research team has been running NELL around the clock, sifting through hundreds of millions of web pages looking for connections between the information it already knows and what it finds through its search process – to make new connections in a manner that is intended to mimic the way humans learn new information.
NLTK –
Online-translator.com –
Regulus Grammar Compiler – software system for compiling unification grammars into grammars for speech recognition systems.
S Voice –
Siri (software) –
Speaktoit –
TeLQAS –
Weka's classification tools –
word2vec – models that were developed by a team of researchers led by Thomas Milkov at Google to generate word embeddings that can reconstruct some of the linguistic context of words using shallow, two dimensional neural nets derived from a much larger vector space.
Festival Speech Synthesis System –
CMU Sphinx speech recognition system –
Language Grid - Open source platform for language web services, which can customize language services by combining existing language services.

Chatterbots
Chatterbot – a text-based conversation agent that can interact with human users through some medium, such as an instant message service. Some chatterbots are designed for specific purposes, while others converse with human users on a wide range of topics.

Classic chatterbots
Dr. Sbaitso
ELIZA
PARRY
Racter (or Claude Chatterbot)
Mark V Shaney

General chatterbots
Albert One - 1998 and 1999 Loebner winner, by Robby Garner.
A.L.I.C.E. - 2001, 2002, and 2004 Loebner Prize winner developed by Richard Wallace.
Charlix
Cleverbot (winner of the 2010 Mechanical Intelligence Competition)
Elbot - 2008 Loebner Prize winner, by Fred Roberts.
Eugene Goostman - 2012 Turing 100 winner, by Vladimir Veselov.
Fred - an early chatterbot by Robby Garner.
Jabberwacky
Jeeney AI
MegaHAL
Mitsuku, 2013 and 2016 Loebner Prize winner
Rose - ... 2015 - 3x Loebner Prize winner, by Bruce Wilcox.
SimSimi - A popular artificial intelligence conversation program that was created in 2002 by ISMaker.
Spookitalk - A chatterbot used for NPCs in Douglas Adams' Starship Titanic video game.
Ultra Hal - 2007 Loebner Prize winner, by Robert Medeksza.
Verbot

Instant messenger chatterbots
GooglyMinotaur, specializing in Radiohead, the first bot released by ActiveBuddy (June 2001-March 2002)
SmarterChild, developed by ActiveBuddy and released in June 2001
Infobot, an assistant on IRC channels such as #perl, primarily to help out with answering Frequently Asked Questions (June 1995-today)
Negobot, a bot designed to catch online pedophiles by posing as a young girl and attempting to elicit personal details from people it speaks to.

Natural language processing organizations
AFNLP (Asian Federation of Natural Language Processing Associations) – the organization for coordinating the natural language processing related activities and events in the Asia-Pacific region.
Australasian Language Technology Association –
Association for Computational Linguistics – international scientific and professional society for people working on problems involving natural language processing.

Natural language processing-related conferences
Annual Meeting of the Association for Computational Linguistics (ACL)
International Conference on Intelligent Text Processing and Computational Linguistics (CICLing)
International Conference on Language Resources and Evaluation – biennial conference organised by the European Language Resources Association with the support of institutions and organisations involved in Natural language processing
Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)
Text, Speech and Dialogue (TSD) – annual conference
Text Retrieval Conference (TREC) – on-going series of workshops focusing on various information retrieval (IR) research areas, or tracks

Companies involved in natural language processing
AlchemyAPI – service provider of a natural language processing API.
Google, Inc. – the Google search engine is an example of automatic summarization, utilizing keyphrase extraction.
Calais (Reuters product) – provider of a natural language processing services.
Wolfram Research, Inc. developer of natural language processing computation engine Wolfram Alpha.

Natural language processing publications
Books
Connectionist, Statistical and Symbolic Approaches to Learning for Natural Language Processing  – Wermter, S., Riloff E. and Scheler, G. (editors). First book that addressed statistical and neural network learning of language.
Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics – by Daniel Jurafsky and James H. Martin. Introductory book on language technology.

Book series
Studies in Natural Language Processing – book series of the Association for Computational Linguistics, published by Cambridge University Press.

Journals
Computational Linguistics – peer-reviewed academic journal in the field of computational linguistics. It is published quarterly by MIT Press for the Association for Computational Linguistics (ACL)

People influential in natural language processing
Daniel Bobrow –
Rollo Carpenter – creator of Jabberwacky and Cleverbot.
Noam Chomsky – author of the seminal work Syntactic Structures, which revolutionized Linguistics with 'universal grammar', a rule based system of syntactic structures.
Kenneth Colby –
David Ferrucci – principal investigator of the team that created Watson, IBM's AI computer that won the quiz show Jeopardy!
Lyn Frazier –
Daniel Jurafsky – Professor of Linguistics and Computer Science at Stanford University. With James H. Martin, he wrote the textbook Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics
Roger Schank – introduced the conceptual dependency theory for natural language understanding.
Jean E. Fox Tree –
Alan Turing – originator of the Turing Test.
Joseph Weizenbaum – author of the ELIZA chatterbot.
Terry Winograd – professor of computer science at Stanford University, and co-director of the Stanford Human-Computer Interaction Group. He is known within the philosophy of mind and artificial intelligence fields for his work on natural language using the SHRDLU program.
William Aaron Woods –
Maurice Gross – author of the concept of local grammar, taking finite automata as the competence model of language.
Stephen Wolfram – CEO and founder of Wolfram Research, creator of the programming language (natural language understanding) Wolfram Language, and natural language processing computation engine Wolfram Alpha.
Victor Yngve –

See also
References
Bibliography
Crevier, Daniel (1993), AI: The Tumultuous Search for Artificial Intelligence, New York, NY: BasicBooks, ISBN 0-465-02997-3
McCorduck, Pamela (2004), Machines Who Think (2nd ed.), Natick, MA: A. K. Peters, Ltd., ISBN 978-1-56881-205-2, OCLC 52197627.
Russell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2.


== External links ==
Abdul Majid Bhurgri Institute of Language Engineering (Sindhi: عبدالماجد ڀرڳڙي انسٽيٽيوٽ آف لئنگئيج انجنيئرنگ‎) is an autonomous body under the administrative control of the Culture, Tourism and Antiquities Department, Government of Sindh established for bringing Sindhi language at par with national and international languages in all computational process and Natural language processing.

Establishment
In recognition to services of Abdul-Majid Bhurgri, who is the founder of Sindhi computing, Government of Sindh has established the institute after his name. The institute was primarily initiated on the concept given by a language engineer and linguist Amar Fayaz Buriro in briefing to the Minister, Culture, Tourism and Antiquities, Government of Sindh, Syed Sardar Ali Shah on 21 February 2017 on celebration of International Mother Language Day in Sindhi Language Authority, Hyderabad, Sindh. After the presentation and concept given by Amar Fayaz Buriro, the minister Syed Sardar Ali Shah had announced the Institute. Then, Government of Sindh added the development scheme in the Budget of fiscal year 2017-2018.

Location
The institute is established behind Sindh Museum and Sindhi Language Authority, N-5 National Highway, Qasimabad, Hyderabad, Sindh

See also
Sindhi language
Abdul-Majid Bhurgri
Sindhi Language Authority


== References ==
In linguistics, the affix grammars over a finite lattice (AGFL) formalism is a notation for context-free grammars with finite set-valued features, acceptable to linguists of many different schools.
The AGFL-project aims at the  development of a technology for Natural language processing available under the GNU GPL.

External links
AGFL-project website
AFNLP (Asian Federation of Natural Language Processing Associations) is the organization for coordinating the natural language processing related activities and events in the Asia-Pacific region.

Foundation
AFNLP was founded on 4 October 2000.

Member Associations
ALTA - Australasian Language Technology Association
ANLP Japan Association of Natural Language Processing
ROCLING Taiwan ROC Computational Linguistics Society
SIG-KLC Korea SIG-Korean Language Computing of Korea Information Science Society

Existing Asian Initiatives
NLPRS: Natural Language Processing Pacific Rim Symposium
IRAL: International Workshop on Information Retrieval with Asian Languages
PACLING: Pacific Association for Computational Linguistics
PACLIC: Pacific Asia Conference on Language, Information and Computation
PRICAI: Pacific Rim International Conference on AI
ICCPOL: International Conference on Computer Processing of Oriental Languages
ROCLING: Research on Computational Linguistics Conference

Conferences
IJCNLP-04: The 1st International Joint Conference on Natural Language Processing in Hainan Island, China
IJCNLP-05: The 2nd International Joint Conference on Natural Language Processing in Jeju Island, Korea
IJCNLP-08: The 3rd International Joint Conference on Natural Language Processing in Hyderabad, India
ACL-IJCNLP-2009: Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics (ACL) and 4th International Joint Conference on Natural Language Processing (IJCNLP) in Singapore
IJNCLP-11: The 5th International Joint Conference on Natural Language Processing in Chiang Mai, Thailand

References
External links
http://www.afnlp.org/
Aggregation is a subtask of natural language generation, which involves merging syntactic constituents (such as sentences and phrases) together.  Sometimes aggregation can be done at a conceptual level.

Examples
A simple example of syntactic aggregation is merging the two sentences John went to the shop and John bought an apple into the single sentence John went to the shop and bought an apple.
Syntactic aggregation can be much more complex than this.  For example, aggregation can embed one of the constituents in the other; e.g., we can aggregate John went to the shop and The shop was closed into the sentence John went to the shop, which was closed.
From a pragmatic perspective, aggregating sentences together often suggests to the reader that these sentences are related to each other.  If this is not the case, the reader may be confused.  For example, someone who reads John went to the shop and bought an apple may infer that the apple was bought in the shop; if this is not the case, then these sentences should not be aggregated.

Algorithms and issues
Aggregation algorithms must do two things:

Decide when two constituents should be aggregated
Decide how two constituents should be aggregated, and create the aggregated structureThe first issue, deciding when to aggregate, is poorly understood.  Aggegration decisions certainly depend on the semantic relations between the constituents, as mentioned above; they also depend on the genre (e.g., bureaucratic texts tend to be more aggregated than instruction manuals).  They probably should depend on rhetorical and discourse structure.  The literacy level of the reader is also probably important (poor readers need shorter sentences).  But we have no integrated model which brings all these factors together into a single algorithm.
With regard to the second issue, there have been some studies of different types of aggregation, and how they should be carried out.  Harbusch and Kempen describe several syntactic aggregation strategies.  In their terminology, John went to the shop and bought an apple is an example of forward conjunction Reduction 
.
Much less is known about conceptual aggregation.  Di Eugenio et al. show how conceptual aggregation can be done in an intelligent tutoring system, and demonstrate that performing such aggregation makes the system more effective (and that conceptual aggregation make a bigger impact than syntactic aggregation).

Software
Unfortunately there is not much software available for performing aggregation.  However the SimpleNLG system does include limited support for basic aggregation.  For example, the following code causes SimpleNLG to print out The man is hungry and buys an apple.

External links
simplenlg at GoogleCode


== References ==
The Apache OpenNLP library is a machine learning based toolkit for the processing of natural language text. It supports the most common NLP tasks, such as language detection, tokenization, sentence segmentation, part-of-speech tagging, named entity extraction, chunking, parsing and coreference resolution. These tasks are usually required to build more advanced text processing services.

See also
Unstructured Information Management Architecture (UIMA)
General Architecture for Text Engineering (GATE)
cTAKES

References
External links
Apache OpenNLP Website
Arabic Ontology is a linguistic ontology for the Arabic language, which can be used as an Arabic Wordnet with ontologically-clean content. People use it also as a tree (i.e. classification) of the concepts/meanings of the Arabic terms. It is a formal representation of the concepts that the Arabic terms convey, and its content is ontologically well-founded, and benchmarked to scientific advances and rigorous knowledge sources rather than to speakers’ naïve beliefs as wordnets typically do 
. The Ontology tree can be explored online.

Ontology Structure
The ontology structure (i.e., data model) is similar to Wordnet structure. Each concept in the ontology is given a unique concept identifier (URI), informally described by a gloss, and lexicalized by one or more of synonymous lemma terms. Each term-concept pair is called a sense, and is given a SenseID. A set of senses is called synset. Concepts and senses are described by further attributes such as era and area - to specify when and where it is used, lexicalization type, example sentence, example instances, ontological analysis, and others. Semantic relations (e.g., SubTypeOf, PartOf, and others) are defined between concepts. Some important individuals are included in the ontology, such as individual countries and seas. These individuals are given separate IndividualIDs and linked with their concepts through the InstanceOf relation.

Mappings to other resources
Concepts in the Arabic Ontology are mapped to synsets in Wordnet, as well as to BFO and DOLCE. Terms used in the Arabic Ontology are mapped to lemmas in the LDC's SAMA database.

Arabic Ontology versus Arabic WordNet
The Arabic Ontology can be seen as a next generation of WordNet - or as an ontologically-clean Arabic WordNet. It follows the same structure (i.e., data model) as wordnet, and it is fully mapped to Wordnet. However, there are critical foundational differences between them: 

The ontology is benchmarked on state-of-art scientific discoveries, while WordNet is benchmarked on native-speakers’ naïve knowledge;
The ontology is governed by scientifically and philosophically well-established top levels;
Unlike WordNet, all concepts in the ontology are formal, i.e., a concept is a set of individuals (i.e., a class), thus concepts like (horizon) are not allowed in the ontology; and
Glosses in the ontology are strictly formulated, and focus on the distinguishing characteristics, which is not the case in WordNet.

Applications
The Arabic Ontology can be used in many application domains; such as: 

Information retrieval, to enrich queries (e.g., in search engines) and improve the quality of the results, i.e. meaningful search rather than string-matching search;
Machine translation and word-sense disambiguation, by finding the exact mapping of concepts across languages, especially that the Arabic ontology is also mapped to the WordNet;
Data Integration and interoperability in which the Arabic ontology can be used as a semantic reference to link databases and information systems;
Semantic Web and Web 3.0, by using the Arabic ontology as a semantic reference to disambiguate the meanings used in websites; among many other applications.

URLs Design
The URLs in the Arabic Ontology are designed according to the W3C's Best Practices for Publishing Linked Data, as described in the following URL schemes. This allows one to also explore the whole database like exploring a graph:

Ontology Concept: Each concept in the Arabic Ontology has a ConceptID and can be accessed using: https ://{domain}/concept/{ConceptID | Term}. In case of a term, the set of concepts that this term lexicalizes are all retrieved. In case of a ConceptID, the concept and its direct subtypes are retrieved, e.g. https://ontology.birzeit.edu/concept/293198
Semantic relations: Relationships between concepts can be accessed using these schemes: (i) the URL: https ://{domain}/concept/{RelationName}/{ConceptID} allows retrieval of relationships among ontology concepts. (ii) the URL: https ://{domain}/lexicalconcept/{RelationName}/{lexicalConceptID} allows retrieval of relations between lexical concepts. For example, https://ontology.birzeit.edu/concept/instances/293121 retrieves the instances of the concept 293121. The relations that are currently used in our database are: {subtypes, type, instances, parts, related, similar, equivalent}.


== References ==
Artificial Solutions is a multinational technology company that develops technology to enable enterprises to rapidly build conversational AI systems that allow users to converse with applications and electronic devices in free-format, natural language, using speech, text, touch or gesture.Delivered through Teneo, a conversational AI development and analytics platform, the company’s technology allows business users and developers to collaborate on creating conversational applications in 35 languages, running over any OS, on any device, without the need for specialist linguistic skills.The company's natural language solutions have been deployed in a wide range of industries including automotive, finance, energy, entertainment, telecoms, the public sector, retail and travel.The company was founded in 2001 and became a public company in 2019 after the reverse takeover of Indentive AB. Artificial Solutions is listed on the Nasdaq First North stock exchange under the ticker ASAI.

History
Artificial Solutions focuses on conversational AI platforms for the enterprise.  With a mature, proven and patented platform solution for enterprise conversational AI, the company has a global network of offices and strategic partners deploying artificially intelligent conversational applications for some of the world’s best-known brands.Founded in Stockholm in 2001 by friends Johan Åhlund, Johan Gustavsson and Michael Söderström the company created interactive web assistants using a combination of artificial intelligence and natural language processing. Artificial Solutions expanded with the development of online customer service optimization products and by 2005 it had several offices throughout Europe supporting the development and sales of its online virtual assistants. Artificial Solutions was placed as visionary in the latest Gartner Magic Quadrant for CRM Web Customer Service Applications.In 2008 Elbot, Artificial Solutions’ test-bed to explore the psychology of human-machine communication, won the Loebner Prize. Elbot is the closest contestant of the annual competition based on the Turing Test to reach the 30% threshold.With a change in management in 2010 the company started to focus the basis of its technology on Natural Language Interaction and launched the Teneo Platform, which allows people to hold humanlike, intelligent conversations with applications and services running on electronic devices.
In 2013 Artificial Solutions launched Lyra, a mobile personal assistant that is able to operate and remember the context of the conversation across different platforms and operating systems.A new round of funding was announced in June 2013 to support expansion in the US market.Since then the company has continued to develop the Teneo Platform and patent technology in the conversational AI sector including an intelligent framework for enabling chatbots to interact easily with each other. In 2018, the company raised a total of EUR 13.7 in equity capital to help support global growth and to accelerate the expansion of Teneo.
In 2019, Artificial Solutions Holding successfully completed the Reverse Takeover of Indentive, enabling the business of Artificial Solutions Holding to be traded on Nasdaq First North. Lawrence Flynn remains the CEO.

Products
Teneo
Teneo is an multi award-winning conversational AI platform designed for the global enterprise. The platform allows business users and developers to collaborate on creating sophisticated, highly intelligent conversational applications that run across 35 languages, multiple platforms and channels in record time.
Teneo also includes data analytics to help enterprises make sense of complex conversational data in real time while still enabling businesses to meet privacy legislation such as GDPR.

References
External links
Lyra
Elbot
The AsoSoft text corpus is the first large-scale Kurdish text corpus, collected and processed by the AsoSoft research and development group. It contains 458,000 documents (188 million tokens) that are collected from sources such as websites, news agencies, books, and magazines. The corpus is partially tagged by topic, so it can be used for topic identification tasks. Also, it is applicable for extracting language model and computational lexicon information. Part of the corpus (75 million tokens) is available online for non-commercial use. The corpus uses the TEI format.

References
External links
Official website
AsoSoft-Text-Corpus on GitHub
Attempto Controlled English (ACE) is a controlled natural language, i.e. a subset of standard English with a restricted syntax and restricted semantics described by a small set of construction and interpretation rules. It has been under development at the University of Zurich since 1995. In 2013, ACE version 6.7 was announced.ACE can serve as knowledge representation, specification, and query language, and is intended for professionals who want to use formal notations and formal methods, but may not be familiar with them. Though ACE appears perfectly natural – it can be read and understood by any speaker of English – it is in fact a formal language.ACE and its related tools have been used in the fields of software specifications, theorem proving, text summaries, ontologies, rules, querying, medical documentation and planning.
Here are some simple examples:

Every woman is a human.
A woman is a human.
A man tries-on a new tie. If the tie pleases his wife then the man buys it.ACE construction rules require that each noun be introduced by a determiner (a, every, no, some, at least 5, ...). Regarding the list of examples above, ACE interpretation rules decide that (1) is interpreted as universally quantified, while (2) is interpreted as existentially quantified. Sentences like "Women are human" do not follow ACE syntax and are consequently not valid.
Interpretation rules resolve the anaphoric references in (3): the tie and it of the second sentence refer to a new tie of the first sentence, while his and the man of the second sentence refer to a man of the first sentence. Thus an ACE text is a coherent entity of anaphorically linked sentences.
The Attempto Parsing Engine (APE) translates ACE texts unambiguously into discourse representation structures (DRS) that use a variant of the language of first-order logic. A DRS can be further translated into other formal languages, for instance AceRules with various semantics,   OWL, and SWRL. Translating an ACE text into (a fragment of) first-order logic allows users to  reason about the text, for instance to verify, to validate, and to query it.

Overview
As an overview of the current version 6.6 of ACE this section:

Briefly describes the vocabulary
Gives an account of the syntax
Summarises the handling of ambiguity
Explains the processing of anaphoric references.

Vocabulary
The vocabulary of ACE comprises:

Predefined function words (e.g. determiners, conjunctions)
Predefined phrases (e.g. "it is false that ...", "it is possible that ...")
Content words (e.g. nouns, verbs, adjectives, adverbs).

Grammar
The grammar of ACE defines and constrains the form and the meaning of ACE sentences and texts. ACE's grammar is expressed as a set of  construction rules. The meaning of sentences is described as a small set of interpretation rules. A Troubleshooting Guide describes how to use ACE and how to avoid pitfalls.

ACE texts
An ACE text is a sequence of declarative sentences that can be anaphorically interrelated. Furthermore, ACE supports questions and commands.

Simple sentences
A simple sentence asserts that something is the case — a fact, an event, a state.

The temperature is −2 °C.
A customer inserts 2 cards.
A card and a code are valid.Simple ACE sentences have the following general structure:

subject + verb + complements + adjunctsEvery sentence has a subject and a verb. Complements (direct and indirect objects) are necessary for transitive verbs (insert something) and ditransitive verbs (give something to somebody), whereas adjuncts (adverbs, prepositional phrases) are optional.
All elements of a simple sentence can be elaborated upon to describe the situation in more detail. To further specify the nouns customer and card, we could add adjectives:

A trusted customer inserts two valid cards.possessive nouns and of-prepositional phrases:

John's customer inserts a card of Mary.or variables as appositions:

John inserts a card A.Other modifications of nouns are possible through relative sentences:

A customer who is trusted inserts a card that he owns.which are described below since they make a sentence composite. We can also detail the insertion event, e.g. by adding an adverb:

A customer inserts some cards manually.or, equivalently:

A customer manually inserts some cards.or, by adding prepositional phrases:

A customer inserts some cards into a slot.We can combine all of these elaborations to arrive at:

John's customer who is trusted inserts a valid card of Mary manually into a slot A.

Composite sentences
Composite sentences are recursively built from simpler sentences through coordination, subordination, quantification, and negation. Note that ACE composite sentences overlap with what linguists call compound sentences and complex sentences.

Coordination
Coordination by and is possible between sentences and between phrases of the same syntactic type.

A customer inserts a card and the machine checks the code.
There is a customer who inserts a card and who enters a code.
A customer inserts a card and enters a code.
An old and trusted customer enters a card and a code.Note that the coordination of the noun phrases a card and a code represents a plural object.
Coordination by or is possible between sentences, verb phrases, and relative clauses.

A customer inserts a card or the machine checks the code.
A customer inserts a card or enters a code.
A customer owns a card that is invalid or that is damaged.Coordination by and and or is governed by the standard binding order of logic, i.e. and binds stronger than or. Commas can be used to override the standard binding order. Thus the sentence:

A customer inserts a VisaCard or inserts a MasterCard, and inserts a code.means that the customer inserts a VisaCard and a code, or alternatively a MasterCard and a code.

Subordination
There are four constructs of subordination: relative sentences, if-then sentences, modality, and sentence subordination.
Relative sentences starting with who, which, and that allow to add detail to nouns:

A customer who is trusted inserts a card that he owns.With the help of if-then sentences we can specify conditional or hypothetical situations:

If a card is valid then a customer inserts it.Note the anaphoric reference via the pronoun it in the then-part to the noun phrase a card in the if-part.
Modality allows us to express possibility and necessity:

A trusted customer can/must insert a card.
It is possible/necessary that a trusted customer inserts a card.Sentence subordination comes in various forms:

It is true/false that a customer inserts a card.
It is not provable that a customer inserts a card.
A clerk believes that a customer inserts a card.

Quantification
Quantification allows us to speak about all objects of a certain class (universal quantification), or to denote explicitly the existence of at least one object of this class (existential quantification). The textual occurrence of a universal or existential quantifier opens its scope that extends to the end of the sentence, or in coordinations to the end of the respective coordinated sentence.
To express that all involved customers insert cards we can write

Every customer inserts a card.This sentence means that each customer inserts a card that may, or may not, be the same as the one inserted by another customer. To specify that all customers insert the same card — however unrealistic that situation seems — we can write:

A card is inserted by every customer.or, equivalently:

There is a card that every customer inserts.To state that every card is inserted by a customer we write:

Every card is inserted by a customer.or, somewhat indirectly:

For every card there is a customer who inserts it.

Negation
Negation allows us to express that something is not the case:

A customer does not insert a card.
A card is not valid.To negate something for all objects of a certain class one uses no:

No customer inserts more than 2 cards.or, there is no:

There is no customer who inserts a card.To negate a complete statement one uses sentence negation:

It is false that a customer inserts a card.These forms of negation are logical negations, i.e. they state that something is provably not the case. Negation as failure states that a state of affairs cannot be proved, i.e. there is no information whether the state of affairs is the case or not.

It is not provable that a customer inserts a card.

Queries
ACE supports two forms of queries: yes/no-queries and wh-queries.
Yes/no-queries ask for the existence or non-existence of a specified situation. If we specified:

A customer inserts a card.then we can ask:

Does a customer insert a card?to get a positive answer. Note that interrogative sentences always end with a question mark.
With the help of wh-queries, i.e. queries with query words, we can interrogate a text for details of the specified situation. If we specified:

A trusted customer inserts a valid card manually in the morning in a bank.we can ask for each element of the sentence with the exception of the verb.

Who inserts a card?
Which customer inserts a card?
What does a customer insert?
How does a customer insert a card?
When does a customer enter a card?
Where does a customer enter a card?Queries can also be constructed by a sequence of declarative sentences followed by one interrogative sentence, for example:

There is a customer and there is a card that the customer enters. Does a customer enter a card?

Commands
ACE also supports commands. Some examples:

John, go to the bank!
John and Mary, wait!
Every dog, bark!
A brother of John, give a book to Mary!A command always consists of a noun phrase (the addressee), followed by a comma, followed by an uncoordinated verb phrase. Furthermore, a command has to end with an exclamation mark.

Constraining ambiguity
To constrain the ambiguity of full natural language ACE employs three simple means:

Some ambiguous constructs are not part of the language; unambiguous alternatives are available in their place
All remaining ambiguous constructs are interpreted deterministically on the basis of a small number of interpretation rules
Users can either accept the assigned interpretation, or they must rephrase the input to obtain another one.

Avoidance of ambiguity
In natural language, relative sentences combined with coordinations can introduce ambiguity:

A customer inserts a card that is valid and opens an account.In ACE the sentence has the unequivocal meaning that the customer opens an account, as reflected by the paraphrase:

A card is valid. A customer inserts the card. The customer opens an account.To express the alternative — though not very realistic — meaning that the card opens an account, the relative pronoun that must be repeated, thus yielding a coordination of relative sentences:

A customer inserts a card that is valid and that opens an account.This sentence is unambiguously equivalent in meaning to the paraphrase:

A card is valid. The card opens an account. A customer inserts the card.

Interpretation rules
Not all ambiguities can be safely removed from ACE without rendering it artificial. To deterministically interpret otherwise syntactically correct ACE sentences we use a small set of interpretation rules. For example, if we write:

A customer inserts a card with a code.then with a code attaches to the verb inserts, but not to a card. However, this is probably not what we meant to say. To express that the code is associated with the card we can employ the interpretation rule that a relative sentence always modifies the immediately preceding noun phrase, and rephrase the input as:

A customer inserts a card that carries a code.yielding the paraphrase:

A card carries a code. A customer inserts the card.or — to specify that the customer inserts a card and a code — as:

A customer inserts a card and a code.

Anaphoric references
Usually ACE texts consist of more than one sentence:

A customer enters a card and a code. If a code is valid then SimpleMat accepts a card.To express that all occurrences of card and code should mean the same card and the same code, ACE provides anaphoric references via the definite article:

A customer enters a card and a code. If the code is valid then SimpleMat accepts the card.During the processing of the ACE text, all anaphoric references are replaced by the most recent and most specific accessible noun phrase that agrees in gender and number. As an example of "most recent and most specific", suppose an ACE parser is given the sentence:

A customer enters a red card and a blue card.Then:

The card is correct.refers to the second card, while:

The red card is correct.refers to the first card.
Noun phrases within if-then sentences, universally quantified sentences, negations, modality, and subordinated sentences cannot be referred to anaphorically from subsequent sentences, i.e. such noun phrases are not "accessible" from the following text. Thus for each of the sentences:

If a customer owns a card then he enters it.
Every customer enters a card.
A customer does not enter a card.
A customer can enter a card.
A clerk believes that a customer enters a card.we cannot refer to a card with:

The card is correct.Anaphoric references are also possible via personal pronouns:

A customer enters a card and a code. If it is valid then SimpleMat accepts the card.or via variables:

A customer enters a card X and a code Y. If Y is valid then SimpleMat accepts X.Anaphoric references via definite articles and variables can be combined:

A customer enters a card X and a code Y. If the code Y is valid then SimpleMat accepts the card X.Note that proper names like SimpleMat always refer to the same object.

See also
Gellish
Natural Language Processing
Natural language programming
Structured English
ClearTalk, another machine-readable knowledge representation language
Inform 7, a programming language with English syntax

References
External links
Project Attempto
Attensity provides social analytics and engagement applications for social customer relationship management (social CRM). Attensity's text analytics software applications extract facts, relationships and sentiment from unstructured data, which comprise approximately 85% of the information companies store electronically.

History
Attensity was founded in 2000. An early investor in Attensity was In-Q-Tel, which funds technology to support the missions of the US Government and the broader DOD.  InTTENSITY, an independent company that has combined Inxight with Attensity Software (the only joint development project that combines two InQTel funded software packages), is the exclusive distributor and outlet for Attensity in the Federal Market. In 2009, Attensity Corp., then based in Palo Alto, merged with Germany's Empolis and Living-e AG to form Attensity Group. In 2010, Attensity Group acquired Biz360, Inc., a provider of social media monitoring and market intelligence solutions. In early 2012, Attensity Group divested itself of the Empolis business unit via a management buyout; that unit currently conducts business under its pre-merger name.
Attensity Group is a closely held private company. Its majority shareholder is Aeris Capital, a private Swiss investment office advising a high-net-worth individual and his charitable foundation. Foundation Capital, Granite Ventures, and Scale Venture Partners were among Biz360's investors and thus became shareholders in Attensity Group.In February, 2016, Attensity's IP assets were acquired by InContact, and Attensity closed its doors for good.

See also
Text mining

References
External links
Official website 
Archive of official website
AUTINDEX is a commercial text mining software package based on sophisticated linguistics.AUTINDEX, resulting from research in information extraction, is a product of the Institute of Applied Information Sciences (IAI) which is a non-profit institute that has been researching and developing language technology since its foundation in 1985. IAI is an institute affiliated to Saarland University in Saarbrücken, Germany.
AUTINDEX is the result of a number of research projects funded by the EU (Project BINDEX), by Deutsche Forschungsgemeinschaft and the German Ministry for Economy. Amongst the latter there are the projects LinSearch, and WISSMER, see also the reference to IAI-Website.The basic functionality of AUTINDEX is the extraction of key words from a document to represent the semantics of the document. Ideally the system is integrated with a thesaurus that defines the standardised terms to be used for key word assignment. 
AUTINDEX is used in library applications (e.g. integrated in dandelon.com) as well as in high quality (expert) information systems, and in document management and content management environments. Together with AUTINDEX a number of additional software comes along such as an integration with Apache Solr / Lucene to provide a complete information retrieval environment, a classification and categorisation system on the basis of a machine learning software that assigns domains to the document, and a system for searching with semantically similar terms that are collected in so called tag clouds.

See also
Information retrieval
Linguistics
Knowledge Management
Natural Language Processing
Semantics

References
Publications
Ripplinger, Bärbel 2001: Das Indexierungssystem AUTINDEX, in GLDV Tagung, Giessen.
Paul Schmidt, Thomas Bähr & Dr.-Ing. Jens Biesterfeld &Thomas Risse & Kerstin Denecke & Claudiu Firan, 2008: LINSearch. Aufbereitung von Fachwissen für die gezielte Informationsversorgung. In: Proceedings of Knowtech, Frankfurt.
Paul Schmidt, Mahmoud Gindiyeh, Gintare Grigonyte: Language Technology for Information Systems. In: Proceedings of KDIR – The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira. 6.–8. Oktober 2009, Portugal. 2009, S. 259 - 262.
Paul Schmidt, Mahmoud Gindiyeh: Language Technology for Multilingual Information and Document Management. In: Proceedings of ASLIB. London, 19.–20. November 2009.
Rösener, Christoph, Ulrich Herb: Automatische Schlagwortvergabe aus der SWD für Repositorien. Zusammen mit Ulrich Herb in Proceedings. Berufsverband Information Bibliothek, Bibliothekartage. 97. Deutscher Bibliothekartag, Mannheim, 2008.
Svenja Siedle: Suchst du noch oder weißt du schon? Inhaltserschließung leicht gemacht mit automatischer Indexierung. In: tekom-Jahrestagung und tcworld conference 2013
Michael Gerards, Adreas Gerards, Peter Weiland: Der Einsatz der automatischen Indexierungssoftware AUTINDEX im Zentrum für Psychologische Information und Dokumentation (ZPID). 2006 (Online bei zpid.de, PDF-Datei)
Mahmoud Gindiyeh: Anwendung wahrscheinlichkeitstheoretischer Methoden in der linguistischen Informationsverarbeitung. Logos Verlag, Berlin, 2013.

External links
http://www.iai-sb.de/ Institute for Applied Information Sciences
Automated essay scoring (AES) is the use of specialized computer programs to assign grades to essays written in an educational setting. It is a form of educational assessment and an application of natural language processing. Its objective is to classify a large set of textual entities into a small number of discrete categories, corresponding to the possible grades, for example, the numbers 1 to 6. Therefore, it can be considered a problem of statistical classification.
Several factors have contributed to a growing interest in AES. Among them are cost, accountability, standards, and technology. Rising education costs have led to pressure to hold the educational system accountable for results by imposing standards. The advance of information technology promises to measure educational achievement at reduced cost.
The use of AES for high-stakes testing in education has generated significant backlash, with opponents pointing to research that computers cannot yet grade writing accurately and arguing that their use for such purposes promotes teaching writing in reductive ways (i.e. teaching to the test).

History
Most historical summaries of AES trace the origins of the field to the work of Ellis Batten Page. In 1966, he argued for the possibility of scoring essays by computer, and in 1968 he published his successful work with a program called Project Essay Grade (PEG). Using the technology of that time, computerized essay scoring would not have been cost-effective, so Page abated his efforts for about two decades. Eventually, Page sold PEG to Measurement Incorporated
By 1990, desktop computers had become so powerful and so widespread that AES was a practical possibility. As early as 1982, a UNIX program called Writer's Workbench was able to offer punctuation, spelling and grammar advice. In collaboration with several companies (notably Educational Testing Service), Page updated PEG and ran some successful trials in the early 1990s.Peter Foltz and Thomas Landauer developed a system using a scoring engine called the Intelligent Essay Assessor (IEA). IEA was first used to score essays in 1997 for their undergraduate courses. It is now a product from Pearson Educational Technologies and used for scoring within a number of commercial products and state and national exams.
IntelliMetric is Vantage Learning's AES engine. Its development began in 1996. It was first used commercially to score essays in 1998.Educational Testing Service offers "e-rater", an automated essay scoring program. It was first used commercially in February 1999. Jill Burstein was the team leader in its development. ETS's Criterion Online Writing Evaluation Service uses the e-rater engine to provide both scores and targeted feedback.
Lawrence Rudner has done some work with Bayesian scoring, and developed a system called BETSY (Bayesian Essay Test Scoring sYstem). Some of his results have been published in print or online, but no commercial system incorporates BETSY as yet.
Under the leadership of Howard Mitzel and Sue Lottridge, Pacific Metrics developed a constructed response automated scoring engine, CRASE. Currently utilized by several state departments of education and in a U.S. Department of Education-funded Enhanced Assessment Grant, Pacific Metrics’ technology has been used in large-scale formative and summative assessment environments since 2007.
Measurement Inc. acquired the rights to PEG in 2002 and has continued to develop it.In 2012, the Hewlett Foundation sponsored a competition on Kaggle called the Automated Student Assessment Prize (ASAP). 201 challenge participants attempted to predict, using AES, the scores that human raters would give to thousands of essays written to eight different prompts. The intent was to demonstrate that AES can be as reliable as human raters, or more so. The competition also hosted a separate demonstration among nine AES vendors on a subset of the ASAP data. Although the investigators reported that the automated essay scoring was as reliable as human scoring, this claim was not substantiated by any statistical tests because some of the vendors required that no such tests be performed as a precondition for their participation. Moreover, the claim that the Hewlett Study demonstrated that AES can be as reliable as human raters has since been strongly contested, including by Randy E. Bennett, the Norman O. Frederiksen Chair in Assessment Innovation at the Educational Testing Service. Some of the major criticisms of the study have been that five of the eight datasets consisted of paragraphs rather than essays, four of the eight data sets were graded by human readers for content only rather than for writing ability, and that rather than measuring human readers and the AES machines against the "true score", the average of the two readers' scores, the study employed an artificial construct, the "resolved score", which in four datasets consisted of the higher of the two human scores if there was a disagreement.  This last practice, in particular, gave the machines an unfair advantage by allowing them to round up for these datasets.In 1966, Page hypothesized that, in the future, the computer-based judge will be better correlated with each human judge than the other human judges are. Despite criticizing the applicability of this approach to essay marking in general, this hypothesis was supported for marking free text answers to short questions, such as those typical of the British GCSE system. Results of supervised learning demonstrate that the automatic systems perform well when marking by different human teachers is in good agreement. Unsupervised clustering of answers  showed that excellent papers and weak papers formed well-defined clusters, and the automated marking rule for these clusters worked well, whereas marks given by human teachers for the third cluster ('mixed')  can be controversial, and the reliability of any assessment of works from the 'mixed' cluster  can often be questioned (both human and computer-based).

Different dimensions of essay quality
According to a rencent survey, modern AES systems try to score different dimenions of an essay's quality in order to provide feedback to users. These dimensions include the following items: 

Grammaticality: following grammar rules
Usage: using of prepositions, word usage
Mechanics: following rules for spelling, punctuation, capitalization
Style: world choice, sentense structure variety
Relevance: how relevant of the content to the prompt
Organization: how well the essay is structured
Development: development of ideas with examples
Coherison: appropriate use of transition phrases
Coherence: appropriate transitions between ideas
Thesis Clarity: clarity of the thesis
Persuasivenes: convincingness of the major argument

Procedure
From the beginning, the basic procedure for AES has been to start with a training set of essays that have been carefully hand-scored. The program evaluates surface features of the text of each essay, such as the total number of words, the number of subordinate clauses, or the ratio of uppercase to lowercase letters—quantities that can be measured without any human insight. It then constructs a mathematical model that relates these quantities to the scores that the essays received. The same model is then applied to calculate scores of new essays.
Recently, one such mathematical model was created by Isaac Persing and Vincent Ng. which not only evaluates essays on the above features, but also on their argument strength. It evaluates various features of the essay, such as the agreement level of the author and reasons for the same, adherence to the prompt's topic, locations of argument components (major claim, claim, premise), errors in the arguments, cohesion in the arguments among various other features. In contrast to the other models mentioned above, this model is closer in duplicating human insight while grading essays.
The various AES programs differ in what specific surface features they measure, how many essays are required in the training set, and most significantly in the mathematical modeling technique. Early attempts used linear regression. Modern systems may use linear regression  or other machine learning techniques often in combination with other statistical techniques such as latent semantic analysis and Bayesian inference.

Criteria for success
Any method of assessment must be judged on validity, fairness, and reliability. An instrument is valid if it actually measures the trait that it purports to measure. It is fair if it does not, in effect, penalize or privilege any one class of people. It is reliable if its outcome is repeatable, even when irrelevant external factors are altered.
Before computers entered the picture, high-stakes essays were typically given scores by two trained human raters. If the scores differed by more than one point, a more experienced third rater would settle the disagreement. In this system, there is an easy way to measure reliability: by inter-rater agreement. If raters do not consistently agree within one point, their training may be at fault. If a rater consistently disagrees with how other raters look at the same essays, that rater probably needs extra training.
Various statistics have been proposed to measure inter-rater agreement. Among them are percent agreement, Scott's π, Cohen's κ, Krippendorf's α, Pearson's correlation coefficient r, Spearman's rank correlation coefficient ρ, and Lin's concordance correlation coefficient.
Percent agreement is a simple statistic applicable to grading scales with scores from 1 to n, where usually 4 ≤ n ≤ 6. It is reported as three figures, each a percent of the total number of essays scored: exact agreement (the two raters gave the essay the same score), adjacent agreement (the raters differed by at most one point; this includes exact agreement), and extreme disagreement (the raters differed by more than two points). Expert human graders were found to achieve exact agreement on 53% to 81% of all essays, and adjacent agreement on 97% to 100%.Inter-rater agreement can now be applied to measuring the computer's performance. A set of essays is given to two human raters and an AES program. If the computer-assigned scores agree with one of the human raters as well as the raters agree with each other, the AES program is considered reliable. Alternatively, each essay is given a "true score" by taking the average of the two human raters' scores, and the two humans and the computer are compared on the basis of their agreement with the true score.
Some researchers have reported that their AES systems can, in fact, do better than a human. Page made this claim for PEG in 1994. Scott Elliot said in 2003 that IntelliMetric typically outperformed human scorers. AES machines, however, appear to be less reliable than human readers for any kind of complex writing test.In current practice, high-stakes assessments such as the GMAT are always scored by at least one human. AES is used in place of a second rater. A human rater resolves any disagreements of more than one point.

Criticism
AES has been criticized on various grounds. Yang et al. mention "the over-reliance on surface features of responses, the insensitivity to the content of responses and to creativity, and the vulnerability to new types of cheating and test-taking strategies." Several critics are concerned that students' motivation will be diminished if they know that no human will read their writing. Among the most telling critiques are reports of intentionally gibberish essays being given high scores.

HumanReaders.Org Petition
On 12 March 2013, HumanReaders.Org launched an online petition, "Professionals Against Machine Scoring of Student Essays in High-Stakes Assessment". Within weeks, the petition gained thousands of signatures, including Noam Chomsky, and was cited in a number of newspapers, including The New York Times, and on a number of education and technology blogs.The petition describes the use of AES for high-stakes testing as "trivial", "reductive", "inaccurate", "undiagnostic", "unfair" and "secretive".In a detailed summary of research on AES, the petition site notes, "RESEARCH FINDINGS SHOW THAT no one—students, parents, teachers, employers, administrators, legislators—can rely on machine scoring of essays ... AND THAT machine scoring does not measure, and therefore does not promote, authentic acts of writing."The petition specifically addresses the use of AES for high-stakes testing and says nothing about other possible uses.

Software
Most resources for automated essay scoring are proprietary. 

eRater – published by Educational Testing Service
Intellimetric – by Vantage Learning
Project Essay Grade – by Measurement, Inc.
PaperRater


== References ==
Automatic acquisition of lexicon is a computerized process used for the development of a complex morphological lexicon of a language. The lexicon is essential for the NLP (Natural language processing), as well as a prerequisite to any wide-coverage parser.
The two main requirements represent raw corpus and the morphological description of the language. The aim is to provide lemmas that will serve to the explanation of all the words that occur within the corpus. For the achievement of a quality lexicon it is necessary to manually validate the 
generated lemmas and iterate the whole process several times.
The process is focused on the open word classes (e.g. nouns, adjectives, verbs). Closed classes (e.g. prepositions, pronouns, numerals) are excluded.
This method is applicable to the languages with a rich morphology, such as Slovak, Russian or Croatian.
Applied to Slovak, being an inflectional language, the automatic acquisition focuses on the inflectional morphology as well as on the derivational morphology. This fact enables the users to find out the information about derivational relations (e.g. adjectivizations, prefixes) in the lexicon. For example, Slovak word korpusový is an adjectivization of korpus (eng. corpus).

Three-step loop
Conformably to Benoît Sagot, there are three stages involved in the acquisition of lemmas:

 1.	Generation and inflection
2.	Ranking
3.	Manual validation
The more iteration will be performed, the more accurate lexicon will be obtained. For each iteration are essential the information given by a manual validator.

Generation and inflection
Firstly, all words which represent the closed word classes (pronouns, prepositions, numerals) are manually excluded from the given corpus. Number of their occurrences in the corpus is provided.
Then the automatic generation comes, when the hypothetical lemmas according to the morphological description of a language are created. Generated lemmas are consequently being inflected, so that all of their inflected forms are built. Obtained forms are associated with the corresponding lemma and a morphological tag.

Ranking
There was created a probabilistic model, represented by a fix-point algorithm, to rank the hypothetical lemmas generated in the first step. Best ranked lemmas are expected to be ideally all correct, whereas the least ranked tend to be incorrect.

Manual validation
Correctness of the best- ranked lemmas created in the previous step are checked by the manual validator, who should be a native speaker.
Lemmas are at this stage divided into three categories:
- valid lemmas, appended to lexicon
- erroneous lemmas generated by valid forms ( later associated to another lemmas) 
- erroneous lemmas generated by invalid forms (these need to be excluded)

Future development
Automatic acquisition, in comparison to a purely manual development of the lexicons, seems to be promising, considering the future development, because of the short validation time needed and the relatively small amount of human labor involved.

References
External links
Benoît Sagot publishings [2]
The knowledge acquisition bottleneck is perhaps the major impediment to solving the word sense disambiguation (WSD) problem. Unsupervised learning methods rely on knowledge about word senses, which is barely formulated in dictionaries and lexical databases. Supervised learning methods depend heavily on the existence of manually annotated examples for every word sense, a requisite that can so far be met only for a handful of words for testing purposes, as it is done in the Senseval exercises.

Existing methods
Therefore, one of the most promising trends in WSD research is using the largest corpus ever accessible, the World Wide Web, to acquire lexical information automatically. WSD has been traditionally understood as an intermediate language engineering technology which could improve applications such as information retrieval (IR). In this case, however, the reverse is also true: Web search engines implement simple and robust IR techniques that can be successfully used when mining the Web for information to be employed in WSD.
The most direct way of using the Web (and other corpora) to enhance WSD performance is the automatic acquisition of sense-tagged corpora, the fundamental resource to feed supervised WSD algorithms. Although this is far from being commonplace in the WSD literature, a number of different and effective strategies to achieve this goal have already been proposed. Some of these strategies are:

acquisition by direct Web searching (searches for monosemous synonyms, hypernyms, hyponyms, parsed gloss' words, etc.),
Yarowsky algorithm (bootstrapping),
acquisition via Web directories, and
acquisition via cross-language meaning evidences.

Summary
Optimistic results
The automatic extraction of examples to train supervised learning algorithms reviewed has been, by far, the best explored approach 
to mine the web for word sense disambiguation. Some results are certainly encouraging: 

In some experiments, the quality of the Web data for WSD equals that of human-tagged examples. This is the case of the monosemous relatives plus bootstrapping with Semcor seeds technique and the examples taken from the ODP Web directories. In the first case, however, Semcor-size example seeds are necessary (and only available for English), and it has only been tested with a very limited set of nouns; in the second case, the coverage is quite limited, and it is not yet clear whether it can be grown without compromising the quality of the examples retrieved.
It has been shown that a mainstream supervised learning technique trained exclusively with web data can obtain better results than all unsupervised WSD systems which participated at Senseval-2.
Web examples made a significant contribution to the best Senseval-2 English all-words system.

Difficulties
There are, however, several open research issues related to the use of Web examples in WSD: 

High precision in the retrieved examples (i.e., correct sense assignments for the examples) does not necessarily lead to good supervised WSD results (i.e., the examples are possibly not useful for training).
The most complete evaluation of Web examples for supervised WSD indicates that learning with Web data improves over unsupervised techniques, but the results are nevertheless far from those obtained with hand-tagged data, and do not even beat the most-frequent-sense baseline.
Results are not always reproducible; the same or similar techniques may lead to different results in different experiments. Compare, for instance, Mihalcea (2002) with Agirre and Martínez (2004), or Agirre and Martínez (2000) with Mihalcea and Moldovan (1999). Results with Web data seem to be very sensitive to small differences in the learning algorithm, to when the corpus was extracted (search engines change continuously), and on small heuristic issues (e.g., differences in filters to discard part of the retrieved examples).
Results are strongly dependent on bias (i.e., on the relative frequencies of examples per word sense). It is unclear whether this is simply a problem of Web data, or an intrinsic problem of supervised learning techniques, or just a problem of how WSD systems are evaluated (indeed, testing with rather small Senseval data may overemphasize sense distributions compared to sense distributions obtained from the full Web as corpus).
In any case, Web data has an intrinsic bias, because queries to search engines directly constrain the context of the examples retrieved. There are approaches that alleviate this problem, such as using several different seeds/queries per sense or assigning senses to Web directories and then scanning directories for examples; but this problem is nevertheless far from being solved.
Once a Web corpus of examples is built, it is not entirely clear whether its distribution is safe from a legal perspective.

Future
Besides automatic acquisition of examples from the Web, there are some other WSD experiments that have profited from the Web: 

The Web as a social network has been successfully used for cooperative annotation of a corpus (OMWE, Open Mind Word Expert project), which has already been used in three Senseval-3 tasks (English, Romanian and Multilingual).
The Web has been used to enrich WordNet senses with domain information: topic signatures and Web directories, which have in turn been successfully used for WSD.
Also, some research benefited from the semantic information that the Wikipedia maintains on its disambiguation pages.It is clear, however, that most research opportunities remain largely unexplored. For instance, little is known about how to use lexical information extracted from the Web in knowledge-based WSD systems; and it is also hard to find systems that use Web-mined parallel corpora for WSD, even though there are already efficient algorithms that use parallel corpora in WSD.


== References ==
Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content. 
In addition to text, images and videos can also be summarized. Text summarization finds the most informative sentences in a document; image summarization finds the most representative images within an image collection; video summarization extracts the most important frames from the video content.

Approaches
There are two general approaches to automatic summarization: extraction and abstraction.

Extraction-based summarization
Here, content is extracted from the original data, but the extracted content is not modified in any way. Examples of extracted content include key-phrases that can be used to "tag" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above. For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.

Abstraction-based summarization
This has been applied mainly for text. Abstractive methods build an internal semantic representation of the original content, and then use this representation to create a summary that is closer to what a human might express. Abstraction may transform the extracted content by paraphrasing sections of the source document, to condense a text more strongly than extraction. Such transformation, however, is computationally much more challenging than extraction, involving both natural language processing and often a deep understanding of the domain of the original text in cases where the original document relates to a special field of knowledge.
"Paraphrasing" is even more difficult to apply to image and video, which is why most summarization systems are extractive.

Aided summarization
Approaches aimed at higher summarization quality rely on combined software and human effort. In Machine Aided Human Summarization, extractive techniques highlight candidate passages for inclusion (to which the human adds or removes text). In Human Aided Machine Summarization, a human post-processes software output, in the same way that one edits the output of automatic translation by Google Translate.

Applications and systems for summarization
There are broadly two types of extractive summarization tasks depending on what the summarization program focuses on. The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.). The second is  query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query. Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.
An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document. Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a cluster of articles on the same topic). This problem is called multi-document summarization. A related application is summarizing news articles. Imagine a system, which automatically pulls together news articles on a given topic (from the web), and concisely represents the latest news as a summary.
Image collection summarization is another application example of automatic summarization. It consists in selecting a representative set of images from a larger set of images. A summary in this context is useful to show the most representative images of results in an image collection exploration system. Video summarization is a related domain, where the system automatically creates a trailer of a long video. This also has applications in consumer or personal videos, where one might want to skip the boring or repetitive actions. Similarly, in surveillance videos, one would want to extract important and suspicious activity, while ignoring all the boring and redundant frames captured.
At a very high level, summarization algorithms try to find subsets of objects (like set of sentences, or a set of images), which cover information of the entire set. This is also called the core-set. These algorithms model notions like diversity, coverage, information and representativeness of the summary. Query based summarization techniques, additionally model for relevance of the summary with the query. Some techniques and algorithms which naturally model summarization problems are TextRank and PageRank, Submodular set function, Determinantal point process, maximal marginal relevance (MMR) etc.

Keyphrase extraction
The task is the following. You are given a piece of text, such as a journal article, and you must produce a list of keywords or key[phrase]s that capture the primary topics discussed in the text. In the case of research articles, many authors provide manually assigned keywords, but most text lacks pre-existing keyphrases. For example, news articles rarely have keyphrases attached, but it would be useful to be able to automatically do so for a number of applications discussed below.
Consider the example text from a news article:

"The Army Corps of Engineers, rushing to meet President Bush's promise to protect New Orleans by the start of the 2006 hurricane season, installed defective flood-control pumps last year despite warnings from its own expert that the equipment would fail during a storm, according to documents obtained by The Associated Press".A keyphrase extractor might select "Army Corps of Engineers", "President Bush", "New Orleans", and "defective flood-control pumps" as keyphrases. These are pulled directly from the text. In contrast, an abstractive keyphrase system would somehow internalize the content and generate keyphrases that do not appear in the text, but more closely resemble what a human might produce, such as "political negligence" or "inadequate protection from floods". Abstraction requires a deep understanding of the text, which makes it difficult for a computer system.
Keyphrases have many applications. They can enable document browsing by providing a short summary, improve information retrieval (if documents have keyphrases assigned, a user could search by keyphrase to produce more reliable hits than a full-text search), and be employed in generating index entries for a large text corpus.
Depending on the different literature and the definition of key terms, words or phrases, keyword extraction is a highly related theme.

Supervised learning approaches
Beginning with the work of Turney, many researchers have approached keyphrase extraction as a supervised machine learning problem.
Given a document, we construct an example for each unigram, bigram, and trigram found in the text (though other text units are also possible, as discussed below). We then compute various features describing each example (e.g., does the phrase begin with an upper-case letter?). We assume there are known keyphrases available for a set of training documents. Using the known keyphrases, we can assign positive or negative labels to the examples. Then we learn a classifier that can discriminate between positive and negative examples as a function of the features. Some classifiers make a binary classification for a test example, while others assign a probability of being a keyphrase. For instance, in the above text, we might learn a rule that says phrases with initial capital letters are likely to be keyphrases.
After training a learner, we can select keyphrases for test documents in the following manner. We apply the same example-generation strategy to the test documents, then run each example through the learner. We can determine the keyphrases by looking at binary classification decisions or probabilities returned from our learned model. If probabilities are given, a threshold is used to select the keyphrases.
Keyphrase extractors are generally evaluated using precision and recall. Precision measures how
many of the proposed keyphrases are actually correct. Recall measures how many of the true
keyphrases your system proposed. The two measures can be combined in an F-score, which is the
harmonic mean of the two (F = 2PR/(P + R) ). Matches between the proposed keyphrases and the known keyphrases can be checked after stemming or applying some other text normalization.
Designing a supervised keyphrase extraction system involves deciding on several choices (some of these apply to unsupervised, too). The first choice is exactly how to generate examples. Turney and others have used all possible unigrams, bigrams, and trigrams without intervening punctuation and after removing stopwords. Hulth showed that you can get some improvement by selecting examples to be sequences of tokens that match certain patterns of part-of-speech tags. Ideally, the mechanism for generating examples produces all the known labeled keyphrases as candidates, though this is often not the case. For example, if we use only unigrams, bigrams, and trigrams, then we will never be able to extract a known keyphrase containing four words. Thus, recall may suffer. However, generating too many examples can also lead to low precision.
We also need to create features that describe the examples and are informative enough to allow a learning algorithm to discriminate keyphrases from non- keyphrases. Typically features involve various term frequencies (how many times a phrase appears in the current text or in a larger corpus), the length of the example, relative position of the first occurrence, various boolean syntactic features (e.g., contains all caps), etc. The Turney paper used about 12 such features. Hulth uses a reduced set of features, which were found most successful in the KEA (Keyphrase Extraction Algorithm) work derived from Turney’s seminal paper.
In the end, the system will need to return a list of keyphrases for a test document, so we need to have a way to limit the number. Ensemble methods (i.e., using votes from several classifiers) have been used to produce numeric scores that can be thresholded to provide a user-provided number of keyphrases. This is the technique used by Turney with C4.5 decision trees. Hulth used a single binary classifier so the learning algorithm implicitly determines the appropriate number.
Once examples and features are created, we need a way to learn to predict keyphrases. Virtually any supervised learning algorithm could be used, such as decision trees, Naive Bayes, and rule induction. In the case of Turney's GenEx algorithm, a genetic algorithm is used to learn parameters for a domain-specific keyphrase extraction algorithm. The extractor follows a series of heuristics to identify keyphrases. The genetic algorithm optimizes parameters for these heuristics with respect to performance on training documents with known key phrases.

Unsupervised approach: TextRank
Another keyphrase extraction algorithm is TextRank. While supervised methods have some nice properties, like being able to produce interpretable rules for what features characterize a keyphrase, they also require a large amount of training data. Many documents with known keyphrases are needed. Furthermore, training on a specific domain tends to customize the extraction process to that domain, so the resulting classifier is not necessarily portable, as some of Turney's results demonstrate.
Unsupervised keyphrase extraction removes the need for training data. It approaches the problem from a different angle. Instead of trying to learn explicit features that characterize keyphrases, the TextRank algorithm exploits the structure of the text itself to determine keyphrases that appear "central" to the text in the same way that PageRank selects important Web pages. Recall this is based on the notion of "prestige" or "recommendation" from social networks. In this way, TextRank does not rely on any previous training data at all, but rather can be run on any arbitrary piece of text, and it can produce output simply based on the text's intrinsic properties. Thus the algorithm is easily portable to new domains and languages.
TextRank is a general purpose graph-based ranking algorithm for NLP. Essentially, it runs PageRank on a graph specially designed for a particular NLP task. For keyphrase extraction, it builds a graph using some set of text units as vertices. Edges are based on some measure of semantic or lexical similarity between the text unit vertices. Unlike PageRank, the edges are typically undirected and can be weighted to reflect a degree of similarity. Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the "random surfer model"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph).
The vertices should correspond to what we want to rank. Potentially, we could do something similar to the supervised methods and create a vertex for each unigram, bigram, trigram, etc. However, to keep the graph small, the authors decide to rank individual unigrams in a first step, and then include a second step that merges highly ranked adjacent unigrams to form multi-word phrases. This has a nice side effect of allowing us to produce keyphrases of arbitrary length. For example, if we rank unigrams and find that "advanced", "natural", "language", and "processing" all get high ranks, then we would look at the original text and see that these words appear consecutively and create a final keyphrase using all four together. Note that the unigrams placed in the graph can be filtered by part of speech. The authors found that adjectives and nouns were the best to include. Thus, some linguistic knowledge comes into play in this step.
Edges are created based on word co-occurrence in this application of TextRank. Two vertices are connected by an edge if the unigrams appear within a window of size N in the original text. N is typically around 2–10. Thus, "natural" and "language" might be linked in a text about NLP. "Natural" and "processing" would also be linked because they would both appear in the same string of N words. These edges build on the notion of "text cohesion" and the idea that words that appear near each other are likely related in a meaningful way and "recommend" each other to the reader.
Since this method simply ranks the individual vertices, we need a way to threshold or produce a limited number of keyphrases. The technique chosen is to set a count T to be a user-specified fraction of the total number of vertices in the graph. Then the top T vertices/unigrams are selected based on their stationary probabilities. A post- processing step is then applied to merge adjacent instances of these T unigrams. As a result, potentially more or less than T final keyphrases will be produced, but the number should be roughly proportional to the length of the original text.
It is not initially clear why applying PageRank to a co-occurrence graph would produce useful keyphrases. One way to think about it is the following. A word that appears multiple times throughout a text may have many different co-occurring neighbors. For example, in a text about machine learning, the unigram "learning" might co-occur with "machine", "supervised", "un-supervised", and "semi-supervised" in four different sentences. Thus, the "learning" vertex would be a central "hub" that connects to these other modifying words. Running PageRank/TextRank on the graph is likely to rank "learning" highly. Similarly, if the text contains the phrase "supervised classification", then there would be an edge between "supervised" and "classification". If "classification" appears several other places and thus has many neighbors, its importance would contribute to the importance of "supervised". If it ends up with a high rank, it will be selected as one of the top T unigrams, along with "learning" and probably "classification". In the final post-processing step, we would then end up with keyphrases "supervised learning" and "supervised classification".
In short, the co-occurrence graph will contain densely connected regions for terms that appear often and in different contexts. A random walk on this graph will have a stationary distribution that assigns large probabilities to the terms in the centers of the clusters. This is similar to densely connected Web pages getting ranked highly by PageRank. This approach has also been used in document summarization, considered below.

Document summarization
Like keyphrase extraction, document summarization aims to identify the essence of a text. The only real difference is that now we are dealing with larger text units—whole sentences instead of words and phrases.
Before getting into the details of some summarization methods, we will mention how summarization systems are typically evaluated. The most common way is using the so-called ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measure. This is a recall-based measure that determines how well a system-generated summary covers the content present in one or more human-generated model summaries known as references. It is recall-based to encourage systems to include all the important topics in the text. Recall can be computed with respect to unigram, bigram, trigram, or 4-gram matching. For example, ROUGE-1 is computed as division of count of unigrams in reference that appear in system and count of unigrams in reference summary.
If there are multiple references, the ROUGE-1 scores are averaged. Because ROUGE is based only on content overlap, it can determine if the same general concepts are discussed between an automatic summary and a reference summary, but it cannot determine if the result is coherent or the sentences flow together in a sensible manner. High-order n-gram ROUGE measures try to judge fluency to some degree.
Note that ROUGE is similar to the BLEU measure for machine translation, but BLEU is precision- based, because translation systems favor accuracy.
A promising line in document summarization is adaptive document/text summarization. The idea of adaptive summarization involves preliminary recognition of document/text genre and subsequent application of summarization algorithms optimized for this genre. First summarizes that perform adaptive summarization have been created.

Supervised learning approaches
Supervised text summarization is very much like supervised keyphrase extraction. Basically, if you have a collection of documents and human-generated summaries for them, you can learn features of sentences that make them good candidates for inclusion in the summary. Features might include the position in the document (i.e., the first few sentences are probably important), the number of words in the sentence, etc. The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as "in summary" or "not in summary". This is not typically how people create summaries, so simply using journal abstracts or existing summaries is usually not sufficient. The sentences in these summaries do not necessarily match up with sentences in the original text, so it would be difficult to assign labels to examples for training. Note, however, that these natural summaries can still be used for evaluation purposes, since ROUGE-1 only cares about unigrams.

Maximum entropy-based summarization
During the DUC 2001 and 2002 evaluation workshops, TNO developed a sentence extraction system for multi-document summarization in the news domain. The system was based on a hybrid system using a naive Bayes classifier and statistical language models for modeling salience. Although the system exhibited good results, the researchers wanted to explore the effectiveness of a maximum entropy (ME) classifier for the meeting summarization task, as ME is known to be robust against feature dependencies. Maximum entropy has also been applied successfully for summarization in the broadcast news domain.

TextRank and LexRank
The unsupervised approach to summarization is also quite similar in spirit to unsupervised keyphrase extraction and gets around the issue of costly training data. Some unsupervised summarization approaches are based on finding a "centroid" sentence, which is the mean word vector of all the sentences in the document. Then the sentences can be ranked with regard to their similarity to this centroid sentence.
A more principled way to estimate sentence importance is using random walks and eigenvector centrality. LexRank is an algorithm essentially identical to TextRank, and both use this approach for document summarization. The two methods were developed by different groups at the same time, and LexRank simply focused on summarization, but could just as easily be used for keyphrase extraction or any other NLP ranking task.
In both LexRank and TextRank, a graph is constructed by creating a vertex for each sentence in the document.
The edges between sentences are based on some form of semantic similarity or content overlap. While LexRank uses cosine similarity of TF-IDF vectors, TextRank uses a very similar measure based on the number of words two sentences have in common (normalized by the sentences' lengths). The LexRank paper explored using unweighted edges after applying a threshold to the cosine values, but also experimented with using edges with weights equal to the similarity score. TextRank uses continuous similarity scores as weights.
In both algorithms, the sentences are ranked by applying PageRank to the resulting graph. A summary is formed by combining the top ranking sentences, using a threshold or length cutoff to limit the size of the summary.
It is worth noting that TextRank was applied to summarization exactly as described here, while LexRank was used as part of a larger summarization system (MEAD) that combines the LexRank score (stationary probability) with other features like sentence position and length using a linear combination with either user-specified or automatically tuned weights. In this case, some training documents might be needed, though the TextRank results show the additional features are not absolutely necessary.
Another important distinction is that TextRank was used for single document summarization, while LexRank has been applied to multi-document summarization. The task remains the same in both cases—only the number of sentences to choose from has grown. However, when summarizing multiple documents, there is a greater risk of selecting duplicate or highly redundant sentences to place in the same summary. Imagine you have a cluster of news articles on a particular event, and you want to produce one summary. Each article is likely to have many similar sentences, and you would only want to include distinct ideas in the summary. To address this issue, LexRank applies a heuristic post-processing step that builds up a summary by adding sentences in rank order, but discards any sentences that are too similar to ones already placed in the summary. The method used is called Cross-Sentence Information Subsumption (CSIS).
These methods work based on the idea that sentences "recommend" other similar sentences to the reader. Thus, if one sentence is very similar to many others, it will likely be a sentence of great importance. The importance of this sentence also stems from the importance of the sentences "recommending" it. Thus, to get ranked highly and placed in a summary, a sentence must be similar to many sentences that are in turn also similar to many other sentences. This makes intuitive sense and allows the algorithms to be applied to any arbitrary new text. The methods are domain-independent and easily portable. One could imagine the features indicating important sentences in the news domain might vary considerably from the biomedical domain. However, the unsupervised "recommendation"-based approach applies to any domain.

Multi-document summarization
Multi-document summarization is an automatic procedure aimed at extraction of information from multiple texts written about the same topic. Resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents. In such a way, multi-document summarization systems are complementing the news aggregators performing the next step down the road of coping with information overload. Multi-document summarization may also be done in response to a question.Multi-document summarization creates information reports that are both concise and comprehensive. With different opinions being put together and outlined, every topic is described from multiple perspectives within a single document. While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required.
Automatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased.

Incorporating diversity
Multi-document extractive summarization faces a problem of potential redundancy. Ideally, we would like to extract sentences that are both "central" (i.e., contain the main ideas) and "diverse" (i.e., they differ from one another). LexRank deals with diversity as a heuristic final stage using CSIS, and other systems have used similar methods, such as Maximal Marginal Relevance (MMR), in trying to eliminate redundancy in information retrieval results. There is a general purpose graph-based ranking algorithm like Page/Lex/TextRank that handles both "centrality" and "diversity" in a unified mathematical framework based on absorbing Markov chain random walks. (An absorbing random walk is like a standard random walk, except some states are now absorbing states that act as "black holes" that cause the walk to end abruptly at that state.) The algorithm is called GRASSHOPPER. In addition to explicitly promoting diversity during the ranking process, GRASSHOPPER incorporates a prior ranking (based on sentence position in the case of summarization).
The state of the art results for multi-document summarization, however, are obtained using mixtures of submodular functions. These methods have achieved the state of the art results for Document Summarization Corpora, DUC 04 - 07. Similar results were also achieved with the use of determinantal point processes (which are a special case of submodular functions) for DUC-04.A new method for multi-lingual multi-document summarization that avoids redundancy works by simplifying and generating ideograms that represent the meaning of each sentence in each document and then evaluates similarity "qualitatively" by comparing the shape and position of said ideograms has recently been developed. This tool does not use word frequency, does not need training or preprocessing of any kind and works by generating ideograms that represent the meaning of each sentence and then summarizes using two user-supplied parameters: equivalence (when are two sentences to be considered equivalent) and relevance (how long is the desired summary).

Submodular functions as generic tools for summarization
The idea of a submodular set function has recently emerged as a powerful modeling tool for various summarization problems. Submodular functions naturally model notions of coverage, information, representation and diversity. Moreover, several important combinatorial optimization problems occur as special instances of submodular optimization. For example, the set cover problem is a special case of submodular optimization, since the set cover function is submodular. The set cover function attempts to find a subset of objects which cover a given set of concepts. For example, in document summarization, one would like the summary to cover all important and relevant concepts in the document. This is an instance of set cover. Similarly, the facility location problem is a special case of submodular functions. The Facility Location function also naturally models coverage and diversity. Another example of a submodular optimization problem is using a determinantal point process to model diversity. Similarly, the Maximum-Marginal-Relevance procedure can also be seen as an instance of submodular optimization. All these important models encouraging coverage, diversity and information are all submodular. Moreover, submodular functions can be efficiently combined together, and the resulting function is still submodular. Hence, one could combine one submodular function which models diversity, another one which models coverage and use human supervision to learn a right model of a submodular function for the problem.
While submodular functions are fitting problems for summarization, they also admit very efficient algorithms for optimization. For example, a simple greedy algorithm admits a constant factor guarantee. Moreover, the greedy algorithm is extremely simple to implement and can scale to large datasets, which is very important for summarization problems.
Submodular functions have achieved state-of-the-art for almost all summarization problems. For example, work by Lin and Bilmes, 2012 shows that submodular functions achieve the best results to date on DUC-04, DUC-05, DUC-06 and DUC-07 systems for document summarization. Similarly, work by Lin and Bilmes, 2011, shows that many existing systems for automatic summarization are instances of submodular functions. This was a breakthrough result establishing submodular functions as the right models for summarization problems.Submodular Functions have also been used for other summarization tasks. Tschiatschek et al., 2014 show that mixtures of submodular functions achieve state-of-the-art results for image collection summarization. Similarly, Bairi et al., 2015 show the utility of submodular functions for summarizing multi-document topic hierarchies. Submodular Functions have also successfully been used for summarizing machine learning datasets.

Applications
Specific applications of automatic summarization include:

The Reddit bot "autotldr", created in 2011 summarizes news articles in the comment-section of reddit posts. It was found to be very useful by the reddit community which upvoted its summaries hundreds of thousands of times. The name is reference to TL;DR − Internet slang for "too long; didn't read".

Evaluation techniques
The most common way to evaluate the informativeness of automatic summaries is to compare them with human-made model summaries.
Evaluation techniques fall into intrinsic and extrinsic, inter-textual and intra-textual.

Intrinsic and extrinsic evaluation
An intrinsic evaluation tests the summarization system in and of itself while an extrinsic evaluation tests the summarization based on how it affects the completion of some other task. Intrinsic evaluations have
assessed mainly the coherence and informativeness of summaries. Extrinsic evaluations, on the other hand, have tested the impact of summarization on tasks like relevance assessment, reading comprehension, etc.

Inter-textual and intra-textual
Intra-textual methods assess the output of a specific summarization system, and the inter-textual ones focus on contrastive analysis of outputs of several summarization systems.
Human judgement often has wide variance on what is considered a "good" summary, which means that making the evaluation process automatic is particularly difficult. Manual evaluation can be used, but this is both time and labor-intensive as it requires humans to read not only the summaries but also the source documents. Other issues are those concerning coherence and coverage.
One of the metrics used in NIST's annual Document Understanding Conferences, in which research groups submit their systems for both summarization and translation tasks, is the ROUGE metric (Recall-Oriented Understudy for Gisting Evaluation [2]). It essentially calculates n-gram overlaps between automatically generated summaries and previously-written human summaries. A high level of overlap should indicate a high level of shared concepts between the two summaries. Note that overlap metrics like this are unable to provide any feedback on a summary's coherence. Anaphor resolution remains another problem yet to be fully solved. Similarly, for image summarization, Tschiatschek et al., developed a Visual-ROUGE score which judges the performance of algorithms for image summarization.

Domain specific versus domain independent summarization techniques
Domain independent summarization techniques generally apply sets of general features which can be used to identify information-rich text segments. Recent research focus has drifted to domain-specific summarization techniques that utilize the available knowledge specific to the domain of text. For example, automatic summarization research on medical text generally attempts to utilize the various sources of codified medical knowledge and ontologies.

Evaluating summaries qualitatively
The main drawback of the evaluation systems existing so far is that we need at least one reference summary, and for some methods more than one, to be able to compare automatic summaries with models. This is a hard and expensive task. Much effort has to be done in order to have corpus of texts and their corresponding summaries. Furthermore, for some methods, not only do we need to have human-made summaries available for comparison, but also manual annotation has to be performed in some of them (e.g. SCU in the Pyramid Method). In any case, what the evaluation methods need as an input, is a set of summaries to serve as gold standards and a set of automatic summaries. Moreover, they all perform a quantitative evaluation with regard to different similarity metrics.

See also
Sentence extraction
Text mining
Multi-document summarization

References
Further reading
Hercules, Dalianis (2003). Porting and evaluation of automatic summarization.
Roxana, Angheluta (2002). The Use of Topic Segmentation for Automatic Summarization.
Anne, Buist (2004). Automatic Summarization of Meeting Data: A Feasibility Study (PDF).
Annie, Louis (2009). Performance Confidence Estimation for Automatic Summarization.
Elena, Lloret and Manuel, Palomar (2009). Challenging Issues of Automatic Summarization: Relevance Detection and Quality-based Evaluation.
Andrew, Goldberg (2007). Automatic Summarization.
Alrehamy, Hassan (2017). "SemCluster: Unsupervised Automatic Keyphrase Extraction Using Affinity Propagation". Automatic Keyphrases Extraction. Advances in Intelligent Systems and Computing. 650. pp. 222–235. doi:10.1007/978-3-319-66939-7_19. ISBN 978-3-319-66938-0.
Endres-Niggemeyer, Brigitte (1998). Summarizing Information. ISBN 978-3-540-63735-6.
Marcu, Daniel (2000). The Theory and Practice of Discourse Parsing and Summarization. ISBN 978-0-262-13372-2.
Mani, Inderjeet (2001). Automatic Summarization. ISBN 978-1-58811-060-2.
Huff, Jason (2010). AutoSummarize., Conceptual artwork using automatic summarization software in Microsoft Word 2008.
Lehmam, Abderrafih (2010). Essential summarizer: innovative automatic text summarization software in twenty languages - ACM Digital Library. Riao '10. pp. 216–217., Published in Proceeding RIAO'10 Adaptivity, Personalization and Fusion of Heterogeneous Information, CID Paris, France
Xiaojin, Zhu, Andrew Goldberg, Jurgen Van Gael, and David Andrzejewski (2007). Improving diversity in ranking using absorbing random walks (PDF)., The GRASSHOPPER algorithm
Miranda-Jiménez, Sabino, Gelbukh, Alexander, and Sidorov, Grigori (2013). "Summarizing Conceptual Graphs for Automatic Summarization Task". Conceptual Structures for STEM Research and Education. Lecture Notes in Computer Science. 7735. pp. 245–253. doi:10.1007/978-3-642-35786-2_18. ISBN 978-3-642-35785-5., Conceptual Structures for STEM Research and Education.
Automatic taxonomy construction (ATC) is the use of software programs to generate taxonomical classifications from a body of texts called a corpus. ATC is a branch of natural language processing, which in turn is a branch of artificial intelligence.
Among other things, a taxonomy can be used to organize and index knowledge (stored as documents, articles, videos, etc.), such as in the form of a library classification system, or a search engine taxonomy, so that users can more easily find the information they are searching for. Taxonomies are typically tree structured and divide a domain (subject, field, or set of things the taxonomy represents) into categories based on the value of properties called taxa.
Manually developing and maintaining a taxonomy is a labor-intensive task requiring significant time and resources, including familiarity of or expertise in the taxonomy's domain. Also, domain modelers have their own points of view which inevitably, even if unintentionally, work their way into the taxonomy. ATC uses artificial intelligence techniques to automatically generate a taxonomy for a domain in order to avoid these problems.

Approaches
There are several approaches to ATC. One approach is to use rules to detect patterns in the corpus and use those patterns to infer relations such as hyponymy. Other approaches use machine learning techniques such as Bayesian inferencing and Artificial Neural Networks.

Keyword extraction
One approach to building a taxonomy is to automatically gather the keywords from a domain using keyword extraction, then analyze the relationships between them (see Hyponymy, below), and then arrange them as a taxonomy based on those relationships.

Hyponymy and "is-a" relations
In ATC programs, one of the most important tasks is the discovery of hypernym and hyponym relations among words. One way to do that from a body of text is to search for certain phrases like "is a" and "such as".
In linguistics, is-a relations are called hyponymy. Words that describe categories are called hypernyms and words that are examples of categories are hyponyms. For example, dog is a hypernym and Fido is one of its hyponyms. A word can be both a hyponym and a hypernym. So, dog is a hyponym of mammal and also a hypernym of Fido.
Taxonomies are often represented as is-a hierarchies where each level is more specific (in mathematical language "a subset of") the level above it. For example, a basic biology taxonomy would have concepts such as mammal, which is a subset of animal, and dogs and cats, which are subsets of mammal. This kind of taxonomy is called an is-a model because the specific objects are considered instances of a concept. For example, Fido is-a instance of the concept dog and Fluffy is-a cat.

Applications
ATC can be used to build  taxonomies for search engines, to improve search results.
ATC systems are a key component of ontology learning (also known as automatic ontology construction), and have been used to automatically generate large ontologies for domains such as insurance and finance. They have also been used to enhance existing large networks such as Wordnet to make them more complete and consistent.

ATC software
Other names
Other names for automatic taxonomy construction include:

Taxonomy generation
Automatic taxonomy generation
Taxonomy learning
Automatic taxonomy learning
Taxonomy extraction
Automatic taxonomy extraction
Taxonomy building
Automatic taxonomy building
Taxonomy induction
Automatic taxonomy induction
Semantic taxonomy induction

See also
Document classification
Information extraction

References
Further reading
Automatic Taxonomy Construction from Keywords
Domain taxonomy learning from text: The subsumption method versus hierarchical clustering from Data & Knowledge Engineering, Volume 83, January 2013, Pages 54–69Learning taxonomic relations from a set of text documents
Learning Taxonomic Relations from Heterogeneous Sources of Evidence
A Metric-based Framework for Automatic Taxonomy Induction
A New Method for Evaluating Automatically Learned Terminological Taxonomies
Problematizing and Addressing the Article-as-Concept Assumption in Wikipedia 
Structured Learning for Taxonomy Induction with Belief Propagation
Taxonomy Learning Using Word Sense Induction

External links
Taxonomy 101: The Basics and Getting Started with Taxonomies – shows where ATC fits in to the general activity of managing taxonomies for a business enterprise in need of knowledge management.
Averbis has a focus on healthcare, pharma, automotive and intellectual property analytics. Averbis is involved in various research projects of the German Federal Ministry of Economics and Energy and the European Union such as DebugIT, EUCases, Mantra  and SEMCARE.In addition to these projects, Averbis was also involved in the following projects:
Greenpilot is a virtual library, which provides technical information in the fields of nutrition, environment and agriculture.
Medpilot is a virtual library, which provides information about medicine and related sciences.
In 2013, Averbis has been nominated for the German Founder Prize 2013.Averbis GmbH provides text analytics and text mining software to transform unstructured text into actionable information. It was founded in 2007 by IT experts after years of relevant scientific experience in the field of text mining and multilingual information retrieval. Averbis works in the field of terminology management, natural language processing, machine learning and semantic search. Its text mining software is embedded into the text mining framework UIMA.

See also
Enterprise Search
Information retrieval
Linguistics
Knowledge Management
Natural Language Processing
Semantics

References
External links
Official website
Technology Competition Federal Ministry of Economy and Energy
BMWi Research Project Trusted Cloud
BMWi Research Project cloud4health
The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision.The bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier.An early reference to "bag of words" in a linguistic context can be found in Zellig Harris's 1954 article on Distributional Structure.

Example implementation
The following models a text document using bag-of-words. Here are two simple text documents:

Based on these two text documents, a list is constructed as follows for each document:

Representing each bag-of-words as a JSON object, and attributing to the respective JavaScript variable:

Each key is the word, and each value is the number of occurrences of that word in the given text document.
The order of elements is free, so, for example {"too":1,"Mary":1,"movies":2,"John":1,"watch":1,"likes":2,"to":1} is also equivalent to BoW1. It is also what we expect from a strict JSON object representation.
Note: if another document is like a union of these two, 

its JavaScript representation will be:

So, as we see in the bag algebra, the "union" of two documents in the bags-of-words representation is, formally, the disjoint union, summing the multiplicities of each element.
 
  
    
      
        B
        o
        W
        3
        =
        B
        o
        W
        1
        ⨄
        B
        o
        W
        2
      
    
    {\displaystyle BoW3=BoW1\biguplus BoW2}
  .

Application
In practice, the Bag-of-words model is mainly used as a tool of feature generation. After transforming the text into a "bag of words", we can calculate various measures to characterize the text. The most common type of characteristics, or features calculated from the Bag-of-words model is term frequency, namely, the number of times a term appears in the text. For the example above, we can construct the following two lists to record the term frequencies of all the distinct words (BoW1 and BoW2 ordered as in BoW3):

Each entry of the lists refers to the count of the corresponding entry in the list (this is also the histogram representation). For example, in the first list (which represents document 1), the first two entries are "1,2":

The first entry corresponds to the word "John" which is the first word in the list, and its value is "1" because "John" appears in the first document once.
The second entry corresponds to the word "likes", which is the second word in the list, and its value is "2" because "likes" appears in the first document twice.This list (or vector) representation does not preserve the order of the words in the original sentences. This is just the main feature of the Bag-of-words model. This kind of representation has several successful applications, such as email filtering.However, term frequencies are not necessarily the best representation for the text. Common words like "the", "a", "to" are almost always the terms with highest frequency in the text. Thus, having a high raw count does not necessarily mean that the corresponding word is more important. To address this problem, one of the most popular ways to "normalize" the term frequencies is to weight a term by the inverse of document frequency, or tf–idf. Additionally, for the specific purpose of classification, supervised alternatives have been developed to account for the class label of a document. Lastly, binary (presence/absence or 1/0) weighting is used in place of frequencies for some problems (e.g., this option is implemented in the WEKA machine learning software system).

n-gram model
The Bag-of-words model is an orderless document representation — only the counts of words matter. For instance, in the above example "John likes to watch movies. Mary likes movies too", the bag-of-words representation will not reveal that the verb "likes" always follows a person's name in this text. As an alternative, the n-gram model can store this spatial information. Applying to the same example above, a bigram model will parse the text into the following units and store the term frequency of each unit as before.

Conceptually, we can view bag-of-word model as a special case of the n-gram model, with n=1. For n>1 the model is named w-shingling (where w is equivalent to n denoting the number of grouped words). See language model for a more detailed discussion.

Python implementation
Hashing trick
A common alternative to using dictionaries is the hashing trick, where words are mapped directly to indices with a hashing function. Thus, no memory is required to store a dictionary. Hash collisions are typically dealt via freed-up memory to increase the number of hash buckets. In practice, hashing simplifies the implementation of bag-of-words models and improves scalability.

Example usage: spam filtering
In Bayesian spam filtering, an e-mail message is modeled as an unordered collection of words selected from one of two probability distributions: one representing spam and one representing legitimate e-mail ("ham"). 
Imagine there are two literal bags full of words. One bag is filled with words found in spam messages, and the other with words found in legitimate e-mail. While any given word is likely to be somewhere in both bags, the "spam" bag will contain spam-related words such as "stock", "Viagra", and "buy" significantly more frequently, while the "ham" bag will contain more words related to the user's friends or workplace.
To classify an e-mail message, the Bayesian spam filter assumes that the message is a pile of words that has been poured out randomly from one of the two bags, and uses Bayesian probability to determine which bag it is more likely to be in.

See also
Additive smoothing
Bag-of-words model in computer vision
Document classification
Document-term matrix
Feature extraction
Hashing trick
Machine learning
MinHash
n-gram
Natural language processing
Vector space model
w-shingling
tf-idf

Notes
References
McTear, Michael (et al) (2016). The Conversational Interface. Springer International Publishing.
Bidirectional Encoder Representations from Transformers (BERT) is a technique for NLP (Natural Language Processing) pre-training developed by Google. BERT was created and published in 2018 by Jacob Devlin and his colleagues from Google. Google is leveraging BERT to better understand user searches.

Performance
When BERT was published, it achieved state-of-the-art performance on a number of natural language understanding tasks:
GLUE (General Language Understanding Evaluation) task set (consisting of 9 tasks)
SQuAD (Stanford Question Answering Dataset) v1.1 and v2.0.
SWAG (Situations With Adversarial Generations)

Analysis
The reasons for BERT's state-of-the-art performance on these natural language understanding tasks are not yet well understood. Current research has focused on investigating the relationship behind BERT's output as a result of carefully chosen input sequences, analysis of internal vector representations through probing classifiers, and the relationships represented by attention weights.

History
BERT has its origins from pre-training contextual representations including Semi-supervised Sequence Learning, Generative Pre-Training, ELMo, and ULMFit. Unlike previous models, BERT is a deeply bidirectional, unsupervised language representation, pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary, where BERT is deeply bidirectional.On October 25, 2019, Google Search announced that they had started applying BERT models for English language search queries within the US. On December 9, 2019, it was reported that BERT had been adopted by Google Search for over 70 languages.

Recognition
BERT won the Best Long Paper Award at the 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).

See also


== References ==
A bigram or digram is a sequence of two adjacent elements from a string of tokens, which are typically letters, syllables, or words. A bigram is an n-gram for n=2. The frequency distribution of every bigram in a string is commonly used for simple statistical analysis of text in many applications, including in computational linguistics, cryptography, speech recognition, and so on.
Gappy bigrams or skipping bigrams are word pairs which allow gaps (perhaps avoiding connecting words, or allowing some simulation of dependencies, as in a dependency grammar).
Head word bigrams are gappy bigrams with an explicit dependency relationship.
Bigrams help provide the conditional probability of a token given the preceding token, when the relation of the conditional probability is applied:

  
    
      
        P
        (
        
          W
          
            n
          
        
        
          |
        
        
          W
          
            n
            −
            1
          
        
        )
        =
        
          
            
              P
              (
              
                W
                
                  n
                  −
                  1
                
              
              ,
              
                W
                
                  n
                
              
              )
            
            
              P
              (
              
                W
                
                  n
                  −
                  1
                
              
              )
            
          
        
      
    
    {\displaystyle P(W_{n}|W_{n-1})={P(W_{n-1},W_{n}) \over P(W_{n-1})}}
  
That is, the probability 
  
    
      
        P
        (
        )
      
    
    {\displaystyle P()}
   of a token 
  
    
      
        
          W
          
            n
          
        
      
    
    {\displaystyle W_{n}}
   given the preceding token 
  
    
      
        
          W
          
            n
            −
            1
          
        
      
    
    {\displaystyle W_{n-1}}
   is equal to the probability of their bigram, or the co-occurrence of the two tokens 
  
    
      
        P
        (
        
          W
          
            n
            −
            1
          
        
        ,
        
          W
          
            n
          
        
        )
      
    
    {\displaystyle P(W_{n-1},W_{n})}
  , divided by the probability of the preceding token.

Applications
Bigrams are used in most successful language models for speech recognition. They are a special case of N-gram.
Bigram frequency attacks can be used in cryptography to solve cryptograms. See frequency analysis.
Bigram frequency is one approach to statistical language identification.
Some activities in logology or recreational linguistics involve bigrams. These include attempts to find English words beginning with every possible bigram, or words containing a string of repeated bigrams, such as logogogue.

Bigram frequency in the English language
The frequency of the most common letter bigrams in a small English corpus is:
th 1.52       en 0.55       ng 0.18
he 1.28       ed 0.53       of 0.16
in 0.94       to 0.52       al 0.09
er 0.94       it 0.50       de 0.09
an 0.82       ou 0.50       se 0.08
re 0.68       ea 0.47       le 0.08
nd 0.63       hi 0.46       sa 0.06
at 0.59       is 0.46       si 0.05
on 0.57       or 0.43       ar 0.04
nt 0.56       ti 0.34       ve 0.04
ha 0.56       as 0.33       ra 0.04
es 0.56       te 0.27       ld 0.02
st 0.55       et 0.19       ur 0.02

Complete bigram frequencies for a larger corpus are available.

See also
Digraph (orthography)
N-gram
Letter frequency
Sørensen–Dice coefficient


== References ==
Bottlenose.com, also known as Bottlenose, is an enterprise trend intelligence company that analyzes big data and business data to detect trends for brands. It helps Fortune 500 enterprises discover and track emerging trends that affect their brands. The company uses natural language processing, sentiment analysis, statistical algorithms, data mining and machine learning heuristics to determine trends, and has a search engine that gathers information from social networks. KPMG Capital has invested a "substantial amount" in the company.Bottlenose processed 72 billion messages per day, in real-time, from across social and broadcast (radio and television) media, as of December 2014.

History
In 2010, Nova Spivack and Dominiek ter Heide co-founded Bottlenose with a team of web engineers. The company is based in Los Angeles, CA. Bottlenose is a real-time trend intelligence tool that measures social media campaigns and trends. The company also provides a free version of its Sonar tool that shows real-time trends across social media.In October 2012, the company received $1 million of funding from ff Venture Capital and Prosper Capital. By 2014, the company raised about $7 million in funding. In December 2014, KPMG Capital announced further investment in the company. In February 2015 the company confirmed it had raised $13.4 million in Series B funding led by KPMG Capital.Bottlenose partnered with the nonprofit No Labels during the 2014 State of the Union Address to analyze Twitter conversations for bipartisanship. The company also partnered with media monitoring company Critical Mention to analyze broadcast analytics. The Bottlenose Nerve Center integrated with the Critical Mention API to analyze real-time trends in television and radio broadcasts.In June 2014, Bottlenose updated its trend detection product to Nerve Center 2.0. It creates a newsfeed to show changes in trends and sends alerts when trends occur. It also has "emotion detection," which will displays the emotions associated with specific comments on trending topics. In 2016, Bottlenose released its Nerve Center 3.0 platform, which was designed to automate the work of data scientists and lower the cost of artificial intelligence for businesses.

See also
Sentiment analysis
 Big data analysis

References
External links
Official website
Bottlenose Offers Real-Time Trend Intelligence For Social Media and Beyond
The Brill tagger is an inductive method for part-of-speech tagging. It was described and invented by Eric Brill in his 1993 PhD thesis. It can be summarized as an "error-driven transformation-based tagger". It is: 

a form of supervised learning, which aims to minimize error; and,
a transformation-based process, in the sense that a tag is assigned to each word and changed using a set of predefined rules.In the transformation process, if the word is known, it first assigns the most frequent tag, or if the word is unknown, it naively assigns the tag "noun" to it. Applying over and over these rules, changing the incorrect tags, a quite high accuracy is achieved. This approach ensures that valuable information such as the morphosyntactic construction of words is employed in an automatic tagging process.

Algorithm
The algorithm starts with initialization, which is the assignment of tags based on their probability for each word (for example, "dog" is more often a noun than a verb). Then "patches" are determined via rules that correct (probable) tagging errors made in the initialization phase:
Initialization:
Known words (in vocabulary): assigning the most frequent tag associated to a form of the word
Unknown word

Rules and processing
The input text is first tokenized, or broken into words. Typically
in natural language processing, contractions such as "'s", "n't", and the like are considered 
separate word tokens, as are punctuation marks.
A dictionary and some morphological rules then provide an initial tag for each word token.
For example, a simple lookup would reveal that "dog" may be a noun or a verb (the most frequent tag is simply chosen), while an unknown word will be assigned some tag(s) based on capitalization,
various prefix or suffix strings, etc. (such morphological analyses, which Brill calls Lexical Rules, may vary between implementations).
After all word tokens have (provisional) tags, contextual rules apply iteratively, to correct the tags by examining small amounts of context. This is where the Brill method differs from other part of speech tagging methods such as
those using Hidden Markov Models. Rules are reapplied repeatedly, until a threshold is reached, or no more rules can apply.
Brill rules are of the general form:

   tag1 → tag2 IF Condition

where the Condition tests the preceding and/or following word tokens, or their tags (the notation for such rules differs between implementations). For example, in Brill's notation:

   IN NN WDPREVTAG DT while

would change the tag of a word from IN (preposition) to NN (common noun), if the preceding word's tag is DT (determiner) and the word itself is "while". This covers cases like "all the while" or "in a while", where "while" should be tagged as a noun rather than its more common use as a preposition (many rules are more general).
Rules should only operate if the tag being changed is also known to be permissible, for the word in question or in principle (for example, most adjectives in English can also be used as nouns).
Rules of this kind can be implemented by simple Finite-state machines.
See Part of speech tagging for more general information including descriptions of the Penn Treebank and other sets of tags.
Typical Brill taggers use a few hundred rules, which may be developed by linguistic intuition or by machine learning on a pre-tagged corpus.

Code
Brill's code pages at Johns Hopkins University are no longer on the web. An archived version of a mirror of the Brill tagger at its latest version as it was available at Plymouth Tech can be found on Archive.org.  The software uses the MIT License.

References
External links
Brill tagger trained for Dutch (online and offline version)
Brill tagger trained for New Norwegian
Brill tagger trained for Danish (online demo)
Brill tagger trained for English (online demo)
taggerXML Modernized version of Eric Brill's Part Of Speech tagger (source code of the Danish and English versions above)
The Bulgarian Sense-annotated Corpus (BulSemCor) (Bulgarian: Български семантично анотиран корпус (БулСемКор)) is a structured corpus of Bulgarian texts in which each lexical item is assigned a sense tag. BulSemCor was created by the Department of Computational Linguistics at the Institute for Bulgarian Language of the Bulgarian Academy of Sciences.

Structure
BulSemCor was created as part of a nationally funded project titled "BulNet – A lexico-semantic network for the Bulgarian Language" (2005–2010). It follows the general methodology of SemCor combined with some specific principles. The corpus for annotation consists of 101,791 tokens covering an excerpt from the Bulgarian "Brown" Corpus modelled on the Brown Corpus.Francis Kucera An important feature of BulSemCor is that the samples are selected using heuristics that provide optimal coverage of ambiguous lexis.
BulSemCor is manually sense-annotated according to the Bulgarian WordNet. Its size is comparable to that of other contemporary semantically annotated corpora or pool of acceptable linguistic components. The semantic annotation consists in associating each lexical item in the corpus with exactly one synonym set (synset) in the Bulgarian WordNet that best describes its sense in the particular context. The selection of the best match among the suggested candidates is based on a set of procedures, such as the other synset members, the synset gloss (explanatory definition) and the position of a given candidate in the WordNet structure.

Scale
The number of annotated tokens is 99,480 (the difference in the number of tokens compared to the initial corpus is due to the fact that some of them are not linguistic items). The simple word count is 86,842 and multiword expressions (MWE) are 5,797 (12,638 tokens).

Specific features
All words in BulSemCor are assigned a sense, while according to established practice only simple content words or content word classes (typically nouns and verbs) are annotated. Since 2000 the development of language resources, has broadened to include annotation of function words and multiword expressions covering particular senses or types of words and expressions. In this respect, BulSemCor's annotation is more exhaustive and hence provides greater opportunities for linguistic observations and non-linear programming (NLP) applications.
Annotated items inherit the linguistic information associated with the corresponding synset, which along with morphological and semantic tags may include annotation on one or more of the following additional levels:
Partial information about the syntactic structure of MWE types – particularly, information about syntactic heads and their dependents;
Information about the category of the named entities – names, locations, organisations, dates, numbers, etc.;
Information about the taxonomic category of adverbs, such as time, place, manner, degree, quantity, etc.;
Information about the type of the syntactic relationships – coordination or subordination – expressed by conjunctions;
Information about the original part-of-speech of substantivised words (non-nouns that act as nouns in a particular context);
Stylistic/register, grammatical and other information about synsets or individual synset members;

See also
Corpus linguistics
Natural language processing
Bulgarian National Corpus
Bulgarian WordNet
BulPosCor

References
Koeva, Svetla (2010). "Balgarskiyat semantichno anotiran korpus" [The Bulgarian Sense-annotated Corpus].CS1 maint: ref=harv (link)
Koeva, Svetla; Leseva, S.; Todorova, M. (May 23, 2006). Bulgarian Sense Tagged Corpus. 5th SALTMIL Workshop on Minority Languages: Strategies for Developing Machine Translation for Minority Languages. pp. 79–87.CS1 maint: ref=harv (link)
Miller, G. A. (1995). "Building Semantic Concordances: Disambiguation vs. Annotation AAAI Technical Report SS-95-01" (PDF): 92–94. CS1 maint: ref=harv (link)
Todorova, M.; Kukova, H.; Leseva, S. (2014). Semantichno anotirani resursi za balgarskiya ezik – BulSemCor (Semantically-annotated Resources for Bulgarian – BulSemCor) [Language Resources and Technologies for Bulgarian]. Ezikovi resursi i tehnologii za balgarski ezik. Academic Publishing House. pp. 80–104. ISBN 978-954-322-797-6.CS1 maint: ref=harv (link)
Francis, N.; Kucera, H. (1979), Manual of Information to Accompany a Standard Sample of Present-day Edited American English, for Use with Digital Computers, Providence, Rhode Island: Department of Linguistics, Brown University, archived from the original on May 18, 2014, retrieved July 7, 2013CS1 maint: ref=harv (link)

External links
BulSemCor Search Interface
BulSemCor in META-SHARE
BulNet in META-SHARE
Department of Computational Linguistics
A cache language model is a type of statistical language model. These occur in the natural language processing subfield of computer science and assign probabilities to given sequences of words by means of a probability distribution. Statistical language models are key components of  speech recognition systems and of many machine translation systems: they tell such systems which possible output word sequences are probable and which are improbable. The particular characteristic of a cache language model is that it contains a cache component and assigns relatively high probabilities to words or word sequences that occur elsewhere in a given text. The primary, but by no means sole, use of cache language models is in speech recognition systems.To understand why it is a good idea for a statistical language model to contain a cache component one might consider someone who is dictating a letter about elephants to a speech recognition system. Standard (non-cache) N-gram language models will assign a very low probability to the word "elephant" because it is a very rare word in English. If the speech recognition system does not contain a cache component the person dictating the letter may be annoyed: each time the word "elephant" is spoken another sequence of words with a higher probability according to the N-gram language model may be recognized (e.g., "tell a plan"). These erroneous sequences will have to be deleted manually and replaced in the text by "elephant" each time "elephant" is spoken. If the system has a cache language model, "elephant" will still probably be misrecognized the first time it is spoken and will have to be entered into the text manually; however, from this point on the system is aware that "elephant" is likely to occur again – the estimated probability of occurrence of "elephant" has been increased, making it more likely that if it is spoken it will be recognized correctly. Once "elephant" has occurred several times the system is likely to recognize it correctly every time it is spoken until the letter has been completely dictated. This increase in the probability assigned to the occurrence of "elephant" is an example of a consequence of machine learning and more specifically of pattern recognition.
There exist variants of the cache language model in which not only single words but also multi-word sequences that have occurred previously are assigned higher probabilities (e.g., if "San Francisco" occurred near the beginning of the text subsequent instances of it would be assigned a higher probability).
The cache language model was first proposed in a paper published in 1990, after which the IBM speech-recognition group experimented with the concept. The group found that implementation of a form of cache language model yielded a 24% drop in word-error rates once the first few hundred words of a document had been dictated. A detailed survey of language modeling techniques concluded that the cache language model was one of the few new language modeling techniques that yielded improvements over the standard N-gram approach: "Our caching results show that caching is by far the most useful technique for perplexity reduction at small and medium training data sizes".The development of the cache language model has generated considerable interest among those concerned with computational linguistics in general and statistical natural language processing in particular: recently there has been interest in applying the cache language model in the field of statistical machine translation.The success of the cache language model in improving word prediction rests on the human tendency to use words in a "bursty" fashion: when one is discussing a certain topic in a certain context the frequency with which one uses certain words will be quite different from their frequencies when one is discussing other topics in other contexts. The traditional N-gram language models, which rely entirely on information from a very small number (four, three, or two) of words preceding the word to which a probability is to be assigned, do not adequately model this "burstiness".
Recently, the cache language model concept - originally conceived for the N-gram statistical language model paradigm - has been adapted for use in the neural paradigm. For instance, recent work on continuous cache language models in the recurrent neural network (RNN) setting has applied the cache concept to much larger contexts than before, yielding significant reductions in perplexity
. Another recent line of research involves incorporating a cache component in a feed-forward neural language model (FN-LM) to achieve rapid domain adaptation 
.

See also
Artificial intelligence
History of natural language processing
History of machine translation
Speech recognition
Statistical machine translation

References
Further reading
Jelinek, Frederick (1997). Statistical Methods for Speech Recognition. The MIT Press. ISBN 0-262-10066-5. Archived from the original on 2011-08-05. Retrieved 2011-09-24.
Calais is a service by Thomson Reuters that automatically extracts semantic information from web pages in a format that can be used on the semantic web. Calais was launched in January 2008, and is free to use.The Calais Web service reads unstructured text and returns Resource Description Framework formatted results identifying entities, facts and events within the text. The service appears to be based on technology acquired when Reuters purchased ClearForest in 2007.The technology has also been used to automatically tag blog articles and organize museum collections.Calais uses natural language processing technologies delivered via a web service interface.

References
External links
Official website
Curry, E.; Freitas, A.; O’Riáin, S. (2010). "The Role of Community-Driven Data Curation for Enterprises" (PDF).  In Wood, D. (ed.). Linking Enterprise Data. Preprint. Boston, MA: Springer US. pp. 25–47. doi:10.1007/978-1-4419-7665-9. ISBN 978-1441976642. Case Study of Open Calais
ChaSen  is a morphological parser for the Japanese language. This tool for analyzing morphemes was developed at the Matsumoto laboratory, Nara Institute of Science and Technology.

See also
MeCab

External links
ChaSen home page
Nara Institute of Science and Technology Matsumoto Laboratory
ChatScript is a combination  Natural Language engine and  dialog management system designed initially for creating chatbots, but is currently also used for various forms of NL processing. It is written in C++. The engine is an open source project at SourceForge. and GitHub.ChatScript was  written by Bruce Wilcox  and originally released in 2011, after Suzette (written in ChatScript) won the 2010 Loebner Prize, fooling one of four human judges.

Features
In general ChatScript aims to author extremely concisely, since the limiting scalability of hand-authored chatbots is how much/fast one can write the script.
Because ChatScript is designed for interactive conversation, it automatically maintains user state across volleys. A volley is any number of sentences the user inputs at once and the chatbots response.
The basic element of scripting is the rule. A rule consists of a type, a label (optional), a pattern, and an output. There are three types of rules. Gambits are something a chatbot might say when it has control of the conversation. Rejoinders are rules that respond to a user remark tied to what the chatbot just said. Responders  are rules that respond to arbitrary user input which is not necessarily tied to what the chatbot just said. Patterns describe conditions under which a rule may fire. Patterns range from extremely simplistic to deeply complex (analogous to Regex but aimed for NL).  Heavy use is typically made of concept sets, which are lists of words sharing a meaning. ChatScript contains some 2000 predefined concepts and scripters can easily write their own. Output of a rule intermixes literal words to be sent to the user along with common C-style programming code.
Rules are bundled into collections called topics. Topics can have keywords, which allows the engine to automatically search the topic for relevant rules based on user input.

Example code
Words starting with ~ are concept sets. For example, ~fruit is the list of all known fruits.  The simple pattern (~fruit) reacts if any fruit is mentioned immediately after the chatbot asks for favorite food. The slightly more complex pattern for the rule labelled WHATMUSIC requires all the words what, music, you and any word or phrase meaning to like, but they may occur in any order.  Responders come in three types.  ?: rules react to user questions.  s: rules react to user statements.  u: rules react to either.
ChatScript code supports  standard if-else, loops, user-defined functions and calls, and variable assignment and access.

Data
Some data in ChatScript is transient, meaning it will disappear at the end of the current volley. Other data is permanent, lasting forever until explicitly killed off.  Data can be local to a single user or shared across all users at the bot level.  
Internally all data is represented as text and is automatically converted to a numeric form as needed.

Variables
User variables come in several kinds. Variables purely local to a topic or function are transient. Global variables can be declared as transient or permanent.  A variable is generally declared merely by using it, and its type depends on its prefix ($, $$, $_).

Facts
In addition to variables, ChatScript supports facts – triples of data, which can also be transient or permanent. Functions can query for facts having particular values of some of the fields, making them act like an in-memory database.  Fact retrieval is very quick and efficient the number of available in-memory facts is largely constrained to the available memory of the machine running the ChatScript engine.  Facts can represent record structures and are how ChatScript represents JSON internally. Tables of information can be defined to generate appropriate facts.

The above table links people to what they invented (1 per line) with Einstein getting a list of things he did.

External communication
ChatScript embeds the Curl library and can directly read and write facts in JSON to a website.

Server
A ChatScript engine can run in local or server mode.

Pos-tagging, parsing, and ontology
ChatScript comes with a copy of English WordNet embedded within, including its ontology, and creates and extends its own ontology via concept declarations. It has an English language pos-tagger and parser and supports integration with TreeTagger for pos-tagging a number of other languages (TreeTagger commercial license required).

Databases
In addition to an internal fact database, ChatScript supports PostgreSQL and MongoDB both for access by scripts, but also as a central filesystem if desired so ChatScript can be scaled horizontally.  A common use case is to use a centralized database to host the user files and multiple servers to run the ChatScript engine.

JavaScript
ChatScript also embeds DukTape, ECMAScript E5/E5.1 compatibility, with some semantics updated from ES2015+.

Control Flow
A chatbot's control flow is managed by the control script. This is merely another ordinary topic of rules, that invokes API functions of the engine.  Thus control is fully configurable by the scripter (and functions exist to allow introspection into the engine).  There are pre-processing control flow and post-processing control flow options available, for special processing.


== References ==
Classic monolingual Word Sense Disambiguation evaluation tasks uses WordNet as its sense inventory and is largely based on supervised / semi-supervised classification with the manually sense annotated corpora:
Classic English WSD uses the Princeton WordNet as it sense inventory and the primary classification input is normally based on the SemCor corpus.
Classical WSD for other languages uses their respective WordNet as sense inventories and sense annotated corpora tagged in their respective languages. Often researchers will also tapped on the SemCor corpus and aligned bitexts with English as its source language

Sense inventories
During the first Senseval workshop the HECTOR sense inventory was adopted. The reason for adopting a previously unknown sense inventory was mainly to avoid the use of popular fine-grained word senses (such as WordNet), which could make the experiments unfair or biased. However, given the lack of coverage of such inventories, since the second Senseval workshop the WordNet sense inventory has been adopted.
WSD exercises require a dictionary, to specify the word senses which are to be disambiguated, and a corpus of language data to be disambiguated. WordNet is the most popular example of sense inventory. The reason for adopting the HECTOR database during Senseval-1 was that the WordNet inventory was already publicly available.

Task Description
Comparison of methods can be divided in 2 groups by amount of words to test. The difference consists in the amount of analysis and processing:

all-words task implies disambiguating all the words of the text
lexical sample consists in disambiguating some previously chosen target words.It is assumed that the former one is more realistic evaluation, although with very laborious testing of results. Initially only the latter was used in evaluation but later the former was included. 
Lexical sample organizers had to choose samples on which the systems were to be tested. A criticism of earlier forays into lexical-sample WSD evaluation is that the lexical sample had been chosen according to the whim of the experimenter (or, to coincide with earlier experimenters' selections). For English Senseval, a sampling frame was devised in which words were classified according to their frequency (in the BNC) and their polysemy level (in WordNet). Also, inclusion POS-tagging problem was a matter of discussion and it was decided that samples should be words with known part of speech and some indeterminants (for ex. 15 noun tasks, 13 verb tasks, 8 adjectives, and 5 indeterminates).
For comparison purposes, known, yet simple, algorithms named baselines are used. These include different variants of Lesk algorithm or most frequent sense algorithm.

Evaluation measures
During the evaluation of WSD systems two main performance measures are used:

Precision: the fraction of system assignments made that are correct
Recall: the fraction of total word instances correctly assigned by a systemIf a system makes an assignment for every word, then precision and recall are the same, and can be called accuracy. This model has been extended to take into account systems that return a set of senses with weights for each occurrence.

See also
Word sense disambiguation
Other variants of WSD evaluations
Word sense
WordNet
SemEval


== References ==
The Constituent Likelihood Automatic Word-tagging System or CLAWS is a program that performs part-of-speech tagging. It was developed in the 1980s at Lancaster University by the University Centre for Computer Corpus Research on Language.  It has an overall accuracy rate of 96-97% with the latest version (CLAWS4) tagging around 100 million words of the British National Corpus.

History
A Part-Of-Speech Tagger (POS Tagger) is a piece of software that reads text in some language and assigns parts of speech to each word (and other token), such as noun, verb, adjective, etc., although generally computational applications use more fine-grained POS tags like 'noun-plural'. Developed in the early 1980s, CLAWS was built to fill the ever-growing gap created by always-changing POS necessities. Since its inception, CLAWS has been hailed for its functionality and adaptability. Still, it is not without flaws, and though it boasts an error-rate of only 1.5% when judged in major categories, CLAWS still remains with c.3.3% ambiguities unresolved. Ambiguity arises in cases such as with the word flies, and whether it should be classified as a noun or a verb. It's these ambiguities that will require the various upgrades and tagsets that CLAWS will endure.

Rules and Processing
CLAWS uses a Hidden Markov model to determine the likelihood of sequences of words / parts of speech.

Sample Output
This excerpt from Bram Stoker's Dracula (1897) has been tagged using both the CLAWS C5 and C7 tagsets. This is what a CLAWS output will generally look like, with the most likely part-of-speech tag following each word.

Tagsets
CLAWS1 tagset
The first tagset developed in CLAWS, CLAWS1 tagset, has 132 word tags. In terms of form and application, C1 tagset is similar to Brown Corpus tags. See Table of tags in C1 tagset here.

CLAWS2 tagset
CLAWS2 tasget with 166 word tags was developed at Lancaster in 1983-1986. See Table of tags in C2 tagset here.

CLAWS4 tagset
CLAWS4 is a general-purpose grammatical tagger. It was used for the 100-million-word British National Corpus (BNC). It is a successor of CLAWS1 tagger. The latest version of CLAWS4 is offered by UCREL, a research center of Lancaster University.

CLAWS5 tagset
CLAWS5 tagset, which was used for BNC, has over 60 tags. See Table of tags in C5 tagset here.

CLAWS6 tagset
CLAWS6 tagset, which was used for BNC sampler corpus, has over 160 tags . See Table of tags in C6 tagset here.

CLAWS7 tagset
The standard CLAWS7 tagset is used currently. It is only different in the punctuation tags when compared to the CLAWS6 tagset . See Table of tags in C7 tagset here.

CLAWS8 tagset
CLAWS8 tagset was extended from C7 tagset with further distinctions in the determiner and pronoun categories, as well as auxiliary verbs . See Table of tags in C8 tagset here.

External links
CLAWS part-of-speech tagger for English
Brill tagger
Part-of-speech tagging
Sliding window based part-of-speech tagging
British National Corpus (BNC)
Brown Corpus
Lancaster University
Hidden Markov model


== References ==
ClearForest  was an Israeli software company that developed and marketed text analytics and text mining solutions.

History
Founded in 1998, ClearForest had its headquarters just outside Boston and a development center in Or Yehuda.  The company was acquired by Reuters in April, 2007.   It now markets its services under the names Calais, OpenCalais, and OneCalais.
ClearForest was previously venture-backed; its last funding round was led by Greylock Ventures and closed in 2005.  Other investors included DB Capital Partners, Pitango, Walden Israel, Booz Allen, JP Morgan Partners and HarbourVest Partners.
On February 7, 2008 Reuters announced the launch of Open Calais, a named-entity recognition and semantic analysis service that uses ClearForest technology.
On April 30, 2007, Reuters announced that it would acquire ClearForest.  Sources estimate the acquisition to be for $25 Million.

Solutions and Products
ClearForest offers several hosted solutions, including:

OpenCalais, a free web service and open API (for commercial and non-commercial use) that performs named-entity recognition and enables automatic metadata generation using the ClearForest financial module.
Semantic Web Services (SWS), an on-demand service that makes ClearForest's natural language processing tools available as a standard web service.  A subset of ClearForest's capabilities is available via SWS at no cost.
Gnosis, a free Firefox extension that uses SWS to analyze the content of a web page.  Gnosis identifies named entities such as people, companies, organizations, geographies and products on the page being viewed. Gnosis also automatically processes pages from Wikipedia, providing additional links for people, geographies and other entities which were not explicitly linked within the subject article.
Harvest, a real-time machine readable news service that uses SWS to process a company's news and document feeds and return machine-readable information about people, companies, locations and over 200 other entities facts and events.ClearForest also offers Text Analytics solutions targeted at specific business problems, including:

Equity valuation for hedge funds and alternative investments firms
Metadata & database creation for publishers and information providers/services
Tapping "voice of customer" for market and survey research firms
Quality Early Warning for vehicle, capital equipment & durable goods manufacturers

See also
Economy of Israel

References
External links
ClearForest web site
ClearForest semantic web services and Gnosis Firefox extension web site
Cloem is a company based in Cannes, France, which applies natural language processing (NLP) technologies to assist patent applicants in creating variants of patent claims, called "cloems". According to the company, these "computer-generated claims can be published to keep potential competitors from attempting to file adjacent patent claims."

Technology
According to Cloem, dictionaries, ontologies and proprietary claim-drafting algorithms are used to draft alternative claims based on a client's original set of claims. In particular, the original set of claims is subject to various permutations and linguistic manipulations "by considering alternative definitions for terms as well as “synonyms, hyponyms, hyperonyms, meronyms, holonyms, and antonyms.”"

Possible uses
Cloem can optionally publish one or more created texts, as electronic publications or as paper-printed publications. These can potentially serve –through a defensive publication– as prior art to prevent another party for obtaining a patent on the subject-matter at stake. In other words, after an initial patent filing, an "improvement" patent (adjacent invention) can be applied for by another party, such as a competitor. By publishing variants of a patent claim, the risk of adverse patenting may potentially be decreased (improvement inventions may no longer be patentable).
Cloems may also be potentially patentable. One of the issues of patentability, however, is that only a natural person can be a listed as an inventor on a patent. Since cloems are produced by a computer based on a person's input, it is not clear if the computer or the person is the inventor. The inventorship of Cloem texts is an open question.


== References ==
The CMU Pronouncing Dictionary (also known as CMUdict) is an open-source pronouncing dictionary originally created by the Speech Group at Carnegie Mellon University (CMU) for use in speech recognition research.
CMUdict provides a mapping orthographic/phonetic for English words in their North American pronunciations. It is commonly used to generate representations for speech recognition (ASR), e.g. the CMU Sphinx system, and speech synthesis (TTS), e.g. the Festival system. CMUdict can be used as a training corpus for building statistical grapheme-to-phoneme (g2p) models that will generate pronunciations for words not yet included in the dictionary.
The most recent release is 0.7b; it contains over 134,000 entries. An interactive lookup version is available.

Database Format
The database is distributed as a plain text file with one entry to a line in the format "WORD  <pronunciation>" with a two-space separator between the parts. If  multiple pronunciations are available for a word, variants are identified using numbered versions (e.g. WORD(1)). The pronunciation is encoded using a modified form of the ARPABET system, with the addition of stress marks on vowels of levels 0, 1, and 2. A line-initial ;;; token indicates a comment. A derived format, directly suitable for speech recognition engines is also available as part of the distribution; this format collapses stress distinctions (typically not used in ASR).

History
Applications
The Unifon converter is based on the CMU Pronouncing Dictionary.
The Natural Language Toolkit contains an interface to the CMU Pronouncing Dictionary.
The Carnegie Mellon Logios tool incorporates the CMU Pronouncing Dictionary.
PronunDict, a pronunciation dictionary of American English, uses the CMU Pronouncing Dictionary as its data source. Pronunciation is transcribed in IPA symbols. This dictionary also supports searching by pronunciation.

See also
Moby Pronunciator, a similar project

References
External links
The current version of the dictionary is at SourceForge, although there is also a version maintained on GitHub.
Homepage – includes database search
RDF converted to Resource Description Framework by the open source Texai project.
Computational semantics is the study of how to automate the process of constructing and reasoning with meaning representations of natural language expressions. It consequently plays an important role in natural language processing and computational linguistics.
Some traditional topics of interest are: construction of meaning representations, semantic underspecification, anaphora resolution, presupposition projection, and quantifier scope resolution. Methods employed usually draw from formal semantics or statistical semantics. Computational semantics has points of contact with the areas of lexical semantics (word sense disambiguation and semantic role labeling), discourse semantics, knowledge representation and automated reasoning (in particular, automated theorem proving). Since 1999 there has been an ACL special interest group on computational semantics, SIGSEM.

See also
Discourse representation theory
Minimal recursion semantics
Natural language understanding
Semantic compression
Semantic parsing
Semantic Web
SemEval
WordNet

Further reading
Blackburn, P., and Bos, J. (2005), Representation and Inference for Natural Language : A First Course in Computational Semantics, CSLI Publications. ISBN 1-57586-496-7.
Bunt, H., and Muskens, R. (1999), Computing Meaning, Volume 1, Kluwer Publishing, Dordrecht. ISBN 1-4020-0290-4.
Bunt, H., Muskens, R., and Thijsse, E. (2001), Computing Meaning, Volume 2, Kluwer Publishing, Dordrecht. ISBN 1-4020-0175-4.
Copestake, A., Flickinger, D. P., Sag, I. A., & Pollard, C. (2005). Minimal Recursion Semantics. An introduction. In Research on Language and Computation. 3:281–332.
Eijck, J. van, and C. Unger (2010): Computational Semantics with Functional Programming. Cambridge University Press. ISBN 978-0-521-75760-7
Wilks, Y., and Charniak, E. (1976), Computational Semantics: An Introduction to Artificial Intelligence and Natural Language Understanding, North-Holland, Amsterdam. ISBN 0-444-11110-7.

References
External links
Special Interest Group on Computational Semantics (SIGSEM) of the Association for Computational Linguistics (ACL)
IWCS - International Workshop on Computational Semantics (endorsed by SIGSEM)
ICoS - Inference in Computational Semantics (endorsed by SIGSEM)
Wolfram Group - Semantic Representation of Pure Mathematics
Concept mining is an activity that results in the extraction of concepts from artifacts. Solutions to the task typically involve aspects of artificial intelligence and statistics, such as data mining and text mining. Because artifacts are typically a loosely structured sequence of words and other symbols (rather than concepts), the problem is nontrivial, but it can provide powerful insights into the meaning, provenance and similarity of documents.

Methods
Traditionally, the conversion of words to concepts has been performed using a thesaurus, and for computational techniques the tendency is to do the same. The thesauri used are either specially created for the task, or a pre-existing language model, usually related to Princeton's WordNet.
The mappings of words to concepts are often ambiguous. Typically each word in a given language will relate to several possible concepts. Humans use context to disambiguate the various meanings of a given piece of text, where available machine translation systems cannot easily infer context.
For the purposes of concept mining however, these ambiguities tend to be less important than they are with machine translation, for in large documents the ambiguities tend to even out, much as is the case with text mining.
There are many techniques for disambiguation that may be used. Examples are linguistic analysis of the text and the use of word and concept association frequency information that may be inferred from large text corpora. Recently, techniques that base on semantic similarity between the possible concepts and the context have appeared and gained interest in the scientific community.

Applications
Detecting and indexing similar documents in large corpora
One of the spin-offs of calculating document statistics in the concept domain, rather than the word domain, is that concepts form natural tree structures based on hypernymy and meronymy. These structures can be used to produce simple tree membership statistics, that can be used to locate any document in a Euclidean concept space. If the size of a document is also considered as another dimension of this space then an extremely efficient indexing system can be created. This technique is currently in commercial use locating similar legal documents in a 2.5 million document corpus.

Clustering documents by topic
Standard numeric clustering techniques may be used in "concept space" as described above to locate and index documents by the inferred topic. These are numerically far more efficient than their text mining cousins, and tend to behave more intuitively, in that they map better to the similarity measures a human would generate.

See also
Formal concept analysis
Information extraction
Compound term processing


== References ==
A confusion network (sometimes called a word confusion network or informally known as a sausage) is a natural language processing method that combines outputs from multiple automatic speech recognition or machine translation systems. Confusion networks are simple linear directed acyclic graphs with the property that each a path from the start node to the end node goes through all the other nodes.The set of words represented by edges between two nodes is called a confusion set. In machine translation, the defining characteristic of confusion networks is that they allow multiple ambiguous inputs, deferring committal translation decisions until later stages of processing. This approach is used in the open source machine translation software Moses and the proprietary translation API in IBM Bluemix Watson.


== References ==
Content determination is the subtask of natural language generation that involves deciding on the information to be communicated in a generated text.  It is closely related to the task of document structuring.

Example
Consider an NLG system which summarises information about sick babies.  Suppose this system has four pieces of information it can communicate

The baby is being given morphine via an IV drop
The baby's heart rate shows bradycardia's (temporary drops)
The baby's temperature is normal
The baby is cryingWhich of these bits of information should be included in the generated texts?

Issues
There are three general issues which almost always impact the content determination task, and can be illustrated with the above example.
Perhaps the most fundamental issue is the communicative goal of the text, i.e. its purpose and reader.  In the above example, for instance, a doctor who wants to make a decision about medical treatment would probably be most interested in the heart rate bradycardias, while a parent who wanted to know how her child was doing would probably be more interested in the fact that the baby was being given morphine and was crying.
The second issue is the size and level of detail of the generated text.  For instance, a short summary which was sent to a doctor as a 160 character SMS text message might only mention the heart rate bradycarias, while a longer summary which was printed out as a multipage document might also mention the fact that the baby is on a morphine IV.
The final issue is how unusual and unexpected the information is.  For example, neither doctors nor parents would place a high priority on being told that the baby's temperature was normal, if they expected this to be the case.
Regardless, content determination is very important to users, indeed in many cases the quality of content determination is the most important factor (from the user's perspective) in determining the overall quality of the generated text.

Techniques
There are three basic approaches to document structuring: schemas (content templates), statistical approaches, and explicit reasoning.
Schemas  are templates which explicitly specify the content of a generated text (as well as document structuring information).  Typically they are constructed by manually analysing a corpus of human-written texts in the target genre, and extracting a content template from these texts.  Schemas work well in practice in domains where content is somewhat standardised, but work less well in domains where content is more fluid (such as the medical example above).
Statistical techniques use statistical corpus analysis techniques to automatically determine the content of the generated texts.  Such work is in its infancy, and has mostly been applied to contexts where the communicative goal, reader, size, and level of detail are fixed.  For example, generation of newswire summaries of sporting events.Explicit reasoning approaches have probably attracted the most attention from researchers.  The basic idea is to use AI reasoning techniques (such as knowledge-based rules, planning, pattern detection, case-based reasoning, etc.) to examine the information available to be communicated (including how unusual/unexpected it is), the communicative goal and reader, and the characteristics of the generated text (including target size), and decide on the optimal content for the generated text.   A very wide range of techniques has been explored, but there is no consensus as to which is most effective.


== References ==
Controlled natural languages (CNLs) are subsets of natural languages that are obtained by restricting the grammar and vocabulary in order to reduce or eliminate ambiguity and complexity. Traditionally, controlled languages fall into two major types: those that improve readability for human readers (e.g. non-native speakers),
and those that enable reliable automatic semantic analysis of the language.
The first type of languages (often called "simplified" or "technical" languages), for example ASD Simplified Technical English, Caterpillar Technical English, IBM's Easy English, are used in the industry to increase the quality of technical documentation, and possibly simplify the (semi-)automatic translation of the documentation. These languages restrict the writer by general rules such as "Keep sentences short", "Avoid the use of pronouns", "Only use dictionary-approved words", and "Use only the active voice".The second type of languages have a formal syntax and semantics, and can be mapped to an existing formal language, such as first-order logic. Thus, those languages can be used as knowledge representation languages, and writing of those languages is supported by fully automatic consistency and redundancy checks, query answering, etc.

Languages
Existing controlled natural languages include:

See also
References
External links
Controlled Natural Languages
A conversational interface (CUI) is a user interface for computers that emulates a conversation with a real human. Historically, computers have relied on text-based user interfaces and graphical user interfaces (GUIs) (such as the user pressing a "back" button) to translate the user's desired action into commands the computer understands. While an effective mechanism of completing computing actions, there is a learning curve for the user associated with GUI. Instead, CUIs provide opportunity for the user to communicate with the computer in their natural language rather than in a syntax specific commands. To do this, conversational interfaces use natural language processing (NLP) to allow computers to understand, analyze, and create meaning from human language. Unlike word processors, NLP considers the structure of human language (i.e., words make phrases; phrases make sentences which convey the idea or intent the user is trying to invoke). The ambiguous nature of human language makes it difficult for a machine to always correctly interpret the user's requests, which is why we have seen a shift toward natural-language understanding (NLU).
NLU allows for sentiment analysis and conversational searches which allows a line of questioning to continue, with the context carried throughout the conversation. NLU allows conversational interfaces to handle unstructured inputs that the human brain is able to understand such as spelling mistakes of follow-up questions .  For example, through leveraging NLU, a user could first ask for the population of the United States. If the user then asks "Who is the president?", the search will carry forward the context of the United States and provide the appropriate response.
Conversational interfaces have emerged as a tool for businesses to efficiently provide consumers with relevant information, in a cost-effective manner. CUI provide ease of access to relevant, contextual information to the end user without the complexities and learning curve typically associated with technology.
While there are a variety of interface brands, to date, there are two main categories of conversational interfaces; voice assistants and chatbots.

Voice assistants
A voice assistant is a user interface that allows a user to complete an action simply by speaking a command. Introduced in October 2011, Apple's Siri was one of the first voice assistants widely adopted. Siri allowed users of iPhone to get information and complete actions on their device simply by asking Siri.
Further development has continued since Siri's introduction to include home based devices such as Google Home or Amazon Echo (powered by Alexa) that allow users to "connect" their homes through a series of smart devices to further the options of tangible actions they can complete. Users can now turn off the lights, set reminders and call their friends all with a verbal queue.
These conversational interfaces that utilize a voice assistant have become an efficient and popular way for businesses to interact with their customers as the interface removes the typical friction in a customer journey. Customers no longer need to remember a long list of usernames and passwords to their various accounts; they simply link each account to Google or Amazon once, and gone are the days where you needed to wait on hold for an hour to ask a simple question.

Chatbots
A chatbot is a web- or mobile-based interface that allows the user to ask questions and retrieve information. This information can be generic in nature such as the Google Assistant chat window that allows for internet searches, or it can be a specific brand or service which allows the user to gain information about the status of their various accounts. Their backend systems work in the same manner as a voice assistant, with the front end utilizing a visual interface to convey information. This visual interface can be beneficial for companies that need to do more complex business transactions with customers, as instructions, deep links and graphics can all be utilized to convey an answer. The complexity to which a chatbot answers questions depends on the development of the back end. Chatbots with hard-coded answers has a smaller base on information and corresponding skills. Chatbots that leverage machine learning will continue to grow and develop larger content bases for more complex responses.
More frequently, companies are leveraging chatbots as a way to offload simple questions and transactions from human agents. These chatbots provide the option to assist a user, but then directly transfer the customer to a live agent within the same chat window if the conversation becomes too complex.

See also
Chatbot
Natural language processing
Natural-language understanding
Voice command device
Voice computing


== References ==
Conversica is a US-based cloud software technology company.  Conversica offers a suite of Intelligent Virtual Assistants for business, with a focus on Customer Experience business functions (Marketing, Sales, Customer Success, Account Management and Payments).  Powered by Artificial Intelligence, the Intelligent Virtual Assistants (IVA) interact with leads and customers in a human-like way, like an entry level employee.  The IVA software interacts over multiple channels, including email and SMS text messages, and in multiple languages.  Conversica is a pioneer in providing AI-driven lead engagement software for marketing and sales organizations.  Conversica is headquartered in Silicon Valley, (Foster City, California).

History
2007: The Company was founded by Ben Brigham in Bellingham, Washington as AutoFerret.com.  The company initially produced a Customer Relationship Management (CRM) product, targeted at automotive dealerships. This soon expanded to lead generation, and then lead validation and qualification. The AI for which Conversica is known today was born out of a need to follow-up on and filter out low-quality leads, and in time it was clear that this was where the real value of the product lay.  The focus of the company shifted toward this automated lead engagement technology.
2010: The Company started commercially selling AVA, the first Automated Virtual Assistant for sales, and the company name is changed to AVA.ai®.  Early customers for AVA are automotive dealerships.  and subsequent history of the company has been iteration and refinement of the AI-lead interaction. As the company moved away from generating leads themselves, and providing the CRM themselves, it became necessary to integrate with existing CRMs and Marketing Automation platforms, such as DealerSocket, VinSolutions and Salesforce. 
2013: Company raises $16m Series A funding, lead by Kennet Partners, names Mark Bradley as CEO of AVA.ai.  Headquarters move from Bellingham, Washington to Foster City, California. Other key executives joining the business include David Marod (Automotive Sales) and William Webb-Purkis (Product Management).  Expands sales office in Kansas City, Missouri. 
2014: Company changes name from AVA.ai to Conversica.  
2015: Alex Terry joins Conversica as CEO. Other key executives joining the business include Jason Lund (CFO), Dr. Werner Koepf (SVP Engineering).  Business expands to include material number of customers in additional industry verticals, including technology, education, and financial services.
2016: Company raises $34m Series B funding, lead by Providence Strategic Growth.  Conversica launches second type of IVA - a Customer Success Assistant to help drive expansion revenue from existing customers.  
2017: Conversica expands intelligent automation platform and IVAs to support additional communication channels (e-mail and SMS text messaging) and additional communication languages.  Conversica opens new technology center of excellence in Seattle, Washington to expand AI and Machine Learning capabilities.  Dr. Sid Reddy joins Conversica as Chief Scientist.
2018: Company raises $31m Series C funding, lead by Providence Strategic Growth.  Conversica acquires Intelligens.ai, providing a regional presence in Latin America with an office in Las Condes, Santiago, Chile. Launches AI-powered Admissions Assistant for Higher Education industry.  Rashmi Vittal joins Conversica as CMO. 
2019: Conversica was selected by Fast Company magazine as one of the Top 10 Most Innovative AI Companies in the World, and was named Marketo's Technology Partner of the Year,.  The company officially expanded into the EMEA region with the opening of a London office.  As of August 2019, Conversica has over 50 different integrations with third parties. In October Conversica won three awards at the fourth annual Global Annual Achievement Awards for Artificial Intelligence. Also that month, Alex Terry stepped down from his role as CEO, to be replaced by former Janrain leader Jim Kaskade.

Technology
Conversica's Intelligent Virtual Assistants are AI assistants who communicate with leads, prospects, customers, employees and other persons of interest (Contacts) in a human-like manner, via email and SMS text.  The IVAs are built on an Intelligent Automation platform that leverages natural language understanding, natural language processing, natural language generation, deep learning and machine learning. The IVAs interact in a number of languages, including English, French, German, Spanish, Portuguese, and Japanese. The Assistants are generally deployed alongside sales and marketing, customer success, account management and higher education admissions teams, as part of an augmented workforce.  The Intelligent Automation platform integrates with over 50 external systems, including CRM, Marketing Automation, and other systems of record. A partial list of integration partners includes: Salesforce, Marketo, Oracle, HubSpot, DealerSocket, Reynolds & Reynolds, CDK Global, VinSolutions and many more.

References
External links
Official website
Cross-language information retrieval (CLIR) is a subfield of information retrieval dealing with retrieving information written in a language different from the language of the user's query. 
The term "cross-language information retrieval" has many synonyms, of which the following are perhaps the most frequent: cross-lingual information retrieval, translingual information retrieval, multilingual information retrieval. The term "multilingual information retrieval" refers more generally both to technology for retrieval of multilingual collections and to technology which has been moved to handle material in one language to another. The term Multilingual Information Retrieval (MLIR) involves the study of systems that accept queries for information in various languages and return objects (text, and other media) of various languages, translated into the user's language. Cross-language information retrieval refers more specifically to the use case where users formulate their information need in one language and the system retrieves relevant documents in another. To do so, most CLIR systems use various translation techniques.  CLIR techniques can be classified into different categories based on different translation resources:
Dictionary-based CLIR techniques
Parallel corpora based CLIR techniques
Comparable corpora based CLIR techniques
Machine translator based CLIR techniquesCLIR systems have improved so much that the most accurate multi-lingual and cross-lingual adhoc information retrieval systems today are nearly as effective as monolingual systems. Other related information access tasks, such as media monitoring, information filtering and routing, sentiment analysis, and information extraction require more sophisticated models and typically more processing and analysis of the information items of interest. Much of that processing needs to be aware of the specifics of the target languages it is deployed in.
Mostly, the various mechanisms of variation in human language pose coverage challenges for information retrieval systems: texts in a collection may treat a topic of interest but use terms or expressions which do not match the expression of information need given by the user. This can be true even in a mono-lingual case, but this is especially true in cross-lingual information retrieval, where users may know the target language only to some extent. The benefits of CLIR technology for users with poor to moderate competence in the target language has been found to be greater than for those who are fluent. Specific technologies in place for CLIR services include morphological analysis to handle inflection, decompounding or compound splitting to handle compound terms, and translations mechanisms to translate a query from one language to another. 
The first workshop on CLIR was held in Zürich during the SIGIR-96 conference. Workshops have been held yearly since 2000 at the meetings of the Cross Language Evaluation Forum (CLEF). Researchers also convene at the annual Text Retrieval Conference (TREC) to discuss their findings regarding different systems and methods of information retrieval, and the conference has served as a point of reference for the CLIR subfield.Google Search had a cross-language search feature that was removed in 2013.

See also
EXCLAIM (EXtensible Cross-Linguistic Automatic Information Machine)
CLEF (Conference and Labs of the Evaluation Forum, formerly known as Cross-Language Evaluation Forum)
MLIR (Multi-Lingual Information Retrieval)

References
External links
A resource page for CLIR
A search engine for CLIR
Frederick J. Damerau (December 25, 1931 – January 27, 2009) was a pioneer of research on natural language processing and data mining.
After earning his B.A. from Cornell University in 1953, he spent most of his career at IBM, in the Thomas J. Watson Research Center.
One of his most influential and ground-breaking papers was "A technique for computer detection and correction of spelling errors" published in 1964.  He also developed and patented for IBM the first algorithm for placing hyphens automatically in words.
In 1971 he published the book "Markov Models and Linguistic Theory : An Experimental Study of a Model for English."
After being active in research for over four decades, Fred Damerau died on January 27, 2009.

See also
Damerau–Levenshtein distance

References
External links
cornellalumnimagazine.com
DATR is a language for lexical knowledge representation. The lexical knowledge is encoded in a network of nodes. Each node has a set of attributes encoded with it. A node can represent a word or a word form.
DATR was developed in the late 1980s by Roger Evans, Gerald Gazdar and Bill Keller, and used extensively in the 1990s; the standard specification is contained in the Evans and Gazdar RFC, available on the Sussex website (below). DATR has been implemented in a variety of programming languages, and several implementations are available on the internet, including an RFC compliant implementation at the Bielefeld website (below).
DATR is still used for encoding inheritance networks in various linguistic and non-linguistic domains and is under discussion as a standard notation for the representation of lexical information.

References
External links
DATR at the University of Sussex
DATR repository and RFC compliant ZDATR implementation at Universität Bielefeld
Deep linguistic processing is a natural language processing framework which draws on theoretical and descriptive linguistics. It models language predominantly by way of theoretical syntactic/semantic theory (e.g. CCG, HPSG, LFG, TAG, the Prague School). Deep linguistic processing approaches differ from "shallower" methods in that they yield more expressive and structural representations which directly capture long-distance dependencies and underlying predicate-argument structures. 
The knowledge-intensive approach of deep linguistic processing requires considerable computational power, and has in the past sometimes been judged as being intractable. However, research in the early 2000s had made considerable advancement in efficiency of deep processing. Today, efficiency is no longer a major problem for applications using deep linguistic processing.

Contrast to "shallow linguistic processing"
Traditionally, deep linguistic processing has been concerned with computational grammar development (for use in both parsing and generation). These grammars were manually developed, maintained and were computationally expensive to run. In recent years, machine learning approaches (also known as shallow linguistic processing) have fundamentally altered the field of natural language processing. The rapid creation of robust and wide-coverage machine learning NLP tools requires substantially lesser amount of manual labor. Thus deep linguistic processing methods have received less attention.
However, it is the belief of some computational linguists that in order for computers to understand natural language or inference, detailed syntactic and semantic representation is necessary. Moreover, while humans can easily understand a sentence and its meaning, shallow linguistic processing might lack human language 'understanding'. For example: 
a) Things would be different if Microsoft were located in Georgia.In sentence (a), a shallow information extraction system might infer wrongly that Microsoft's headquarters was located in Georgia. While as humans, we understand from the sentence that Microsoft office was never in Georgia.
b) The National Institute for Psychology in Israel was established in May 1971 as the Israel Center for Psychobiology by Prof. Joel.In sentence (b), a shallow system could wrongly infer that Israel was established in May 1971. Humans know that it is the National Institute for Psychobiology that was established in 1971.
In summary of the comparison between deep and shallow language processing, deep linguistic processing provides a knowledge-rich analysis of language through manually developed grammars and language resources. Whereas, shallow linguistic processing provides a knowledge-lean analysis of language through statistical/machine learning manipulation of texts and/or annotated linguistic resource.

Sub-communities
"Deep" computational linguists are divided in different sub-communities based on the grammatical formalism they adopted for deep linguistic processing. The major sub-communities includes the:

DEep Linguistic Processing with HPSG - INitiative (DELPH-IN) collaboration working with the HPSG formalism. The HPSG Conference is the central conference to share knowledge/advancement of HPSG based deep processing.
ParGram/ParSem is international collaboration on LFG-based grammar and semantics development. The LFG Conference is the central conference to share knowledge/advancement of LFG based deep processing.
XTAG Research group working with the TAG formalism. The TAG+ conference is the central conference to share knowledge/advancement of TAG based deep processing.The shortlist above is not exhaustively representative of all the communities working on deep linguistic processing.

See also
Combinatory categorial grammar
Head-driven phrase structure grammar
Lexical functional grammar
Natural language processing
Tree-adjoining grammar


== References ==
Eclipse Deeplearning4j is a deep learning programming library written for Java and the Java virtual machine (JVM) and a computing framework with wide support for deep learning algorithms. Deeplearning4j includes implementations of the restricted Boltzmann machine, deep belief net, deep autoencoder, stacked denoising autoencoder and recursive neural tensor network, word2vec, doc2vec, and GloVe. These algorithms all include distributed parallel versions that integrate with Apache Hadoop and Spark.Deeplearning4j is open-source software released under Apache License 2.0, developed mainly by a machine learning group headquartered in San Francisco. It is supported commercially by the startup Skymind, which bundles DL4J, TensorFlow, Keras and other deep learning libraries in an enterprise distribution called the Skymind Intelligence Layer. Deeplearning4j was contributed to the Eclipse Foundation in October 2017.

Introduction
Deeplearning4j relies on the widely used programming language Java, though it is compatible with Clojure and includes a Scala application programming interface (API). It is powered by its own open-source numerical computing library, ND4J, and works with both central processing units (CPUs) and graphics processing units (GPUs).Deeplearning4j has been used in several commercial and academic applications. The code is hosted on GitHub. A support forum is maintained on Gitter.The framework is composable, meaning shallow neural nets such as restricted Boltzmann machines, convolutional nets, autoencoders, and recurrent nets can be added to one another to create deep nets of varying types. It also has extensive visualization tools, and a computation graph.

Distributed
Training with Deeplearning4j occurs in a cluster. Neural nets are trained in parallel via iterative reduce, which works on Hadoop-YARN and on Spark. Deeplearning4j also integrates with CUDA kernels to conduct pure GPU operations, and works with distributed GPUs.

Scientific computing for the JVM
Deeplearning4j includes an n-dimensional array class using ND4J that allows scientific computing in Java and Scala, similar to the functions that NumPy provides to Python. It's effectively based on a library for linear algebra and matrix manipulation in a production environment.

DataVec vectorization library for machine-learning
DataVec vectorizes various file formats and data types using an input/output format system similar to Hadoop's use of MapReduce; that is, it turns various data types into columns of scalars termed vectors. DataVec is designed to vectorize CSVs, images, sound, text, video, and time series.

Text and NLP
Deeplearning4j includes a vector space modeling and topic modeling toolkit, implemented in Java and integrating with parallel GPUs for performance. It is designed to handle large text sets.
Deeplearning4j includes implementations of term frequency–inverse document frequency (tf–idf), deep learning, and Mikolov's word2vec algorithm, doc2vec, and GloVe, reimplemented and optimized in Java. It relies on t-distributed stochastic neighbor embedding (t-SNE) for word-cloud visualizations.

Real-world use cases and integrations
Real-world use cases for Deeplearning4j include network intrusion detection and cybersecurity, fraud detection for the financial sector, anomaly detection in industries such as manufacturing, recommender systems in e-commerce and advertising, and image recognition. Deeplearning4j has integrated with other machine-learning platforms such as RapidMiner, Prediction.io, and Weka.

Machine Learning Model Server
Deeplearning4j serves machine-learning models for inference in production using the free developer edition of SKIL, the Skymind Intelligence Layer. A model server serves the parametric machine-learning models that makes decisions about data. It is used for the inference stage of a machine-learning workflow, after data pipelines and model training. A model server is the tool that allows data science research to be deployed in a real-world production environment.
What a Web server is to the Internet, a model server is to AI. Where a Web server receives an HTTP request and returns data about a Web site, a model server receives data, and returns a decision or prediction about that data: e.g. sent an image, a model server might return a label for that image, identifying faces or animals in photographs.
The SKIL model server is able to import models from Python frameworks such as Tensorflow, Keras, Theano and CNTK, overcoming a major barrier in deploying deep learning models.

Benchmarks
Deeplearning4j is as fast as Caffe for non-trivial image recognition tasks using multiple GPUs. For programmers unfamiliar with HPC on the JVM, there are several parameters that must be adjusted to optimize neural network training time. These include setting the heap space, the garbage collection algorithm, employing off-heap memory and pre-saving data (pickling) for faster ETL. Together, these optimizations can lead to a 10x acceleration in performance with Deeplearning4j.

API Languages: Java, Scala, Python , Clojure & Kotlin
Deeplearning4j can be used via multiple API languages including Java, Scala, Python, Clojure and Kotlin. Its Scala API is called ScalNet.  Keras serves as its Python API.  And its Clojure wrapper is known as DL4CLJ. The core languages performing the large-scale mathematical operations necessary for deep learning are C, C++ and CUDA C.

Tensorflow, Keras & Deeplearning4j
Tensorflow, Keras and Deeplearning4j work together. Deeplearning4j can import models from Tensorflow and other Python frameworks if they have been created with Keras.

See also

Comparison of deep learning software
Artificial intelligence
Machine learning
Deep learning


== References ==
Deep Linguistic Processing with HPSG - INitiative (DELPH-IN) is a collaboration where computational linguists worldwide develop natural language processing tools for deep linguistic processing of human language. The goal of DELPH-IN is to combine linguistic and statistical processing methods in order to computationally understand the meaning of texts and utterances.
The tools developed by DELPH-IN adopt two linguistic formalisms for deep linguistic analysis, viz. head-driven phrase structure grammar (HPSG) and minimal recursion semantics (MRS). All tools under the DELPH-IN collaboration are developed for general use of open-source licensing.
Since 2005, DELPH-IN has held an annual summit. This is a loosely structured unconference where people update each other about the work they are doing, seek feedback on current work, and occasionally hammer out agreement on standards and best practice.

DELPH-IN technologies and resources
The DELPH-IN collaboration has been progressively building computational tools for deep linguistic analysis, such as:

LKB system (Linguistic Knowledge Builder): a grammar engineering environment where linguists can build unification grammars with the Head-driven Phrase Structure Grammar formalism
PET parser (Platform for Experimentation with efficient HPSG processing Techniques): an open source parser which produces HPSG parse trees with Minimal Recursion Semantics (MRS) outputs 
ACE processor (Answer Constraint Engine): an efficient system to process DELPH-IN grammars that provide HPSG syntactic parses with MRS outputs. The latest version of ACE is able to generate natural language sentences.
LOGON infrastructure is a collection of software and DELPH-IN grammars to provide transfer-based machine translation. The LOGON approach to machine translation has proven to provide quality oriented hybrid (rule-based and stochastic) translations.Other than deep linguistic processing tools, the DELPH-IN collaboration supplies computational resources for Natural Language Processing such as computational HPSG grammars and language prototypes e.g.:

DELPH-IN grammars: a catalogue of computational HPSG grammar hand-crafted to capture deep linguistics analysis specific to the respective languages 
LinGO Grammar Matrix: an open-source starter-kit for rapid prototyping of precision broad-coverage grammars compatible with the LKB. It contains a library of common language phenomena that computational grammarians can inherit for their HPSG grammars.
CLIMB libraries (Comparative Libraries of Implementations with Matrix Basis): an extended language library built on the Grammar Matrix. The objective of the CLIMB library is to maintain alternative analyses of the same phenomenon across different languages to test their impact on long-term grammar development.Another range of DELPH-IN resources are not unlike the data use for shallow linguistic processing, such as Text_corpus and treebanks:

MRS Test Suite: a short but representative set of sentences designed to capture some minimal recursion semantics phenomena. The test suites are available in Bulgarian, English, French, German, Greek, Japanese, Mandarin, Norwegian, Portuguese, Russian and Spanish.
Wikiwoods: WikiWoods is a parsed corpus that provides rich syntacto-semantic annotations for the English Wikipedia.
DeepBank: an ongoing project to annotate the one million words of 1989 Wall Street Journal text (the same set of sentences annotated in the original Penn Treebank project) with the English Resource Grammar, augmented with a robust approximating PCFG for complete coverage.
Cathedral and the Bazaar: a compilation of an early essay on Open Source by Eric Raymond with translations into multiple languages. It was proposed as a multilingual shared test suite to enable us to compare parses across different grammars.The open-source culture of the DELPH-IN collaboration provides the Natural Language Processing community with an array of deep linguistic processing tools and resources. However, the usability of DELPH-IN tools has been an issue with users and application developers new to the DELPH-IN ecology. The DELPH-IN developers are aware of these usability issues and there are ongoing attempts to improve documentation and tutorials of DELPH-IN technologies.

See also
Head-driven Phrase Structure Grammar
Minimal Recursion Semantics

References
External links
DELPH-IN website
DELPH-IN wiki forum
Short tutorial to DELPH-IN's ecology of tools and resources
A discourse relation (or rhetorical relation) is a description of how two segments of discourse are logically connected to one another.
One method of modeling discourse involves a set of concepts that constitute "segmented discourse representation theory" (SDRT).

SDRT
Asher and Lascarides categorize the discourse relations formalized in SDRT into five classes.

Content-level relations
Text structuring relations
Divergent relations
Metatalk relations
Consequence*(α,β)
Explanation*(α,β)
Explanation*q(α,β)
Result*(α,β)

See also
Speech act
Contrast (linguistics)

Notes and references
Bibliography
Asher, Nicholas and Alex Lascarides (2003). Logics of Conversation. Studies in Natural Language Processing. Cambridge University Press. ISBN 0-521-65058-5
Pitler, Emily and others (2008). "Easily Identifiable Discourse Relations". University of Pennsylvania Department of Computer and Information Science Technical Report No. MS-CIS-08-24.
Grosz, Barbara J. and Candice L. Sidner (1986). "Attention, Intentions, and the Structure of Discourse". Computational Linguistics 12: 175–204. [aka DSM]
Alistair Knott, 'An Algorithmic Framework for Specifying the Semantics of Discourse Relations', Computational Intelligence 16 (2000).
Mann, William C. and Sandra A .Thompson (1988). "Rhetorical Structure Theory: A theory of text organization". Text 8: 243–281. [aka RST]

External links
Rhetorical Structure Theory — RST website, created by William C. Mann, maintained by Maite Taboada
Document classification or document categorization is a problem in library science, information science and computer science. The task is to assign a document to one or more classes or categories. This may be done "manually" (or "intellectually") or algorithmically. The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, however, and there is therefore interdisciplinary research on document classification.
The documents to be classified may be texts, images, music, etc. Each kind of document possesses its special classification problems. When not otherwise specified, text classification is implied.
Documents may be classified according to their subjects or according to other attributes (such as document type, author, printing year etc.). In the rest of this article only subject classification is considered. There are two main philosophies of subject classification of documents: the content-based approach and the request-based approach.

"Content-based" versus "request-based" classification
Content-based classification is classification in which the weight given to particular subjects in a document determines the class to which the document is assigned. It is, for example, a common rule for classification in libraries, that at least 20% of the content of a book should be about the class to which the book is assigned. In automatic classification it could be the number of times given words appears in a document.
Request-oriented classification (or -indexing) is classification in which the anticipated request from users is influencing how documents are being classified. The classifier asks themself: “Under which descriptors should this entity be found?” and “think of all the possible queries and decide for which ones the entity at hand is relevant” (Soergel, 1985, p. 230).
Request-oriented classification may be classification that is targeted towards a particular audience or user group. For example, a library or a database for feminist studies may classify/index documents differently when compared to a historical library.  It is probably better, however, to understand request-oriented classification as policy-based classification: The classification is done according to some ideals and reflects the purpose of the library or database doing the classification. In this way it is not necessarily a kind of classification or indexing based on user studies. Only if empirical data about use or users are applied should request-oriented classification be regarded as a user-based approach.

Classification versus indexing
Sometimes a distinction is made between assigning documents to classes ("classification") versus assigning subjects to documents ("subject indexing") but as Frederick Wilfrid Lancaster has argued, this distinction is not fruitful. "These terminological distinctions,” he writes, “are quite meaningless and only serve to cause confusion” (Lancaster, 2003, p. 21). The view that this distinction is purely superficial is also supported by the fact that a classification system may be transformed into a thesaurus and vice versa (cf., Aitchison, 1986, 2004; Broughton, 2008; Riesthuis & Bliedung, 1991). Therefore, is the act of labeling a document (say by assigning a term from a controlled vocabulary to a document) at the same time to assign that document to the class of documents indexed by that term (all documents indexed or classified as X belong to the same class of documents).

Automatic document classification (ADC)
Automatic document classification tasks can be divided into three sorts: supervised document classification where some external mechanism (such as human feedback) provides information on the correct classification for documents, unsupervised document classification (also known as document clustering), where the classification must be done entirely without reference to external information, and semi-supervised document classification, where parts of the documents are labeled by the external mechanism. There are several software products under various license models available.

Techniques
Automatic document classification techniques include:

Expectation maximization (EM)
Naive Bayes classifier
tf–idf
Instantaneously trained neural networks
Latent semantic indexing
Support vector machines (SVM)
Artificial neural network
K-nearest neighbour algorithms
Decision trees such as ID3 or C4.5
Concept Mining
Rough set-based classifier
Soft set-based classifier
Multiple-instance learning
Natural language processing approaches

Applications
Classification techniques have been applied to

spam filtering, a process which tries to discern E-mail spam messages from legitimate emails
email routing, sending an email sent to a general address to a specific address or mailbox depending on topic
language identification, automatically determining the language of a text
genre classification, automatically determining the genre of a text
readability assessment, automatically determining the degree of readability of a text, either to find suitable materials for different age groups or reader types or as part of a larger text simplification system
sentiment analysis, determining the attitude of a speaker or a writer with respect to some topic or the overall contextual polarity of a document.
health-related classification using social media in public health surveillance 
article triage, selecting articles that are relevant for manual literature curation, for example as is being done as the first step to generate manually curated annotation databases in biology.

See also
Further reading
Fabrizio Sebastiani. Machine learning in automated text categorization. ACM Computing Surveys, 34(1):1–47, 2002.
Stefan Büttcher, Charles L. A. Clarke, and Gordon V. Cormack. Information Retrieval: Implementing and Evaluating Search Engines. MIT Press, 2010.

References
External links
Introduction to document classification
Bibliography on Automated Text Categorization
Bibliography on Query Classification
Text Classification analysis page
Learning to Classify Text - Chap. 6 of the book Natural Language Processing with Python (available online)
TechTC - Technion Repository of Text Categorization Datasets
David D. Lewis's Datasets
BioCreative III ACT (article classification task) dataset
A document-term matrix or term-document matrix is a mathematical matrix that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms. There are various schemes for determining the value that each entry in the matrix should take. One such scheme is tf-idf. They are useful in the field of natural language processing.

General Concept
When creating a database of terms that appear in a set of documents the document-term matrix contains rows corresponding to the documents and columns corresponding to the terms. For instance if one has the following two (short) documents:

D1 = "I like databases"
D2 = "I hate databases",then the document-term matrix would be:

which shows which documents contain which terms and how many times they appear.
Note that more sophisticated weights can be used; one typical example, among others, would be tf-idf.

Choice of Terms
A point of view on the matrix is that each row represents a document. In the vectorial semantic model, which is normally the one used to compute a document-term matrix, the goal is to represent the topic of a document by the frequency of semantically significant terms. The terms are semantic units of the documents. It is often assumed, for Indo-European languages, that nouns, verbs and adjectives are the more significant categories, and that words from those categories should be kept as terms. 
Adding collocation as terms improves the quality of the vectors, especially when computing similarities between documents.

Applications
Improving search results
Latent semantic analysis (LSA, performing singular-value decomposition on the document-term matrix) can improve search results by disambiguating polysemous words and searching for synonyms of the query. However, searching in the high-dimensional continuous space is much slower than searching the standard trie data structure of search engines.

Finding topics
Multivariate analysis of the document-term matrix can reveal topics/themes of the corpus. Specifically, latent semantic analysis and data clustering can be used, and more recently probabilistic latent semantic analysis and non-negative matrix factorization have been found to perform well for this task.

See also
Bag of words model

Implementations
Gensim: Open source Python framework for Vector Space modelling. Contains memory-efficient algorithms for constructing term-document matrices from text plus common transformations (tf-idf, LSA, LDA).
Documenting Hate is a project of ProPublica, in collaboration with a number of journalistic, academic, and computing organizations, for systematic tracking of hate crimes and bias incidents. It uses an online form to facilitate reporting of incidents by the general public. Since August 2017, it has also used machine learning and natural language processing techniques to monitor and collect news stories about hate crimes and bias incidents. As of October 2017, over 100 news organizations had joined the project.

History
Origin
Documenting Hate was created in response to ProPublica's dissatisfaction with the quality of reporting and tracking of evidence of hate crimes and bias incidents after the United States presidential election of 2016. The project was launched on 17 January 2017, after the publication on 15 November 2016 of a ProPublica news story about the difficulty of obtaining hard data on hate crimes.

Introduction of the Documenting Hate News Index
On 18 August 2017, ProPublica and Google announced the creation of the Documenting Hate News Index, which uses the Google Cloud Natural Language API for automated monitoring and collection of news stories about hate crimes and bias incidents. The API uses machine learning and natural language processing techniques. The findings of the Index are integrated with reports from members of the public. The Index is a joint project of ProPublica, Google News Lab, and the data visualization studio Pitch Interactive.

Response
Participation
As of May 2017, thousands of incidents had been reported via Documenting Hate. As of October 2017, over 100 news organizations had joined the project, including the Boston Globe, the New York Times, Vox, and the Georgetown University Hoya.

Relationship to government statistical monitoring
A policy analyst for the Center for Data Innovation (an affiliate of the Information Technology and Innovation Foundation), while supporting ProPublica's critique of the present state of hate-crime statistics, and praising ProPublica for drawing attention to the problem, has argued that a nongovernmental project like Documenting Hate cannot solve it unaided; instead, intervention at the federal level is needed.

See also
Unite the Right rally

References
External links
Documenting Hate on ProPublica (www.documentinghate.com redirects to this ProPublica page)
Documenting Hate News Index
Google News Lab
Google Cloud Natural Language API
Pitch Interactive
Empirical Methods in Natural Language Processing or EMNLP is a leading conference in the area of Natural Language Processing.  EMNLP is organized by the ACL special interest group on linguistic data (SIGDAT).
EMNLP was started in 1996, based on an earlier conference series called Workshop on Very Large Corpora (WVLC).As of 2014, EMNLP has a field rating of 36 within computer science and a citation count of 6937, according to Microsoft Academic Search, both within the top-300 for the field of CS.


== References ==
In natural language processing, entity linking, also referred to as named-entity linking (NEL), named-entity disambiguation (NED),  named-entity recognition and disambiguation (NERD) or named-entity normalization (NEN) is the task of assigning a unique identity to entities (such as famous individuals, locations, or companies) mentioned in text. For example, given the sentence "Paris is the capital of France", the idea is to determine that "Paris" refers to the city of Paris and not to Paris Hilton or any other entity that could be referred to as "Paris". Entity linking is different from named-entity recognition (NER) in that NER identifies the occurrence of a named entity in text but it does not identify which specific entity it is (see Differences from other techniques).

Introduction
In entity linking, words of interest (names of persons, locations and companies) are mapped from an input text to corresponding unique entities in a target knowledge base. Words of interest are called named entities (NEs), mentions, or surface forms. The target knowledge base depends on the intended application, but for entity linking systems intended to work on open-domain text it is common to use knowledge-bases derived from Wikipedia (such as Wikidata or DBpedia). In this case, each individual Wikipedia page is regarded as a separate entity. Entity linking techniques that map named entities to Wikipedia entities are also called wikification.Considering again the example sentence "Paris is the capital of France", the expected output of an entity linking system will be Paris and France.  These uniform resource locators (URLs) can be used as unique uniform resource identifiers
(URIs) for the entities in the knowledge base. Using a different knowledge base will return different URIs, but for knowledge bases built starting from Wikipedia there exist one-to-one URI mappings.In most cases, knowledge bases are manually built, but in applications where large text corpora are available, the knowledge base can be inferred automatically from the available text.Entity linking is a critical step to bridge web data with knowledge bases, which is beneficial for annotating the huge amount of raw and often noisy data on the Web and contributes to the vision of Semantic Web. In addition to entity linking, there are other critical steps including but not limited to event extraction, and event linking etc.

Applications
Entity linking is beneficial in fields that need to extract abstract representations from text, as it happens in text analysis, recommender systems, semantic search and chatbots. In all these fields, concepts relevant to the application are separated from text and other non-meaningful data.For example, a common task performed by search engines is to find documents that are similar to one given as input, or to find additional information about the persons that are mentioned in it.
Consider a sentence that contains the expression "the capital of France": without entity linking, the search engine that looks at the content of documents would not be able to directly retrieve documents containing the word "Paris", leading to so-called false negatives (FN). Even worse, the search engine might produce
spurious matches (or false positives (FP)), such as retrieving documents referring to "France" as a country.
Many approaches orthogonal to entity linking exist to retrieve documents similar to an input document. For example, latent semantic analysis (LSA) or comparing document embeddings obtained with
doc2vec. However, these techniques do not allow the same fine-grained control that is offered by entity linking, as they will return other
documents instead of creating high-level representations of the original one. For example, obtaining schematic information about "Paris", as presented by Wikipedia infoboxes would be much less straightforward, or sometimes even unfeasible, depending on the query complexity.Moreover, entity linking has been used to improve the performance of information retrieval systems and to improve search performance on digital libraries.  Entity linking is also a key input for semantic search.

Challenges in entity linking
An entity linking system has to deal with a number of challenges before being performant in real-life applications. Some of these issues are intrinsic to the task of entity linking, such as text ambiguity, while others, such as scalability and execution time, become relevant when considering real-life usage of such systems.

Name variations: the same entity might appear with textual representations. Sources of these variations include abbreviations (New York, NY), aliases (New York, Big Apple), or spelling variations and errors (New yokr).Ambiguity: the same mention can often refer to many different entities, depending on the context, as many entity names tend to be polysemous (i.e. have multiple meanings). The word Paris, among other things, could be referring to the French capital or to Paris Hilton. In some cases (as in the capital of France), there is no textual similarity between the mention text and the actual target entity (Paris).Absence: sometimes, some named entities might not have a correct entity link in the target knowledge base. This might happen when dealing with very specific or unusual entities, or when processing documents about recent events, in which there might be mentions of persons or events that do not have yet a corresponding entity in the knowledge base. Another common situation in which there are missing entities is when using domain-specific knowledge bases (for example, a biology knowledge base or a movie database). In all these cases, the entity linking system should return a NIL entity link. Understanding when to return a NIL prediction is not straightforward, and many different approaches have been proposed; for example, by thresholding some kind of confidence score in the entity linking system, or by adding an additional NIL entity to the knowledge base, which is treated in the same way as the other entities. Moreover, in some cases providing a wrong, but related, entity link prediction might be better than no result at all from the perspective of an end user.Scalability and Speed: it is desirable for an industrial entity linking system to provide results in a reasonable time, and often in real-time. This requirement is critical for search engines, chat-bots and for entity linking systems offered by data-analytics platforms. Ensuring low execution time can be challenging when using large knowledge bases or when processing large documents. For example, Wikipedia contains nearly 9 million entities and more than 170 million relationships among them.Evolving Information: an entity linking system should also deal with evolving information, and easily integrate updates in the knowledge base. The problem of evolving information is sometimes connected to the problem of missing entities, for example when processing recent news articles in which there are mentions of events that do not have a corresponding entry in the knowledge base due to their novelty.Multiple Languages: an entity linking systems might support queries performed in multiple languages. Ideally, the accuracy of the entity linking system should not be influenced by the input language, and entities in the knowledge base should be the same across different languages.

Differences from other techniques
Entity linking is also known as named-entity disambiguation (NED), and is deeply connected to Wikification and record linkage.
Definitions are often blurry and vary slightly among different authors: Alhelbawy et al. consider entity linking as a broader version of NED, as NED should assume that the entity that correctly match a certain textual named entity mention is in the knowledge base. Entity linking systems might deal with cases in which no entry for the named entity is available in the reference knowledge base. Other authors do not make such distinction, and use the two names interchangeably.
Wikification is the task of linking textual mentions to entities in Wikipedia (generally, limiting the scope to the English Wikipedia in case of cross-lingual wikification).Record linkage (RL) is considered a broader field than entity linking, and consists in finding records, across multiple and often heterogeneous data-sets, that refer to the same entity. Record linkage is a key component to digitalize archives, and to join multiple knowledge bases.Named-entity recognition locates and classifies named entities in unstructured text into pre-defined categories such as the names, organizations, locations, and more. For example, the following sentence:Paris is the capital of France.
would be processed by a NER system to obtain the following output:[Paris]City is the capital of [France]Country.
Named-entity recognition is usually a preprocessing step of an entity linking system, as it can be useful to know in advance which words should be linked to entities of the knowledge base.Coreference resolution understands whether multiple words in a text refers to the same entity. It can be useful, for example, to understand the word a pronoun refers to. Consider the following example:Paris is the capital of France. It is also the largest city in France. 
In this example, a coreference resolution algorithm would identify that the pronoun It refers to Paris, and not to France or to another entity. A notable distinction compared to entity linking is that Coreference Resolution does not assign any unique identity to the words it matches, but it simply says whether they refer to the same entity or not.

Approaches to entity linking
Entity linking has been a hot topic in industry and academia for the last decade. However, as of today most existing challenges are still unsolved, and many entity linking systems, with widely different strengths and weaknesses, have been proposed.Broadly speaking, modern entity linking systems can be divided into two categories:

Text-based approaches, which make use of textual features extracted from large text corpora (e.g. Term frequency - Inverse document frequency (Tf-Idf), word co-occurrence probabilities, etc...).
Graph-based approaches, which exploit the structure of knowledge graphs to represent the context and the relation of entities.Often entity linking systems cannot be strictly categorized in either category, but they make use of knowledge graphs that have been enriched with additional textual features extracted, for example, from the text corpora that were used to build the knowledge graphs themselves.

Text-based entity linking
The seminal work by Cucerzan in 2007 proposed one of the first entity linking systems that appeared in the literature, and tackled the task of wikification, linking textual mentions to Wikipedia pages. This system partitions pages as entity, disambiguation, or list pages, used to assign categories to each entity. The set of entities present in each entity page is used to build the entity's context. The final entity linking step is a collective disambiguation performed by comparing binary vectors obtained from hand-crafted features, and from each entity's context.
Cucerzan's entity linking system is still used as baseline for many recent works.The work of Rao et al. is a well-known paper in the field of entity linking. The authors propose a two-step algorithm to link named entities to entities in a target knowledge base. First, a set of candidate entities is chosen using string matching, acronyms, and known aliases. Then the best link among the candidates is chosen with a ranking support vector machine (SVM) that uses linguistic features. 
Recent systems, such as the one proposed by Tsai et al., employ word embeddings obtained with a skip-gram model as language features, and can be applied to any language as long as a large corpus to build word embeddings is provided. Similarly to most entity linking systems, the linking is done in two steps, with an initial candidate entities selection and a linear ranking SVM as second step. 
Various approaches have been tried to tackle the problem of entity ambiguity. In the seminal approach of Milne and Witten, supervised learning is employed using the anchor texts of Wikipedia entities as training data. Other approaches also collected training data based on unambiguous synonyms.
Kulkarni et al. exploited the common property that topically coherent documents refer to entities belonging to strongly related types.

Graph-based entity linking
Modern entity linking systems do not limit their analysis to textual features generated from input documents or text corpora, but employ large knowledge graphs created from knowledge bases such as Wikipedia. These systems extract complex features which take advantage of the knowledge graph topology, or leverage multi-step connections between entities, which would be hidden by simple text analysis. Moreover, creating multilingual entity linking systems based on natural language processing (NLP) is inherently difficult, as it requires either large text corpora, often absent for many languages, or hand-crafted grammar rules, which are widely different among languages. Han et al. propose the creation of a disambiguation graph (a subgraph of the knowledge base which contains candidate entities). This graph is employed for a purely collective ranking procedure that finds the best candidate link for each textual mention.
Another famous entity linking approach is AIDA, which uses a series of complex graph algorithms, and a greedy algorithm that identifies coherent mentions on a dense subgraph by also considering context similarities and vertex importance features to perform collective disambiguation.Graph ranking (or vertex ranking) denotes algorithms such as PageRank (PR) and Hyperlink-Induced Topic Search (HITS), whose goal is to assign a score to each vertex that represents its relative importance in the overall graph. The entity linking system presented in Alhelbawy et al. employs PageRank to perform collective entity linking on a disambiguation graph, and to understand which entities are more strongly related with each other and would represent a better linking.

See also
Controlled vocabulary
Explicit semantic analysis
Geoparsing
Information extraction
Linked data
Named entity
Named-entity recognition
Record linkage
Word sense disambiguation
Author Name Disambiguation
Coreference


== References ==
eTBLAST is a now-defunct free text similarity service search engine originally developed by Alexander Pertsemlidis and Harold “Skip” Garner at The University of Texas Southwestern Medical Center. eTBLAST offered access to the MEDLINE database, the National Institutes of Health (NIH) CRISP database, the Institute of Physics (IOP) database, Wikipedia, arXiv, the NASA technical reports database, Virginia Tech class descriptions and a variety of databases of clinical interest. eTBLAST searched citation databases and databases containing full text, such as PUBMED and compared a user's natural text query to target databases using a hybrid search algorithm consisting of a low-sensitivity weighted keyword-based first pass followed by a novel sentence-alignment based second pass.  eTBLAST was later offered as a web-based service of The Innovation Laboratory at the Virginia Bioinformatics Institute.
eTBLAST, as a text similarity engine, made possible a large study of duplicate publications and potential plagiarisms in the biomedical literature.  Thousands of random samples of Medline abstracts were submitted to eTBLAST, and those with the highest similarity were studied and entered into an on-line database. This work revealed several trends, including an increasing rate of duplication in the biomedical literature, as reported in the journals Bioinformatics, Anaesthesia and Intensive Care, Clinical Chemistry, Urologic Oncology, Nature, and Science.The system is now called HelioBLAST and is offered — still free of charge — by Harold "Skip" Garner through his company HelioText.  It is continuously expanding with additional text-based databases.

See also
BLAST (Basic Local Alignment Search Tool)
Natural language processing
Medical literature retrieval

External resources
Official website for HelioBlast

References
External links
eTBLAST
A not-for-profit organisation, the European Language Resources Association ("ELRA") is established under the law of the Grand Duchy of Luxembourg.  Its seat is in Luxembourg and its headquarters in Paris (France).

Activities
Since its founding in 1995, the European Language Resources Association has been a conduit for the distribution of speech, written, and terminology language resources (LRs) for human language technology (HLT), a key component of information society technologies (IST) In order to do so, a number of technical and logistic, commercial (prices, fees, royalties), legal (licensing, intellectual property rights, management), and information dissemination issues had to be addressed.
ELRA broadening its objectives and responsibilities towards the HLT community over the years, and is now also involved in the production, or commissioning of the production, of language resources through a number of initiatives, and actively committed to the evaluation of language-engineering tools as well as to the identification of new resources. The set up of the identification number system ISLRN, endorsed by NLP12 in 2013, is the most recent initiative led by ELDA to enhance the identification of language resources and their citation in publications.
Every other year, ELRA organizes a major conference, the International Language Resources and Evaluation Conference (LREC).

Mission
The mission of the Association is to promote language resources and evaluation for the Human Language Technology sector in all their forms and their uses, in a European context. Consequently, the goals are: to coordinate and carry out identification, production, validation, distribution, standardisation of languages resources, as well as support for evaluation of systems, products, tools, etc. Information Dissemination is also part of ELRA's missions which is carried through both the organisation of the conference LREC and the Language Resources and Evaluation Journal edited by Springer.

ELRA Board
Current members of the board of ELRA are:

Board officers
President
Henk van den Heuvel (The Netherlands)
Vice-president
Thierry Declerck (Germany)
Secretary: Maria Gavrilidou (Greece)
Treasurer: Tatjana Gornostaja (Latvia)
Board Members
Gilles Adda (France)
Nuria Bel (Spain)
Antonio Branco (Portugal)
Marko Grobelnik (Slovenia)
Simonetta Montemagni (Italy)
Honorary Presidents
Nicoletta Calzolari (Italy)
Joseph Mariani (France)
ELRA Secretary General
Khalid Choukri (France)

Antonio Zampolli Prize
The ELRA Board has created a prize to honour the memory of its first president, Professor Antonio Zampolli, a pioneer and visionary scientist who was internationally recognized in the field of computational linguistics and Human Language Technologies (HLT). He also contributed much through the establishment of ELRA and the LREC conference. 
To reflect Antonio Zampolli’s specific interest in our field, the Prize is awarded to individuals whose work lies within the areas of Language Resources and Language Technology Evaluation with acknowledged contributions to their advancement. So far, the Antonio Zampolli Prize was awarded to:

Frederick Jelinek, from Johns Hopkins University, Baltimore (USA), at LREC 2004, in Lisbon.
Christiane Fellbaum and George A. Miller, from Princeton University, Princeton (USA), at LREC 2006, in Genoa.
Yorick Wilks, from the Oxford Internet Institute and the Computer Science Department of the University of Sheffield (UK), at LREC 2008, in Marrakech.
Mark Liberman, from the University of Pennsylvania, Philadelphia (USA), at LREC 2010, in Valletta.
Charles Fillmore and Collin F. Baker, from the International Computer Science Institute (ICSI), University of California Berkeley (USA) and Oriental Committee for the Co-Ordination and Standardisation of Speech Databases and Assessment Techniques (Oriental COCOSDA), at LREC 2012, in Istanbul.
Alex Waibel from Carnegie Mellon University (USA) and Karlsruhe Institute of Technology (Germany), at LREC 2014, in Reykjavik.
Roger K. Moore from University of Sheffield (UK) at LREC 2016, in Portorož.
Eva Hajičová from Charles University, Prague, (Czech Republic) at LREC 2018, in Miyazaki.

ELDA (Evaluations and Language Resources Distribution Agency)
To handle every issues related to the association affairs, ELDA, Evaluations & Language resources Distribution Agency, was created, as ELRA operational body. ELDA is responsible for the development and the execution of ELRA’s strategies and plans, and handles issues related to the distribution of language resources.

See also
LRE Map
Natural Language Processing
Speech Technology
Corpus Linguistics
Machine Translation
Linguistic Data Consortium - a US-based institute with a similar mission.
ISO/TC37
JTC 1/SC 35 User interfaces
Language Grid - a platform for language resources, operated by NPO Language Grid Association.

References
External links
www.elra.info
www.elda.org
www.lrec-conf.org
In natural language processing and information retrieval, explicit semantic analysis (ESA) is a vectoral representation of text (individual words or entire documents) that uses a document corpus as a knowledge base. Specifically, in ESA, a word is represented as a column vector in the tf–idf matrix of the text corpus and a document (string of words) is represented as the centroid of the vectors representing its words. Typically, the text corpus is English Wikipedia, though other corpora including the Open Directory Project have been used.ESA was designed by Evgeniy Gabrilovich and Shaul Markovitch as a means of improving text categorization
and has been used by this pair of researchers to compute what they refer to as "semantic relatedness" by means of cosine similarity between the aforementioned vectors, collectively interpreted as a space of "concepts explicitly defined and described by humans", where Wikipedia articles (or ODP entries, or otherwise titles of documents in the knowledge base corpus) are equated with concepts.
The name "explicit semantic analysis" contrasts with latent semantic analysis (LSA), because the use of a knowledge base makes it possible to assign human-readable labels to the concepts that make up the vector space.

Model
To perform the basic variant of ESA, one starts with a collection of texts, say, all Wikipedia articles; let the number of documents in the collection be N. These are all turned into "bags of words", i.e., term frequency histograms, stored in an inverted index. Using this inverted index, one can find for any word the set of Wikipedia articles containing this word; in the vocabulary of Egozi, Markovitch and Gabrilovitch, "each word appearing in the Wikipedia corpus can be seen as triggering each of the concepts it points to in the inverted index."The output of the inverted index for a single word query is a list of indexed documents (Wikipedia articles), each given a score depending on how often the word in question occurred in them (weighted by the total number of words in the document). Mathematically, this list is an N-dimensional vector of word-document scores, where a document not containing the query word has score zero. To compute the relatedness of two words, one compares the vectors (say u and v) by computing the cosine similarity,

  
    
      
        
          
            s
            i
            m
          
        
        (
        
          u
        
        ,
        
          v
        
        )
        =
        
          
            
              
                u
              
              ⋅
              
                v
              
            
            
              ‖
              
                u
              
              ‖
              ‖
              
                v
              
              ‖
            
          
        
        =
        
          
            
              
                ∑
                
                  i
                  =
                  1
                
                
                  N
                
              
              
                u
                
                  i
                
              
              
                v
                
                  i
                
              
            
            
              
                
                  
                    ∑
                    
                      i
                      =
                      1
                    
                    
                      N
                    
                  
                  
                    u
                    
                      i
                    
                    
                      2
                    
                  
                
              
              
                
                  
                    ∑
                    
                      i
                      =
                      1
                    
                    
                      N
                    
                  
                  
                    v
                    
                      i
                    
                    
                      2
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\mathsf {sim}}(\mathbf {u} ,\mathbf {v} )={\frac {\mathbf {u} \cdot \mathbf {v} }{\|\mathbf {u} \|\|\mathbf {v} \|}}={\frac {\sum _{i=1}^{N}u_{i}v_{i}}{{\sqrt {\sum _{i=1}^{N}u_{i}^{2}}}{\sqrt {\sum _{i=1}^{N}v_{i}^{2}}}}}}
  and this gives numeric estimate of the semantic relatedness of the words. The scheme is extended from single words to multi-word texts by simply summing the vectors of all words in the text.

Analysis
ESA, as originally posited by Gabrilovich and Markovitch, operates under the assumption that the knowledge base contains topically orthogonal concepts. However, it was later shown by Anderka and Stein that ESA also improves the performance of information retrieval systems when it is based not on Wikipedia, but on the Reuters corpus of newswire articles, which does not satisfy the orthogonality property; in their experiments, Anderka and Stein used newswire stories as "concepts".
To explain this observation, links have been shown between ESA and the generalized vector space model.
Gabrilovich and Markovitch replied to Anderka and Stein by pointing out that their experimental result was achieved using "a single application of ESA (text similarity)" and "just a single, extremely small and homogenous test collection of 50 news documents".

Applications
Word relatedness
ESA is considered by its authors a measure of semantic relatedness (as opposed to semantic similarity). On datasets used to benchmark relatedness of words, ESA outperforms other algorithms, including WordNet semantic similarity measures and skip-gram Neural Network Language Model (Word2vec).

Document relatedness
ESA is used in commercial software packages for computing relatedness of documents. Domain-specific restrictions on the ESA model are sometimes used to provide more robust document matching.

Extensions
Cross-language explicit semantic analysis (CL-ESA) is a multilingual generalization of ESA.
CL-ESA exploits a document-aligned multilingual reference collection (e.g., again, Wikipedia) to represent a document as a language-independent concept vector. The relatedness of two documents in different languages is assessed by the cosine similarity between the corresponding vector representations.

See also
Topic model

References
External links
Explicit semantic analysis on Evgeniy Gabrilovich's homepage; has links to implementations
A filtered-popping recursive transition network (FPRTN), or simply filtered-popping network (FPN), is a recursive transition network (RTN) extended with a map of states to keys where returning from a subroutine jump requires the acceptor and return states to be mapped to the same key. RTNs are finite-state machines that can be seen as finite-state automata extended with a stack of return states; as well as consuming transitions and 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  -transitions, RTNs may define call transitions. These transitions perform a subroutine jump by pushing the transition's target state onto the stack and bringing the machine to the called state. Each time an acceptor state is reached, the return state at the top of the stack is popped out, provided that the stack is not empty, and the machine is brought to this state.
Throughout this article we refer to filtered-popping recursive transition networks as FPNs, though this acronym is ambiguous (e.g.: fuzzy Petri nets). Filtered-popping networks and FPRTNs are unambiguous alternatives.

Formal Definition
A FPN is a structure 
  
    
      
        (
        Q
        ,
        K
        ,
        Σ
        ,
        δ
        ,
        κ
        ,
        
          Q
          
            I
          
        
        ,
        F
        )
      
    
    {\displaystyle (Q,K,\Sigma ,\delta ,\kappa ,Q_{I},F)}
   where

  
    
      
        Q
      
    
    {\displaystyle Q}
   is a finite set of states,

  
    
      
        K
      
    
    {\displaystyle K}
   is a finite set of keys,

  
    
      
        Σ
      
    
    {\displaystyle \Sigma }
   is a finite input alphabet,

  
    
      
        δ
        :
        Q
        ×
        (
        Σ
        ∪
        {
        ε
        }
        ∪
        Q
        )
        →
        Q
      
    
    {\displaystyle \delta :Q\times (\Sigma \cup \{\varepsilon \}\cup Q)\to Q}
   is a partial transition function, 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
   being the empty symbol,

  
    
      
        κ
        :
        Q
        →
        K
      
    
    {\displaystyle \kappa :Q\to K}
   is a map of states to keys,

  
    
      
        
          Q
          
            I
          
        
        ⊆
        Q
      
    
    {\displaystyle Q_{I}\subseteq Q}
   is the set of initial states, and

  
    
      
        F
        ⊆
        Q
      
    
    {\displaystyle F\subseteq Q}
   is the set of acceptance states.

Transitions
Transitions represent the possibility of bringing the FPN from a source state 
  
    
      
        
          q
          
            s
          
        
      
    
    {\displaystyle q_{s}}
   to a target state 
  
    
      
        
          q
          
            t
          
        
      
    
    {\displaystyle q_{t}}
   by possibly performing an additional action. Depending on this action, we distinguish the following types of explicitly-defined transitions:

  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  -transitions are transitions of the form 
  
    
      
        δ
        (
        
          q
          
            s
          
        
        ,
        ε
        )
        →
        
          q
          
            t
          
        
      
    
    {\displaystyle \delta (q_{s},\varepsilon )\to q_{t}}
   and perform no additional action,
consuming transitions are transitions of the form 
  
    
      
        δ
        (
        
          q
          
            s
          
        
        ,
        σ
        )
        →
        
          q
          
            t
          
        
      
    
    {\displaystyle \delta (q_{s},\sigma )\to q_{t}}
   and consume an input symbol 
  
    
      
        σ
      
    
    {\displaystyle \sigma }
  , and
call transitions are transitions of the form 
  
    
      
        δ
        (
        
          q
          
            s
          
        
        ,
        
          q
          
            c
          
        
        )
        →
        
          q
          
            t
          
        
      
    
    {\displaystyle \delta (q_{s},q_{c})\to q_{t}}
   and perform a subroutine jump to called state 
  
    
      
        
          q
          
            c
          
        
      
    
    {\displaystyle q_{c}}
   before reaching 
  
    
      
        
          q
          
            t
          
        
      
    
    {\displaystyle q_{t}}
  .The behaviour of call transitions is governed by two kinds of implicitly-defined transitions:

for each call transition 
  
    
      
        δ
        (
        
          q
          
            s
          
        
        ,
        
          q
          
            c
          
        
        )
        →
        
          q
          
            t
          
        
      
    
    {\displaystyle \delta (q_{s},q_{c})\to q_{t}}
   the FPN implicitly defines a push transition that brings the machine from 
  
    
      
        
          q
          
            s
          
        
      
    
    {\displaystyle q_{s}}
   to 
  
    
      
        
          q
          
            c
          
        
      
    
    {\displaystyle q_{c}}
   by pushing 
  
    
      
        
          q
          
            t
          
        
      
    
    {\displaystyle q_{t}}
   onto the stack, and
for each pair of states 
  
    
      
        (
        
          q
          
            f
          
        
        ,
        
          q
          
            r
          
        
        )
        ∈
        F
        ×
        Q
      
    
    {\displaystyle (q_{f},q_{r})\in F\times Q}
   the FPN implicitly defines a pop transition that brings the machine from 
  
    
      
        
          q
          
            f
          
        
      
    
    {\displaystyle q_{f}}
   to 
  
    
      
        
          q
          
            r
          
        
      
    
    {\displaystyle q_{r}}
   by popping 
  
    
      
        
          q
          
            r
          
        
      
    
    {\displaystyle q_{r}}
   from the stack iff 
  
    
      
        
          q
          
            r
          
        
      
    
    {\displaystyle q_{r}}
   is the state at the top of the stack and 
  
    
      
        κ
        (
        
          q
          
            f
          
        
        )
        =
        κ
        (
        
          q
          
            r
          
        
        )
      
    
    {\displaystyle \kappa (q_{f})=\kappa (q_{r})}
  .Push transitions initialize subroutine jumps and pop transitions are equivalent to return statements.

Purpose
A (natural language) text can be enriched with meta-information by the application of a RTN with output; for instance, a RTN inserting XML tags can be used for transforming a plain text into a structured XML document. A RTN with output representing a natural language grammar would delimit and add the syntactic structure of each text sentence (see parsing). Other RTNs with output could simply mark text segments containing relevant information (see information extraction). The application of a RTN with output representing an ambiguous grammar results in a set of possible translations or interpretations of the input. Computing this set has an exponential worst-case cost, even for an Earley parser for RTNs with output, due to cases in which the number of translations increases exponentially w.r.t. the input length; for instance, the number of interpretations of a natural language sentence increases exponentially w.r.t. the number of unresolved prepositional phrase attachments:
in sentence the girl saw the monkey with the telescope, it is unknown whether the girl used the telescope or the monkey was holding it (21 interpretations),
in sentence the girl saw the monkey with the telescope in the garden, it is also unknown whether the monkey was in the garden or the action took place in the garden (22 interpretations),
in sentence the girl saw the monkey with the telescope in the garden under the tree, it is unknown as well whether the monkey was under the tree or the action took place under the tree (23 interpretations),
etc.FPNs serve as a compact representation of this set of translations, allowing to compute it in cubic time by means of an Earley-like parser. FPN states correspond to execution states (see instruction steps) of an Earley-parser for RTNs without output, and FPN transitions correspond to possible translations of input symbols. The 
  
    
      
        κ
      
    
    {\displaystyle \kappa }
   map of the resulting FPN gives the correspondence between the represented output segments and the recognized input segments: given a recognized input sequence 
  
    
      
        
          σ
          
            1
          
        
        …
        
          σ
          
            l
          
        
      
    
    {\displaystyle \sigma _{1}\ldots \sigma _{l}}
   and a FPN path 
  
    
      
        p
      
    
    {\displaystyle p}
   starting at a state 
  
    
      
        q
      
    
    {\displaystyle q}
   and ending at a state 
  
    
      
        
          q
          
            ′
          
        
      
    
    {\displaystyle q^{\prime }}
  , 
  
    
      
        p
      
    
    {\displaystyle p}
   represents a possible translation of input segment 
  
    
      
        
          σ
          
            κ
            (
            q
            )
            +
            1
          
        
        …
        
          σ
          
            κ
            (
            
              q
              
                ′
              
            
            )
          
        
      
    
    {\displaystyle \sigma _{\kappa (q)+1}\ldots \sigma _{\kappa (q^{\prime })}}
  . The filtered-popping feature is required in order to avoid FPN paths to represent translations of disconnected or overlapping input segments: a FPN call may contain several translation paths from the called state to an acceptor state, where the input segments they correspond to share the same start point but do not necessarily have the same length. Only return states corresponding to the same input point than the acceptor state finishing the call are valid return states.


== References ==
Robby Garner (born 1963) is an American natural language programmer and software developer. He won the 1998 and 1999 Loebner Prize contests with the program called Albert One. He is listed in the 2001 Guinness Book of World Records as having written the "most human" computer program.

Life
A native of Cedartown, Georgia, Robby attended Cedartown High School. He worked in his father's television repair shop and began programming for his family's business at age 15. He was commander of his AFJROTC squadron as a junior in high school, while attending joint-enrollment college classes at the local community college. Forming a software company called Robitron Software Research, Inc. in 1987 with his father, Robert J. Garner, and his sister Pam, he worked as a software developer until 1997 when his father retired and the company was disbanded.

Early conversational systems
One of the first web chatterbots, named Max Headcold, was written by Garner in 1995. Max served two purposes, to collect data about web chat behavior and to entertain customers of the FringeWare online bookstore. This program was eventually implemented as a Java package called JFRED, written by Paco Nathan based on the C++ FRED CGI program, and his own influences from Stanford and various corporations. Garner and Nathan took part in the world's largest online Turing test in 1998. Their JFRED program was perceived as human by 17% of the participants.

Philosophy and collaborations
A computational behaviorist after the term coined by Dr. Thomas Whalen in 1995, Garner's first attempts at simulating conversation involved collections of internet chat viewed as a sequence of stimuli and responses. Kevin Copple of Ellaz Systems has collaborated with Garner on several projects, including Copple's Ella, for which, Garner contributed voice recordings and music. Garner and Copple believe that intelligence may be built one facet at a time, rather than depending on some general purpose theory to emerge.

Loebner Prize contest
Competing in six Loebner Prize contests, he used the competition as a way to test his prototypes on the judges each year. After winning the contest twice in 1998 and 1999 with his program called Albert One, he began collaborating with other software developers in a variety of conversational systems. Garner created the Robitron Yahoo Group in 2002 as a forum and virtual watering hole for Loebner Prize contest participants and discussion of related topics.

Current works
The multifaceted approach, presented at a colloquium on conversational systems in November 2005, involves multiple chat bots working under the control of a master control program.  Using this technique, the strengths of various web agents may be united under the control of a Java applet or servlet.  The control program categorizes stimuli and delegates responses to other programs in a hierarchy. A spin-off of this technique is the Turing Hub, an automated Turing test featuring four of the top Loebner Prize contest competitors.

See also
Rollo Carpenter
Richard Wallace

References
External links
JFred Chatterbots at SimonLaven.com
A GeneRIF or Gene Reference Into Function is a short (255 characters or fewer) statement about the function of a gene.  GeneRIFs provide a simple mechanism for allowing scientists to add to the functional annotation of genes described in the Entrez Gene database. In practice, function is construed quite broadly.  For example, there are GeneRIFs that discuss the role of a gene in a disease, GeneRIFs that point the viewer towards a review article about the gene, and GeneRIFs that discuss the structure of a gene.  However, the stated intent is for GeneRIFs to be about gene function. Currently over half a million geneRIFs have been created for genes from almost 1000 different species.GeneRIFs are always associated with specific entries in the Entrez Gene database.  Each GeneRIF has a pointer to the PubMed ID (a type of document identifier) of a scientific publication that provides evidence for the statement made by the GeneRIF.  GeneRIFs are often extracted directly from the document that is identified by the PubMed ID, very frequently from its title or from its final sentence.
GeneRIFs are usually produced by NCBI indexers, but anyone may submit a GeneRIF.
To be processed, a valid Gene ID must exist for the specific gene, or the Gene staff must have assigned an overall Gene ID to the species. The latter case is implemented via records in Gene with the symbol NEWENTRY. Once the Gene ID is identified, only three types of information are required to complete a submission:

a concise phrase describing a function or functions (less than 255 characters in length, preferably more than a restatement of the title of the paper);
a published paper describing that function, implemented by supplying the PubMed ID of a citation in PubMed;
a valid e-mail address (which will remain confidential).

Example
Here are some GeneRIFs taken from Entrez Gene for GeneID 7157, the human gene TP53.
The PubMed document identifiers have been omitted from the examples.  Note the wide variability with respect to the presence or absence of punctuation and of sentence-initial capital letters.

p53 and c-erbB-2 may have independent role in carcinogenesis of gall bladder cancer
Degradation of endogenous HIPK2 depends on the presence of a functional p53 protein.
p53 codon 72 alleles influence the response to anticancer drugs in cells from aged people by regulating the cell cycle inhibitor p21WAF1
Logistic regression analysis showed p53 and COX-2 as dependent predictors in pancreatic carcinogenesis, and a reciprocal relationship to neoplastic progression between p53 and COX-2.GeneRIFs are an unusual type of textual genre, and they have recently been the subject of a number of articles from the natural language processing community.

External links
NCBI's web page describing GeneRIFs
Mitchell JA, Aronson AR, Mork JG, Folk LC, Humphrey SM, Ward JM (2003). "Gene indexing: characterization and analysis of NLM's GeneRIFs". AMIA Annu Symp Proc: 460–4. PMC 1480312. PMID 14728215.

References

William Hersh, Ravi Teja Bhupatiraju (2003). TREC Genomics Track Overview (PDF). Paper describing a Text Retrieval Conference "shared task" involving automatic prediction of GeneRIFs.
Lu, Zhiyong; K. Bretonnel Cohen; Lawrence Hunter (2006). Finding GeneRIFs via Gene Ontology annotations (PDF). Proc. Pacific Symposium on Biocomputing 2006. pp. 52–63. Archived from the original (PDF) on 2006-02-13. Lu et al.'s paper describing a system that automatically suggests GeneRIFs.
The Google Ngram Viewer or Google Books Ngram Viewer is an online search engine that charts the frequencies of any set of comma-delimited search strings using a yearly count of grams found in sources printed between 1500 and 2008 in Google's text corpora in English, Chinese (simplified), French, German, Hebrew, Italian, Russian, or Spanish. There are also some specialized English corpora, such as American English, British English, English Fiction, and English One Million; and the 2009 version of most corpora is also available.The program can search for a single word or a phrase, including misspellings or gibberish. The n-grams are matched with the text within the selected corpus, optionally using case-sensitive spelling (which compares the exact use of uppercase letters), and, if found in 40 or more books, are then plotted on a graph.The Google Ngram Viewer, as of January 2016, supports searches for parts of speech and wildcards.

History
The program was developed by Jon Orwant and Will Brockman and released in mid-December 2010. It was inspired by a prototype (called "Bookworm") created by Jean-Baptiste Michel and Erez Aiden from Harvard's Cultural Observatory and Yuan Shen from MIT and Steven Pinker.The Ngram Viewer was initially based on the 2009 edition of the Google Books Ngram Corpus. As of January 2016, the program can search an individual language's corpus within the 2009 or the 2012 edition.

Operation and restrictions
Commas delimit user-entered search-terms, indicating each separate word or phrase to find. The Ngram Viewer returns a plotted line chart within seconds of the user pressing the Enter key or the "Search" button on the screen.
As an adjustment for more books having been published during some years, the data is normalized, as a relative level, by the number of books published in each year.Google populated the database from over 5 million books published up to 2008. Accordingly, as of January 2016, no data will match beyond the year 2008, no matter if the corpus was generated in 2009 or 2012. Due to limitations on the size of the Ngram database, only matches found in at least 40 books are indexed in the database; otherwise the database could not have stored all possible combinations.Typically, search terms cannot end with punctuation, although a separate full stop (a period) can be searched. Also, an ending question mark (as in "Why?") will cause a second search for the question mark separately.Omitting the periods in abbreviations will allow a form of matching, such as using "R M S" to search for "R.M.S." versus "RMS".

Corpora
The corpora used for the search are composed of total_counts, 1-grams, 2-grams, 3-grams, 4-grams, and 5-grams files for each language. The file format of each of the files is tab-separated data. Each line has the following format:
total_counts file
year TAB match_count TAB page_count TAB volume_count NEWLINE
Version 1 ngram file (generated in July 2009)
ngram TAB year TAB match_count TAB page_count TAB volume_count NEWLINE
Version 2 ngram file (generated in July 2012)
ngram TAB year TAB match_count TAB volume_count NEWLINEThe Google Ngram Viewer uses match_count to plot the graph.
As an example, a word "Wikipedia" from the Version 2 file of the English 1-grams is stored as follows:
The graph plotted by the Google Ngram Viewer using the above data is here:

Criticism
The data set has been criticized for its reliance upon inaccurate OCR, an overabundance of scientific literature, and for including large numbers of incorrectly dated and categorized texts. Because of these errors, and because it is  uncontrolled for bias (such as the increasing amount of scientific literature, which causes other terms to appear to decline in popularity), it is risky to use this corpus to study language or test theories. Since the data set does not include metadata, it may not reflect general linguistic or cultural change and can only hint at such an effect.
Another issue is that the corpus is in effect a library, containing one of each book.  A single, prolific author is thereby able to noticeably insert new phrases into the Google Books lexicon, whether the author is widely read or not.

OCR issues
Optical character recognition, or OCR, is not always reliable, and some characters may not be scanned correctly. In particular, systemic errors like the confusion of "s" and "f" in pre-19th century texts (due to the use of the long s which was similar in appearance to "f") can cause systemic bias.  Although Google Ngram Viewer claims that the results are reliable from 1800 onwards, poor OCR and insufficient data mean that frequencies given for languages such as Chinese may only be accurate from 1970 onward, with earlier parts of the corpus showing no results at all for common terms, and data for some years containing more than 50% noise.

See also
Culturomics
Google Trends
Lexical analysis

References
Bibliography
Lin, Yuri;  et al. (July 2012). "Syntactic Annotations for the Google Books Ngram Corpus" (PDF). Proceedings of the 50th Annual Meeting. Demo Papers. Jeju, Republic of Korea: Association for Computational Linguistics. 2: 169–174. 2390499. Whitepaper presenting the 2012 edition of the Google Books Ngram Corpus

External links
Official website
A Gorn address (Gorn, 1967) is a method of identifying and addressing any node within a tree data structure. This notation is often used for identifying nodes in a parse tree defined by phrase structure rules.
The Gorn address is a sequence of zero or more integers conventionally separated by dots, e.g., 0 or 1.0.1. 
The root which Gorn calls * can be regarded as the empty sequence. 
And the 
  
    
      
        j
      
    
    {\displaystyle j}
  -th child of the 
  
    
      
        i
      
    
    {\displaystyle i}
  -th child has an address 
  
    
      
        i
        .
        j
      
    
    {\displaystyle i.j}
  , counting from 0.
It is named after American computer scientist Saul Gorn.

References

Gorn, S. (1967). Explicit definitions and linguistic dominoes. Systems and Computer Science, Eds. J. Hart & S. Takasu. 77-115. University of Toronto Press, Toronto Canada.
A grammar checker, in computing terms, is a program, or part of a program, that attempts to verify written text for grammatical correctness. Grammar checkers are most often implemented as a feature of a larger program, such as a word processor, but are also available as a stand-alone application that can be activated from within programs that work with editable text.
The implementation of a grammar checker makes use of natural language processing.

History
The earliest "grammar checkers" were programs that checked for punctuation and style inconsistencies, rather than a complete range of possible grammatical errors. The first system was called Writer's Workbench, and was a set of writing tools included with Unix systems as far back as the 1970s. The whole Writer's Workbench package included several separate tools to check for various writing problems. The "diction" tool checked for wordy, trite, clichéd or misused phrases in a text. The tool would output a list of questionable phrases, and provide suggestions for improving the writing. The "style" tool analyzed the writing style of a given text. It performed a number of readability tests on the text and output the results, and gave some statistical information about the sentences of the text.
Aspen Software of Albuquerque, New Mexico released the earliest version of a diction and style checker for personal computers, Grammatik, in 1981. Grammatik was first available for a Radio Shack - TRS-80, and soon had versions for CP/M and the IBM PC. Reference Software of San Francisco, California, acquired Grammatik in 1985. Development of Grammatik continued, and it became an actual grammar checker that could detect writing errors beyond simple style checking.
Other early diction and style checking programs included Punctuation & Style, Correct Grammar, RightWriter and PowerEdit. While all the earliest programs started out as simple diction and style checkers, all eventually added various levels of language processing, and developed some level of true grammar checking capability.
Until 1992, grammar checkers were sold as add-on programs. There were a large number of different word processing programs available at that time, with WordPerfect and Microsoft Word the top two in market share. In 1992, Microsoft decided to add grammar checking as a feature of Word, and  licensed CorrecText, a grammar checker from Houghton Mifflin that had not yet been marketed as a standalone product. WordPerfect answered Microsoft's move by acquiring Reference Software, and the direct descendant of Grammatik is still included with WordPerfect.
As of 2019, grammar checkers are built into systems like Google Docs and Sapling.ai, browser extensions like Grammarly and Scribens, desktop applications like Ginger, free and open-source software like LanguageTool, and text editor plugins like those available from WebSpellChecker Software.

Technical issues
The earliest writing style programs checked for wordy, trite, clichéd, or misused phrases in a text. This process was based on simple pattern matching. The heart of the program was a list of many hundreds or thousands of phrases that are considered poor writing by many experts. The list of questionable phrases included alternative wording for each phrase. The checking program would simply break text into sentences, check for any matches in the phrase dictionary, flag suspect phrases and show an alternative. These programs could also perform some mechanical checks. For example, they would typically flag doubled words, doubled punctuation, some capitalization errors, and other simple mechanical mistakes.
True grammar checking is more complex. While a computer programming language has a very specific syntax and grammar, this is not so for natural languages. One can write a somewhat complete formal grammar for a natural language, but there are usually so many exceptions in real usage that a formal grammar is of minimal help in writing a grammar checker. One of the most important parts of a natural language grammar checker is a dictionary of all the words in the language, along with the part of speech of each word. The fact that a natural word may be used as any one of several different parts of speech (such as "free" being used as an adjective, adverb, noun, or verb) greatly increases the complexity of any grammar checker.
A grammar checker will find each sentence in a text, look up each word in the dictionary, and then attempt to parse the sentence into a form that matches a grammar. Using various rules, the program can then detect various errors, such as agreement in tense, number, word order, and so on. It is also possible to detect some stylistic problems with the text. For example, some popular style guides such as The Elements of Style deprecate excessive use of the passive voice. Grammar checkers may attempt to identify passive sentences and suggest an active-voice alternative.
The software elements required for grammar checking are closely related to some of the development issues that need to be addressed for voice recognition software. In voice recognition, parsing can be used to help predict which word is most likely intended, based on part of speech and position in the sentence. In grammar checking, the parsing is used to detect words that fail to follow accepted grammar usage.
Recently, research has focused on developing algorithms which can recognize grammar errors based on the context of the surrounding words.

Criticism
Grammar checkers are considered as a type of foreign language writing aid which non-native speakers can use to proofread their writings as such programs endeavor to identify syntactical errors. However, as with other computerized writing aids such as spell checkers, popular grammar checkers are often criticized when they fail to spot errors and incorrectly flag correct text as erroneous.  The linguist Geoffrey K. Pullum argued in 2007 that they were generally so inaccurate as to do more harm than good: "for the most part, accepting the advice of a computer grammar checker on your prose will make it much worse, sometimes hilariously incoherent."

See also
Spell checker
Link grammar


== References ==
Grammar induction (or grammatical inference) is the process in machine learning of learning a formal grammar (usually as a collection of re-write rules or productions or alternatively as a finite state machine or automaton of some kind) from a set of observations, thus constructing a model which accounts for the characteristics of the observed objects. More generally, grammatical inference is that branch of machine learning where the instance space consists of discrete combinatorial objects such as strings, trees and graphs.

Grammar classes
Grammatical inference has often been very focused on the problem of learning finite state machines of various types (see the article Induction of regular languages for details on these approaches), since there have been efficient algorithms for this problem since the 1980s.
Since the beginning of the century, these approaches have been extended to the problem of inference of context-free grammars and richer formalisms, such as multiple context-free grammars and parallel multiple context-free grammars.
Other classes of grammars for which grammatical inference has been studied are combinatory categorial grammars, stochastic context-free grammars, contextual grammars and pattern languages.

Learning models
The simplest form of learning is where the learning algorithm merely receives a set of examples drawn from the language in question: the aim is to learn the language from examples of it (and, rarely, from counter-examples, that is, example that do not belong to the language).
However, other learning models have been studied. One frequently studied alternative is the case where the learner can ask membership queries as in the exact query learning model or minimally adequate teacher model introduced by Angluin.

Methodologies
There is a wide variety of methods for grammatical inference.  Two of the classic sources are Fu (1977) and Fu (1982). Duda, Hart & Stork (2001) also devote a brief section to the problem, and cite a number of references.  The basic trial-and-error method they present is discussed below. For approaches to infer subclasses of regular languages in particular, see Induction of regular languages. A more recent textbook is de la Higuera (2010), which covers the theory of grammatical inference of regular languages and finite state automata. D'Ulizia, Ferri and Grifoni provide a survey that explores grammatical inference methods for natural languages.

Grammatical inference by trial-and-error
The method proposed in Section 8.7 of Duda, Hart & Stork (2001) suggests successively guessing grammar rules (productions) and testing them against positive and negative observations.  The rule set is expanded so as to be able to generate each positive example, but if a given rule set also generates a negative example, it must be discarded.  This particular approach can be characterized as "hypothesis testing" and bears some similarity to Mitchel's version space algorithm. The Duda, Hart & Stork (2001) text provide a simple example which nicely illustrates the process, but the feasibility of such an unguided trial-and-error approach for more substantial problems is dubious.

Grammatical inference by genetic algorithms
Grammatical induction using evolutionary algorithms is the process of evolving a representation of the grammar of a target language through some evolutionary process. Formal grammars can easily be represented as tree structures of production rules that can be subjected to evolutionary operators. Algorithms of this sort stem from the genetic programming paradigm pioneered by John Koza. Other early work on simple formal languages used the binary string representation of genetic algorithms, but the inherently hierarchical structure of grammars couched in the EBNF language made trees a more flexible approach.
Koza represented Lisp programs as trees. He was able to find analogues to the genetic operators within the standard set of tree operators. For example, swapping sub-trees is equivalent to the corresponding process of genetic crossover, where sub-strings of a genetic code are transplanted into an individual of the next generation. Fitness is measured by scoring the output from the functions of the Lisp code. Similar analogues between the tree structured lisp representation and the representation of grammars as trees, made the application of genetic programming techniques possible for grammar induction.
In the case of grammar induction, the transplantation of sub-trees corresponds to the swapping of production rules that enable the parsing of phrases from some language. The fitness operator for the grammar is based upon some measure of how well it performed in parsing some group of sentences from the target language. In a tree representation of a grammar, a terminal symbol of a production rule corresponds to a leaf node of the tree. Its parent nodes corresponds to a non-terminal symbol (e.g. a noun phrase or a verb phrase) in the rule set. Ultimately, the root node might correspond to a sentence non-terminal.

Grammatical inference by greedy algorithms
Like all greedy algorithms, greedy grammar inference algorithms make, in iterative manner, decisions that seem to be the best at that stage.
The decisions made usually deal with things like the creation of new rules, the removal of existing rules, the choice of a rule to be applied or the merging of some existing rules.
Because there are several ways to define 'the stage' and 'the best', there are also several greedy grammar inference algorithms.
These context-free grammar generating algorithms make the decision after every read symbol:

Lempel-Ziv-Welch algorithm creates a context-free grammar in a deterministic way such that it is necessary to store only the start rule of the generated grammar.
Sequitur and its modifications.These context-free grammar generating algorithms first read the whole given symbol-sequence and then start to make decisions:

Byte pair encoding and its optimizations.

Distributional learning
A more recent approach is based on distributional learning. Algorithms using these approaches have been applied to learning context-free grammars and mildly context-sensitive languages and have been proven to be correct and efficient for large subclasses of these grammars.

Learning of pattern languages
Angluin defines a pattern to be "a string of constant symbols from Σ and variable symbols from a disjoint set".
The language of such a pattern is the set of all its nonempty ground instances  i.e. all strings resulting from consistent replacement of its variable symbols by nonempty strings of constant symbols.
A pattern is called descriptive for a finite input set of strings if its language is minimal (with respect to set inclusion) among all pattern languages subsuming the input set.
Angluin gives a polynomial algorithm to compute, for a given input string set, all descriptive patterns in one variable x.
To this end, she builds an automaton representing all possibly relevant patterns; using sophisticated arguments about word lengths, which rely on x being the only variable, the state count can be drastically reduced.Erlebach et al. give a more efficient version of Angluin's pattern learning algorithm, as well as a parallelized version.Arimura et al. show that a language class  obtained from limited unions of patterns can be learned in polynomial time.

Pattern theory
Pattern theory, formulated by Ulf Grenander, is a mathematical formalism to describe knowledge of the world as patterns. It differs from other approaches to artificial intelligence in that it does not begin by prescribing algorithms and machinery to recognize and classify patterns; rather, it prescribes a vocabulary to articulate and recast the pattern concepts in precise language.
In addition to the new algebraic vocabulary, its statistical approach was novel in its aim to:

Identify the hidden variables of a data set using real world data rather than artificial stimuli, which was commonplace at the time.
Formulate prior distributions for hidden variables and models for the observed variables that form the vertices of a Gibbs-like graph.
Study the randomness and variability of these graphs.
Create the basic classes of stochastic models applied by listing the deformations of the patterns.
Synthesize (sample) from the models, not just analyze signals with it.Broad in its mathematical coverage, pattern theory spans algebra and statistics, as well as local topological and global entropic properties.

Applications
The principle of grammar induction has been applied to other aspects of natural language processing, and has been applied (among many other problems) to semantic parsing, natural language understanding, example-based translation, morpheme analysis, and place name derivations. Grammar induction has also been used for lossless data compression and statistical inference via minimum message length (MML) and minimum description length (MDL) principles. Grammar induction has also been used in some probabilistic models of language acquisition.

See also
Artificial grammar learning#Artificial intelligence
Example-based machine translation
Inductive programming
Kolmogorov complexity
Language identification in the limit
Straight-line grammar
Syntactic pattern recognition

Notes
References
Sources
Duda, Richard O.; Hart, Peter E.; Stork, David G. (2001), Pattern Classification (2 ed.), New York: John Wiley & Sons
Fu, King Sun (1982), Syntactic Pattern Recognition and Applications, Englewood Cliffs, NJ: Prentice-Hall
Fu, King Sun (1977), Syntactic Pattern Recognition, Applications, Berlin: Springer-Verlag
Horning, James Jay (1969), A Study of Grammatical Inference (Ph.D. Thesis ed.), Stanford: Stanford University Computer Science Department, ProQuest 302483145
Gold, E. Mark (1967), Language Identification in the Limit, 10, Information and Control, pp. 447–474, archived from the original on 2016-08-28, retrieved 2016-09-04
Gold, E. Mark (1967), Language Identification in the Limit (PDF), 10, Information and Control, pp. 447–474
Grammatik was the first grammar checking program developed for home computer systems. Aspen Software of Albuquerque, NM, released the earliest version of this diction and style checker for personal computers, c. 1981 - 1983. It was inspired by the Writer's Workbench.Grammatik was first available for a Radio Shack - TRS-80, and soon had versions for CP/M and the IBM PC. Reference Software of San Francisco, California, acquired Grammatik in 1985. Development of Grammatik continued, and it became an actual grammar checker that could detect writing errors beyond simple style checking.Subsequent versions were released for the MS-DOS, Windows, Macintosh and Unix platforms. Grammatik was ultimately acquired by WordPerfect Corporation and is integrated in the WordPerfect word processor.


== References ==
Machine translation is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one natural language to another.
In the 1950s, machine translation became a reality in research, although references to the subject can be found as early as the 17th century. The Georgetown experiment, which involved successful fully automatic translation of more than sixty Russian sentences into English in 1954, was one of the earliest recorded projects. Researchers of the Georgetown experiment asserted their belief that machine translation would be a solved problem within three to five years. In the Soviet Union, similar experiments were performed shortly after. 
Consequently, the success of the experiment ushered in an era of significant funding for machine translation research in the United States. The achieved progress was much slower than expected; in 1966, the ALPAC report found that ten years of research had not fulfilled the expectations of the Georgetown experiment and resulted in dramatically reduced funding.
Interest grew in statistical models for machine translation, which became more common and also less expensive in the 1980s as available computational power increased.
Although there exists no autonomous system of "fully automatic high quality translation of unrestricted text," there are many programs now available that are capable of providing useful output within strict constraints. Several of these programs are available online, such as Google Translate and the SYSTRAN system that powers AltaVista's BabelFish (now Yahoo's Babelfish as of 9 May 2008).

The beginning
The origins of machine translation can be traced back to the work of Al-Kindi, a 9th-century Arabic cryptographer who developed techniques for systemic language translation, including cryptanalysis, frequency analysis, and probability and statistics, which are used in modern machine translation. The idea of machine translation later appeared in the 17th century. In 1629, René Descartes proposed a universal language, with equivalent ideas in different tongues sharing one symbol.In the mid-1930s the first patents for "translating machines" were applied for by Georges Artsrouni, for an automatic bilingual dictionary using paper tape. Russian Peter Troyanskii submitted a more detailed proposal that included both the bilingual dictionary and a method for dealing with grammatical roles between languages, based on the grammatical system of Esperanto. This system was separated into three stages: stage one consisted of a native-speaking editor in the source language to organize the words into their logical forms and to exercise the syntactic functions; stage two required the machine to "translate" these forms into the target language; and stage three required a native-speaking editor in the target language to normalize this output.  Troyanskii's proposal remained unknown until the late 1950s, by which time computers were well-known and utilized.

The early years
The first set of proposals for computer based machine translation was presented in 1949 by Warren Weaver, a researcher at the Rockefeller Foundation,  "Translation memorandum". These proposals were based on information theory, successes in code breaking during the Second World War, and theories about the universal principles underlying natural language.
A few years after Weaver submitted his proposals, research began in earnest at many universities in the United States. On 7 January 1954 the Georgetown-IBM experiment was held in New York at the head office of IBM. This was the first public demonstration of a machine translation system. The demonstration was widely reported in the newspapers and garnered public interest. The system itself, however, was no more than a "toy" system. It had only 250 words and translated 49 carefully selected Russian sentences into English – mainly in the field of chemistry. Nevertheless, it encouraged the idea that machine translation was imminent and stimulated the financing of the research, not only in the US but worldwide.Early systems used large bilingual dictionaries and hand-coded rules for fixing the word order in the final output which was eventually considered too restrictive in linguistic developments at the time. For example, generative linguistics and transformational grammar were exploited to improve the quality of translations. During this period operational systems were installed. The United States Air Force used a system produced by IBM and Washington University, while the Atomic Energy Commission and Euratom, in Italy, used a system developed at Georgetown University. While the quality of the output was poor it met many of the customers' needs, particularly in terms of speed.At the end of the 1950s, Yehoshua Bar-Hillel was asked by the US government to look into machine translation, to assess the possibility of fully automatic high quality translation by machines.  Bar-Hillel described the problem of semantic ambiguity or double-meaning, as illustrated in the following sentence:

Little John was looking for his toy box. Finally he found it. The box was in the pen.
The word pen may have two meanings: the first meaning, something used to write in ink with; the second meaning, a container of some kind. To a human, the meaning is obvious, but Bar-Hillel claimed that without a "universal encyclopedia" a machine would never be able to deal with this problem. At the time, this type of semantic ambiguity could only be solved by writing source texts for machine translation in a controlled language that uses a vocabulary in which each word has exactly one meaning.

The 1960s, the ALPAC report and the seventies
Research in the 1960s in both the Soviet Union and the United States concentrated mainly on the Russian–English language pair. The objects of translation were chiefly scientific and technical documents, such as articles from scientific journals. The rough translations produced were sufficient to get a basic understanding of the articles. If an article discussed a subject deemed to be confidential, it was sent to a human translator for a complete translation; if not, it was discarded.
A great blow came to machine-translation research in 1966 with the publication of the ALPAC report. The report was commissioned by the US government and delivered by ALPAC, the Automatic Language Processing Advisory Committee, a group of seven scientists convened by the US government in 1964. The US government was concerned that there was a lack of progress being made despite significant expenditure. The report concluded that machine translation was more expensive, less accurate and slower than human translation, and that despite the expenditures, machine translation was not likely to reach the quality of a human translator in the near future.
The report recommended, however, that tools be developed to aid translators – automatic dictionaries, for example – and that some research in computational linguistics should continue to be supported.
The publication of the report had a profound impact on research into machine translation in the United States, and to a lesser extent the Soviet Union and United Kingdom. Research, at least in the US, was almost completely abandoned for over a decade. In Canada, France and Germany, however, research continued.  In the US the main exceptions were the founders of Systran (Peter Toma) and Logos (Bernard Scott), who established their companies in 1968 and 1970 respectively and served the US Department of Defense.  In 1970, the Systran system was installed for the United States Air Force, and subsequently by the Commission of the European Communities in 1976. The METEO System, developed at the Université de Montréal, was installed in Canada in 1977 to translate weather forecasts from English to French, and was translating close to 80,000 words per day or 30 million words per year until it was replaced by a competitor's system on 30 September 2001.While research in the 1960s concentrated on limited language pairs and input, demand in the 1970s was for low-cost systems that could translate a range of technical and commercial documents. This demand was spurred by the increase of globalisation and the demand for translation in Canada, Europe, and Japan.

The 1980s and early 1990s
By the 1980s, both the diversity and the number of installed systems for machine translation had increased. A number of systems relying on mainframe technology were in use, such as Systran, Logos, Ariane-G5, and Metal.As a result of the improved availability of microcomputers, there was a market for lower-end machine translation systems. Many companies took advantage of this in Europe, Japan, and the USA. Systems were also brought onto the market in China, Eastern Europe, Korea, and the Soviet Union.During the 1980s there was a lot of activity in MT in Japan especially.  With the fifth generation computer Japan intended to leap over its competition in computer hardware and software, and one project that many large Japanese electronics firms found themselves involved in was creating software for translating into and from English (Fujitsu, Toshiba, NTT, Brother, Catena, Matsushita, Mitsubishi, Sharp, Sanyo, Hitachi, NEC, Panasonic, Kodensha, Nova, Oki).Research during the 1980s typically relied on translation through some variety of intermediary linguistic representation involving morphological, syntactic, and semantic analysis.At the end of the 1980s, there was a large surge in a number of novel methods for machine translation. One system was developed at IBM that was based on statistical methods. Makoto Nagao and his group used methods based on large numbers of translation examples, a technique that is now termed example-based machine translation. A defining feature of both of these approaches was the neglect of syntactic and semantic rules and reliance instead on the manipulation of large text corpora.
During the 1990s, encouraged by successes in speech recognition and speech synthesis, research began into speech translation with the development of the German Verbmobil project.
The Forward Area Language Converter (FALCon) system, a machine translation technology designed by the Army Research Laboratory, was fielded 1997 to translate documents for soldiers in Bosnia.There was significant growth in the use of machine translation as a result of the advent of low-cost and more powerful computers.  It was in the early 1990s that machine translation began to make the transition away from large mainframe computers toward personal computers and workstations.  Two companies that led the PC market for a time were Globalink and MicroTac, following which a merger of the two companies (in December 1994) was found to be in the corporate interest of both. Intergraph and Systran also began to offer PC versions around this time. Sites also became available on the internet, such as AltaVista's Babel Fish (using Systran technology) and Google Language Tools (also initially using Systran technology exclusively).

2000s
The field of machine translation has seen major changes in the last few years. Currently a large amount of research is being done into statistical machine translation and example-based machine translation. 
In the area of speech translation, research has focused on moving from domain-limited systems to domain-unlimited translation systems. In different research projects in Europe (like TC-STAR) and in the United States (STR-DUST and US-DARPA-GALE), solutions for automatically translating Parliamentary speeches and broadcast news have been developed. In these scenarios the domain of the content is no longer limited to any special area, but rather the speeches to be translated cover a variety of topics.
More recently, the French-German project Quaero investigates the possibility of making use of machine translations for a multi-lingual internet. The project seeks to translate not only webpages, but also videos and audio files on the internet.
Today, only a few companies use statistical machine translation commercially, e.g. Omniscien Technologies (formerly Asia Online), SDL / Language Weaver (sells translation products and services), Google (uses its proprietary statistical MT system for some language combinations in Google's language tools), Microsoft (uses its proprietary statistical MT system to translate knowledge base articles), and Ta with you (offers a domain-adapted machine translation solution based on statistical MT with some linguistic knowledge). There has been a renewed interest in hybridisation, with researchers combining syntactic and morphological (i.e., linguistic) knowledge into statistical systems, as well as combining statistics with existing rule-based systems.

See also
History of natural language processing
ALPAC report
Computer-aided translation
Lighthill report
Machine translation

Notes
References
Hutchins, J. (2005). "Milestones in machine translation – No.6: Bar-Hillel and the nonfeasibility of FAHQT]" (PDF).
Van Slype, Georges (1983). Better translation for better communication. Paris: Pergamon Press. ISBN 9780080305349.

Further reading
Hutchins, W. John (1986). Machine Translation: past, present, future. Ellis Horwood series in computers and their applications. Chichester: Ellis Horwood. ISBN 0470203137.
The history of natural language processing describes the advances of natural language processing  (Outline of natural language processing). There is some overlap with the history of machine translation, the history of speech recognition, and the history of artificial intelligence.

Research and development
The history of machine translation dates back to the seventeenth century, when philosophers such as Leibniz and Descartes put forward proposals for codes which would relate words between languages. All of these proposals remained theoretical, and none resulted in the development of an actual machine.
The first patents for "translating machines" were applied for in the mid-1930s. One proposal, by Georges Artsrouni was simply an automatic bilingual dictionary using paper tape. The other proposal, by Peter Troyanskii, a Russian, was more detailed. It included both the bilingual dictionary, and a method for dealing with grammatical roles between languages, based on Esperanto. 
In 1950, Alan Turing published his famous article "Computing Machinery and Intelligence" which proposed what is now called the Turing test as a criterion of intelligence. This criterion depends on the ability of a computer program to impersonate a human in a real-time written conversation with a human judge, sufficiently well that the judge is unable to distinguish reliably — on the basis of the conversational content alone — between the program and a real human.
In 1957, Noam Chomsky’s Syntactic Structures revolutionized Linguistics with 'universal grammar', a rule based system of syntactic structures.The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.  However, real progress was much slower, and after the ALPAC report in 1966, which found that ten years long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced.  Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.
Some notably successful NLP systems developed in the 1960s were SHRDLU, a natural language system working in restricted "blocks worlds" with restricted vocabularies.
In 1969 Roger Schank introduced the conceptual dependency theory for natural language understanding. This model, partially influenced by the work of Sydney Lamb, was extensively used by Schank's students at Yale University, such as Robert Wilensky, Wendy Lehnert, and Janet Kolodner.
In 1970, William A. Woods introduced the augmented transition network (ATN) to represent natural language input. Instead of phrase structure rules ATNs used an equivalent set of finite state automata that were called recursively. ATNs and their more general format called "generalized ATNs" continued to be used for a number of years.During the 70's many programmers began to write 'conceptual ontologies', which structured real-world information into computer-understandable data.  Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981).  During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.

Up to the 1980s, most NLP systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in NLP with the introduction of machine learning algorithms for language processing.  This was due both to the steady increase in computational power resulting from Moore's Law and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing. Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules.  Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models.  Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.
Many of the notable early successes occurred in the field of machine translation, due especially to work at IBM Research, where successively more complicated statistical models were developed.  These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government.  However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems.  As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.
Recent research has increasingly focused on unsupervised and semi-supervised learning algorithms.  Such algorithms are able to learn from data that has not been hand-annotated with the desired answers, or using a combination of annotated and non-annotated data.  Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data.  However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results.

Software
References
Bibliography
Crevier, Daniel (1993), AI: The Tumultuous Search for Artificial Intelligence, New York, NY: BasicBooks, ISBN 0-465-02997-3
McCorduck, Pamela (2004), Machines Who Think (2nd ed.), Natick, MA: A. K. Peters, Ltd., ISBN 978-1-56881-205-2, OCLC 52197627.
Russell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2.
iGlue is an experimental database with detailed search options, containing entities and information editing tool. It organizes interrelated images, videos, individuals, institutions, objects, websites, geographical locations into cohesive data structures.The most important components of iGlue system are: the flexible database which contains semantic elements, and entities, and their relational connections.
In4 Kft. was established in August 2007 as a company specialised in the development of online applications based on university researches, with the participation of young university researchers and of the Power of the Dream Ventures, as financial investor.

References
External links
Official website
Official iGlue Blog
iGlue at Crunch base
Geek & Rolla
iGlue on CreativeBits
Népszabadság article (Hungarian)
kereses.blog.hu article (Hungarian)
Information extraction (IE), information retrieval (IR) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents and other electronically represented sources. In most of the cases this activity concerns processing human language texts by means of natural language processing (NLP). Recent activities in multimedia document processing like automatic annotation and content extraction out of images/audio/video/documents could be seen as information extraction
Due to the difficulty of the problem, current approaches to IE (IR) focus on narrowly restricted domains.  An example is the extraction from newswire reports of corporate mergers, such as denoted by the formal relation: 

  
    
      
        
          M
          e
          r
          g
          e
          r
          B
          e
          t
          w
          e
          e
          n
        
        (
        c
        o
        m
        p
        a
        n
        
          y
          
            1
          
        
        ,
        c
        o
        m
        p
        a
        n
        
          y
          
            2
          
        
        ,
        d
        a
        t
        e
        )
      
    
    {\displaystyle \mathrm {MergerBetween} (company_{1},company_{2},date)}
  ,from an online news sentence such as:

"Yesterday, New York based Foo Inc. announced their acquisition of Bar Corp."A broad goal of IE is to allow computation to be done on the previously unstructured data.  A more specific goal is to allow logical reasoning to draw inferences based on the logical content of the input data.  Structured data is semantically well-defined data from a chosen target domain, interpreted with respect to category and context.
Information Extraction is the part of a greater puzzle which deals with the problem of devising automatic methods for text management, beyond its transmission, storage and display. The discipline of information retrieval (IR) has developed automatic methods, typically of a statistical flavor, for indexing large document collections and classifying documents. Another complementary approach is that of natural language processing (NLP) which has solved the problem of modelling human language processing with considerable success when taking into account the magnitude of the task. In terms of both difficulty and emphasis,  IE deals with tasks in between both IR and NLP. In terms of input, IE assumes the existence of a set of documents in which each document follows a template, i.e. describes one or more entities or events  in a manner that is similar to those in other documents but differing in the details. An example, consider a group of newswire articles on Latin American terrorism with each article presumed to be based upon one or more terroristic acts. We also define for any given IE task a template, which is a(or a set of) case frame(s) to hold the information contained in a single document. For the terrorism example, a template would have slots corresponding to the perpetrator, victim, and weapon of the terroristic act, and the date on which the event happened. An IE system for this problem is required to “understand” an attack article only enough to find data corresponding to the slots in this template.

History
Information extraction dates back to the late 1970s in the early days of NLP.  An early commercial system from the mid-1980s was JASPER built for Reuters by the Carnegie Group Inc</ref> with the aim of providing real-time financial news to financial traders.Beginning in 1987, IE was spurred by a series of Message Understanding Conferences. MUC is a competition-based conference that focused on the following domains: 

MUC-1 (1987), MUC-2 (1989): Naval operations messages.
MUC-3 (1991), MUC-4 (1992): Terrorism in Latin American countries.
MUC-5 (1993): Joint ventures and microelectronics domain.
MUC-6 (1995): News articles on management changes.
MUC-7 (1998): Satellite launch reports.Considerable support came from the U.S. Defense Advanced Research Projects Agency (DARPA), who wished to automate mundane tasks performed by government analysts, such as scanning newspapers for possible links to terrorism.

Present significance
The present significance of IE pertains to the growing amount of information available in unstructured form. Tim Berners-Lee, inventor of the world wide web, refers to the existing Internet as the web of documents  and advocates that more of the content be made available as a web of data.  Until this transpires, the web largely consists of unstructured documents lacking semantic metadata.  Knowledge contained within these documents can be made more accessible for machine processing by means of transformation into relational form, or by marking-up with XML tags.  An intelligent agent monitoring a news data feed requires IE to transform unstructured data into something that can be reasoned with.  A typical application of IE is to scan a set of documents written in a natural language and populate a database with the information extracted.

Tasks and subtasks
Applying information extraction to text is linked to the problem of text simplification in order to create a structured view of the information present in free text. The overall goal being to create a more easily machine-readable text to process the sentences. Typical IE tasks and subtasks include:

Template filling: Extracting a fixed set of fields from a document, e.g. extract perpetrators, victims, time, etc. from a newspaper article about a terrorist attack.
Event extraction: Given an input document, output zero or more event templates. For instance, a newspaper article might describe multiple terrorist attacks.
Knowledge Base Population: Fill a database of facts given a set of documents. Typically the database is in the form of triplets, (entity 1, relation, entity 2), e.g. (Barack Obama, Spouse, Michelle Obama)
Named entity recognition: recognition of known entity names (for people and organizations), place names, temporal expressions, and certain types of numerical expressions, by employing existing knowledge of the domain or information extracted from other sentences. Typically the recognition task involves assigning a unique identifier to the extracted entity. A simpler task is named entity detection, which aims at detecting entities without having any existing knowledge about the entity instances. For example, in processing the sentence "M. Smith likes fishing", named entity detection would denote detecting that the phrase "M. Smith" does refer to a person, but without necessarily having (or using) any knowledge about a certain M. Smith who is (or, "might be") the specific person whom that sentence is talking about.
Coreference resolution: detection of coreference and anaphoric links between text entities. In IE tasks, this is typically restricted to finding links between previously-extracted named entities. For example, "International Business Machines" and "IBM" refer to the same real-world entity. If we take the two sentences "M. Smith likes fishing. But he doesn't like biking", it would be beneficial to detect that "he" is referring to the previously detected person "M. Smith".
Relationship extraction: identification of relations between entities, such as:
PERSON works for ORGANIZATION (extracted from the sentence "Bill works for IBM.")
PERSON located in LOCATION (extracted from the sentence "Bill is in France.")
Semi-structured information extraction which may refer to any IE that tries to restore some kind of information structure that has been lost through publication, such as:
Table extraction: finding and extracting tables from documents .
Table information extraction : extracting information in structured manner from the tables. This is more complex task than table extraction, as table extraction is only the first step, while understanding the roles of the cells, rows, columns, linking the information inside the table and understanding the information presented in the table are additional tasks necessary for table information extraction. 
Comments extraction : extracting comments from actual content of article in order to restore the link between author of each sentence
Language and vocabulary analysis
Terminology extraction: finding the relevant terms for a given corpus
Audio extraction
Template-based music extraction: finding relevant characteristic in an audio signal taken from a given repertoire; for instance  time indexes of occurrences of percussive sounds can be extracted in order to represent the essential rhythmic component of a music piece.Note that this list is not exhaustive and that the exact meaning of IE activities is not commonly accepted and that many approaches combine multiple sub-tasks of IE in order to achieve a wider goal. Machine learning, statistical analysis and/or natural language processing are often used in IE.
IE on non-text documents is becoming an increasingly interesting topic in research, and information extracted from multimedia documents can now be expressed in a high level structure as it is done on text. This naturally leads to the fusion of extracted information from multiple kinds of documents and sources.

World Wide Web applications
IE has been the focus of the MUC conferences. The proliferation of the Web, however, intensified the need for developing IE systems that help people to cope with the enormous amount of data that is available online. Systems that perform IE from online text should meet the requirements of low cost, flexibility in development and easy adaptation to new domains. MUC systems fail to meet those criteria. Moreover, linguistic analysis performed for unstructured text does not exploit the HTML/XML tags and the layout formats that are available in online texts. As a result, less linguistically intensive approaches have been developed for IE on the Web using wrappers, which are sets of highly accurate rules that extract a particular page's content. Manually developing wrappers has proved to be a time-consuming task, requiring a high level of expertise. Machine learning techniques, either supervised or unsupervised, have been used to induce such rules automatically.
Wrappers typically handle highly structured collections of web pages, such as product catalogs and telephone directories. They fail, however, when the text type is less structured, which is also common on the Web. Recent effort on adaptive information extraction motivates the development of IE systems that can handle different types of text, from well-structured to almost free text -where common wrappers fail- including mixed types. Such systems can exploit shallow natural language knowledge and thus can be also applied to less structured texts.
A recent development is Visual Information Extraction, that relies on rendering a webpage in a browser and creating rules based on the proximity of regions in the rendered web page. This helps in extracting entities from complex web pages that may exhibit a visual pattern, but lack a discernible pattern in the HTML source code.

Approaches
The following standard approaches are now widely accepted:

Hand-written regular expressions (or nested group of regular expressions)
Using classifiers
Generative: naïve Bayes classifier
Discriminative: maximum entropy models such as Multinomial logistic regression
Sequence models
Recurrent neural network
Hidden Markov model
Conditional Markov model (CMM) / Maximum-entropy Markov model (MEMM)
Conditional random fields (CRF) are commonly used in conjunction with IE for tasks as varied as extracting information from research papers to extracting navigation instructions.Numerous other approaches exist for IE including hybrid approaches that combine some of the standard approaches previously listed.

Free or open source software and services
General Architecture for Text Engineering (GATE) is bundled with a free Information Extraction system
Apache OpenNLP is a Java machine learning toolkit for natural language processing
OpenCalais is an automated information extraction web service from Thomson Reuters (Free limited version)
Machine Learning for Language Toolkit (Mallet) is a Java-based package for a variety of natural language processing tasks, including information extraction.
DBpedia Spotlight is an open source tool in Java/Scala (and free web service) that can be used for named entity recognition and name resolution.
Natural Language Toolkit  is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for the Python programming language
See also CRF implementations

See also
References
External links
Alias-I "competition" page A listing of academic toolkits and industrial toolkits for natural language information extraction.
Gabor Melli's page on IE Detailed description of the information extraction task.
Information retrieval (IR) is the activity of obtaining information system resources that are relevant to an information need from a collection of those resources.  Searches can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of texts, images or sounds.
Automated information retrieval systems are used to reduce what has been called information overload. An IR system is a software system that provides access to books, journals and other documents; stores and manages those documents. Web search engines are the most visible IR applications.

Overview
An information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of relevancy.
An object is an entity that is represented by information in a content collection or database. User queries are matched against the database information. However, as opposed to classical SQL queries of a database, in information retrieval the results returned may or may not match the query, so results are typically ranked. This ranking of results is a key difference of information retrieval searching compared to database searching.Depending on the application the data objects may be, for example, text documents, images, audio, mind maps or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata.
Most IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.

History
there is ... a machine called the Univac ... whereby letters and figures are coded as a pattern of magnetic spots on a long steel tape. By this means the text of a document, preceded by its subject code symbol, ca be recorded ... the machine ... automatically selects and types out those references which have been coded in any desired way at a rate of 120 words a minute
The idea of using computers to search for relevant pieces of information was popularized in the article As We May Think by Vannevar Bush in 1945. It would appear that Bush was inspired by patents for a 'statistical machine' - filed by Emanuel Goldberg in the 1920s and '30s - that searched for documents stored on film. The first description of a computer searching for information was described by Holmstrom in 1948, detailing an early mention of the Univac computer. Automated information retrieval systems were introduced in the 1950s: one even featured in the 1957 romantic comedy, Desk Set. In the 1960s, the first large information retrieval research group was formed by Gerard Salton at Cornell. By the 1970s several different retrieval techniques had been shown to perform well on small text corpora such as the Cranfield collection (several thousand documents). Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s.
In 1992, the US Department of Defense along with the National Institute of Standards and Technology (NIST), cosponsored the Text Retrieval Conference (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that scale to huge corpora. The introduction of web search engines has boosted the need for very large scale retrieval systems even further.

Model types
For effectively retrieving relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model.

First dimension: mathematical basis
Set-theoretic models represent documents as sets of words or phrases. Similarities are usually derived from set-theoretic operations on those sets. Common models are:
Standard Boolean model
Extended Boolean model
Fuzzy retrieval
Algebraic models represent documents and queries usually as vectors, matrices, or tuples. The similarity of the query vector and document vector is represented as a scalar value.
Vector space model
Generalized vector space model
(Enhanced) Topic-based Vector Space Model
Extended Boolean model
Latent semantic indexing a.k.a. latent semantic analysis
Probabilistic models treat the process of document retrieval as a probabilistic inference. Similarities are computed as probabilities that a document is relevant for a given query. Probabilistic theorems like the Bayes' theorem are often used in these models.
Binary Independence Model
Probabilistic relevance model on which is based the okapi (BM25) relevance function
Uncertain inference
Language models
Divergence-from-randomness model
Latent Dirichlet allocation
Feature-based retrieval models view documents as vectors of values of feature functions (or just features) and seek the best way to combine these features into a single relevance score, typically by learning to rank methods. Feature functions are arbitrary functions of document and query, and as such can easily incorporate almost any other retrieval model as just another feature.

Second dimension: properties of the model
Models without term-interdependencies treat different terms/words as independent. This fact is usually represented in vector space models by the orthogonality assumption of term vectors or in probabilistic models by an independency assumption for term variables.
Models with immanent term interdependencies allow a representation of interdependencies between terms. However the degree of the interdependency between two terms is defined by the model itself. It is usually directly or indirectly derived (e.g. by dimensional reduction) from the co-occurrence of those terms in the whole set of documents.
Models with transcendent term interdependencies allow a representation of interdependencies between terms, but they do not allege how the interdependency between two terms is defined. They rely an external source for the degree of interdependency between two terms. (For example, a human or sophisticated algorithms.)

Performance and correctness measures
The evaluation of an information retrieval system' is the process of assessing how well a system meets the information needs of its users. In general, measurement considers a collection of documents to be searched and a search query. Traditional evaluation metrics, designed for Boolean retrieval or top-k retrieval, include precision and recall. All measures assume a ground truth notion of relevancy: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be ill-posed and there may be different shades of relevancy.

Timeline
Before the 1900s
1801: Joseph Marie Jacquard invents the Jacquard loom, the first machine to use punched cards to control a sequence of operations.
1880s: Herman Hollerith invents an electro-mechanical data tabulator using punch cards as a machine readable medium.
1890 Hollerith cards, keypunches and tabulators used to process the 1890 US Census data.
1920s-1930s
Emanuel Goldberg submits patents for his "Statistical Machine” a document search engine that used photoelectric cells and pattern recognition to search the metadata on rolls of microfilmed documents.
1940s–1950s
late 1940s: The US military confronted problems of indexing and retrieval of wartime scientific research documents captured from Germans.
1945: Vannevar Bush's As We May Think appeared in Atlantic Monthly.
1947: Hans Peter Luhn (research engineer at IBM since 1941) began work on a mechanized punch card-based system for searching chemical compounds.
1950s: Growing concern in the US for a "science gap" with the USSR motivated, encouraged funding and provided a backdrop for mechanized literature searching systems (Allen Kent et al.) and the invention of citation indexing (Eugene Garfield).
1950: The term "information retrieval" was coined by Calvin Mooers.
1951: Philip Bagley conducted the earliest experiment in computerized document retrieval in a master thesis at MIT.
1955: Allen Kent joined Case Western Reserve University, and eventually became associate director of the Center for Documentation and Communications Research. That same year, Kent and colleagues published a paper in American Documentation describing the precision and recall measures as well as detailing a proposed "framework" for evaluating an IR system which included statistical sampling methods for determining the number of relevant documents not retrieved.
1958: International Conference on Scientific Information Washington DC included consideration of IR systems as a solution to problems identified. See: Proceedings of the International Conference on Scientific Information, 1958 (National Academy of Sciences, Washington, DC, 1959)
1959: Hans Peter Luhn published "Auto-encoding of documents for information retrieval."
1960s:
early 1960s: Gerard Salton began work on IR at Harvard, later moved to Cornell.
1960: Melvin Earl Maron and John Lary Kuhns published "On relevance, probabilistic indexing, and information retrieval" in the Journal of the ACM 7(3):216–244, July 1960.
1962:
Cyril W. Cleverdon published early findings of the Cranfield studies, developing a model for IR system evaluation. See: Cyril W. Cleverdon, "Report on the Testing and Analysis of an Investigation into the Comparative Efficiency of Indexing Systems". Cranfield Collection of Aeronautics, Cranfield, England, 1962.
Kent published Information Analysis and Retrieval.
1963:
Weinberg report "Science, Government and Information" gave a full articulation of the idea of a "crisis of scientific information."  The report was named after Dr. Alvin Weinberg.
Joseph Becker and Robert M. Hayes published text on information retrieval. Becker, Joseph; Hayes, Robert Mayo. Information storage and retrieval: tools, elements, theories. New York, Wiley (1963).
1964:
Karen Spärck Jones finished her thesis at Cambridge, Synonymy and Semantic Classification, and continued work on computational linguistics as it applies to IR.
The National Bureau of Standards sponsored a symposium titled "Statistical Association Methods for Mechanized Documentation." Several highly significant papers, including G. Salton's first published reference (we believe) to the SMART system.
mid-1960s:
National Library of Medicine developed MEDLARS Medical Literature Analysis and Retrieval System, the first major machine-readable database and batch-retrieval system.
Project Intrex at MIT.
1965: J. C. R. Licklider published Libraries of the Future.
1966: Don Swanson was involved in studies at University of Chicago on Requirements for Future Catalogs.
late 1960s: F. Wilfrid Lancaster completed evaluation studies of the MEDLARS system and published the first edition of his text on information retrieval.
1968:Gerard Salton published Automatic Information Organization and Retrieval.
John W. Sammon, Jr.'s RADC Tech report "Some Mathematics of Information Storage and Retrieval..." outlined the vector model.1969: Sammon's "A nonlinear mapping for data structure analysis" (IEEE Transactions on Computers) was the first proposal for visualization interface to an IR system.
1970s
early 1970s:
First online systems—NLM's AIM-TWX, MEDLINE; Lockheed's Dialog; SDC's ORBIT.
Theodor Nelson promoting concept of hypertext, published Computer Lib/Dream Machines.
1971: Nicholas Jardine and Cornelis J. van Rijsbergen published "The use of hierarchic clustering in information retrieval", which articulated the "cluster hypothesis."
1975: Three highly influential publications by Salton fully articulated his vector processing framework and term discrimination model:
A Theory of Indexing (Society for Industrial and Applied Mathematics)
A Theory of Term Importance in Automatic Text Analysis (JASIS v. 26)
A Vector Space Model for Automatic Indexing (CACM 18:11)
1978: The First ACM SIGIR conference.
1979: C. J. van Rijsbergen published Information Retrieval (Butterworths). Heavy emphasis on probabilistic models.
1979: Tamas Doszkocs implemented the CITE natural language user interface for MEDLINE at the National Library of Medicine. The CITE system supported free form query input, ranked output and relevance feedback.
1980s
1980: First international ACM SIGIR conference, joint with British Computer Society IR group in Cambridge.
1982: Nicholas J. Belkin, Robert N. Oddy, and Helen M. Brooks proposed the ASK (Anomalous State of Knowledge) viewpoint for information retrieval. This was an important concept, though their automated analysis tool proved ultimately disappointing.
1983: Salton (and Michael J. McGill) published Introduction to Modern Information Retrieval (McGraw-Hill), with heavy emphasis on vector models.
1985: David Blair and Bill Maron publish: An Evaluation of Retrieval Effectiveness for a Full-Text Document-Retrieval System
mid-1980s: Efforts to develop end-user versions of commercial IR systems.
1985–1993: Key papers on and experimental systems for visualization interfaces.
Work by Donald B. Crouch, Robert R. Korfhage, Matthew Chalmers, Anselm Spoerri and others.
1989: First World Wide Web proposals by Tim Berners-Lee at CERN.
1990s
1992: First TREC conference.
1997: Publication of Korfhage's Information Storage and Retrieval with emphasis on visualization and multi-reference point systems.
1999: Publication of Ricardo Baeza-Yates and Berthier Ribeiro-Neto's Modern Information Retrieval by Addison Wesley, the first book that attempts to cover all IR.
late 1990s: Web search engines implementation of many features formerly found only in experimental IR systems. Search engines become the most common and maybe best instantiation of IR models.

Major conferences
SIGIR: Conference on Research and Development in Information Retrieval
ECIR: European Conference on Information Retrieval
CIKM: Conference on Information and Knowledge Management
WWW: International World Wide Web Conference
WSDM: Conference on Web Search and Data Mining
ICTIR: International Conference on Theory of Information Retrieval

Awards in the field
Tony Kent Strix award
Gerard Salton Award

See also
References
Further reading
Ricardo Baeza-Yates, Berthier Ribeiro-Neto. Modern Information Retrieval: The Concepts and Technology behind Search (second edition). Addison-Wesley, UK, 2011.
Stefan Büttcher, Charles L. A. Clarke, and Gordon V. Cormack. Information Retrieval: Implementing and Evaluating Search Engines. MIT Press, Cambridge, Massachusetts, 2010.
"Information Retrieval System". Library & Information Science Network. 24 April 2015.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. Introduction to Information Retrieval. Cambridge University Press, 2008.

External links
ACM SIGIR: Information Retrieval Special Interest Group
BCS IRSG: British Computer Society - Information Retrieval Specialist Group
Text Retrieval Conference (TREC)
Forum for Information Retrieval Evaluation (FIRE)
Information Retrieval (online book) by C. J. van Rijsbergen
Information Retrieval Wiki
Information Retrieval Facility
Information Retrieval @ DUTH
TREC report on information retrieval evaluation techniques
How eBay measures search relevance
Information retrieval performance evaluation tool @ Athena Research Centre
The International Conference on Language Resources and Evaluation is an international conference organised by the European Language Resources Association every other year (on even years) with the support of institutions and organisations involved in Natural language processing. The series of LREC conferences was launched in Granada in 1998.

History of conferences
Forthcoming conference:

LREC 2020 Marseille (France)Past conferences:

2018 Miyazaki (Japan)
2016 Portorož (Slovenia)
2014 Reykjavík (Iceland)
2012 Istanbul (Turkey)
2010 Valletta (Malta)
2008 Marrakech (Morocco)
2006 Genoa (Italy)
2004 Lisbon (Portugal)
2002 Las Palmas (Spain)
2000 Athens (Greece)
1998 Granada (Spain)The survey of the LREC conferences over the period 1998-2013 has been presented during the 2014 conference  in Reykjavik as a closing session. It appears that the number of papers and signatures is increasing over time. The average number of authors per paper is higher as well. The percentage of new authors is between 68% and 78%. The distribution between male (65%) and female (35%) authors is stable over time. The most frequent technical term is "annotation", then comes "part-of-speech".

The LRE Map
The LRE Map was introduced at LREC 2010 and is now a regular feature of the LREC submission process for both the conference papers and the workshop papers. At the submission stage, the authors are asked to provide some basic information about all the resources (in a broad sense, i.e. including tools, standards and evaluation packages), either used or created, described in their papers. All these descriptors are then gathered in a global matrix called the LRE Map. This feature has been extended to several other conferences.

References
External links
Conference website
European Language Resources Association web site
The ISLRN or International Standard Language Resource Number is Persistent Unique Identifier for Language Resources.

Context
On November 18, 2013, 12 major organisations (see list below) from the fields Language Resources and Technologies, Computational Linguistics, and Digital Humanities held a cooperation meeting in Paris (France) and agreed to announce the establishment of the International Standard Language Resource Number (ISLRN), to be assigned to each Language Resource.
Among the 12 organisations, 4 institutions constitute the ISLRN Steering Committee (ST)

ADHO
ACL
Asian Federation of Natural Language Processing ST
COCOSDA, International Committee for the Coordination & Standardisation of Speech Databases and Assessment Techniques
ICCL (COLING)
European Data Forum
ELRA ST
IAMT, International Association for Machine Translation
ISCA
LDC ST
[www.cocosda.org Oriental COCOSDA] ST
RMA, Language Resource Management Agency

Size and Content
The Joint Research Centre(JRC), the [European Commission]'s in-house science   service, was the first organisation to adopt the ISLRN initiative and requested. 
2500 resources and tools have already been allocated an ISLRN. These resources include written data (Annotated corpus, Annotated text, List of misspelled word, Terminological database, Treebank, Wordnet, etc.) and speech corpora (Synthesised Speech, Transcripts and Audiovisual Recordings, Conversational Speech, Folk Sayings, etc.)

Objectives
Providing Language Resources with unique names and identifiers using a standardized nomenclature ensures the identification of each Language Resources and streamlines the citation with proper references in activities within Human Language Technology as well as in documents and scientific publications. Such unique identifier also enhances the reproducibility, an essential feature of scientific work.

References
External links
ISLRN Portal
Jive, also known as the Jive Filter, is a novelty computer program that converts plain English to a comic dialect known as "jive", a parody of African American Vernacular English.  Some versions of the filter were adapted to parody other forms of English speech, such as valspeak, cockney, geordie, Pig Latin, and even the Swedish Chef.  The last form is sometimes known as the "Encheferator" or "Encheferizer".  This family of programs became quite popular in the late 1980s.
The program is very simple and has been duplicated or translated many times on many different programming platforms and many different forms, for instance as a CGI application to run on a website to translate text typed by visitors into a comic dialect.
The program in its classic form is a simple filter that performs text substitution on its input stream to produce an output form.  For instance "black" when preceded by a space is always translated to "brother" and "come" when surrounded by spaces is always translated to "mosey on down".
The original author of the jive filter (and its sister, the valspeak filter) is unknown.  Its earliest known appearance was when it was submitted to the USENET group net.sources in September, 1986 by a contributor from JPL called Adams Douglas, who was not the author of the program.  He resubmitted it to mod.sources.games in April 1987.  This version is still downloadable from archives of that group and still compiles and runs on Unix and Linux. The program was discussed in net.sources in March, 1986 and was apparently quite well known.

Borking
At one stage of the Scientology internet wars of the mid-1990s, some opponents of the church's fight to pursue unauthorized publishing of some of its scriptures using copyright and trade secret law adopted the ruse of publishing copies that had first been garbled, "borked", or "borkified", by passing them through the Jive filter or the "encheferizer".  The holder of the copyright on the church's scriptures, Religious Technology Center, sued in some cases, for instance in Scientology versus Zenon Panoussis (Stockholm, 1998). Samples of the scriptures translated in this way were submitted in evidence.  The defendants claimed that this was parody; the plaintiffs, violation of copyright.
Just This Once is a 1993 romance novel written in the style of Jacqueline Susann by a Macintosh IIcx computer named "Hal" in collaboration with its programmer, Scott French. French reportedly spent $40,000 and 8 years developing an artificial intelligence program to analyze Susann's works and attempt to create a novel that Susann might have written. A legal dispute between the estate of Jacqueline Susann and the publisher resulted in a settlement to split the profits, and the book was referenced in several legal journal articles about copyright laws. The book had two small print runs totaling 35,000 copies, receiving mixed reviews.

Creation
The novel's creation spanned the fields of artificial intelligence, expert systems, and natural language processing.
Scott French first scanned and analyzed portions of two books by Jacqueline Susann, Valley of the Dolls and Once Is Not Enough, to determine constituents of Susann's writing style, which French stated was the most difficult task. This analysis extracted several hundred components including frequency and type of sexual acts and sentence structure. "Once you're there, the writer's style emerges, part of her actual personality comes out, and the computer can be programmed to make a story." French also created several thousand rules to govern tone, plotting, scenes, and characters.
The text generated by Hal, the computer, was intended to mimic what Susann might have written, although the output required significant editing. French credits Hal's work with "almost 100% of the plot, 100% of the theme and style." French estimates that he wrote 10% of the prose, the computer Hal wrote about 25% of the prose, and the remaining two-thirds was more of a collaboration between the two. A typical scenario to write a scene would involve Hal asking questions that French would answer (for example, Hal might ask about the "cattiness factor" involved in a meeting between two key female characters, and French would reply with a range of 1 to 10), and the computer would then generate a few sentences to which French would make minor edits. The process would repeat for the next few sentences until the scene was written.

Legal issues
Jacqueline Susann's publisher was skeptical of the legality of Just This Once. Susann's estate reportedly threatened to sue Scott French but the parties settled out of court; the settlement involved splitting profits between the parties but the terms of the settlement were not disclosed.The publication of Just This Once raised questions in the legal profession concerning how copyright law applies to computer-generated works derived from an analysis of other copyrighted works, and whether the generation of such works infringes on copyright. The publications on this topic suggested that the copyright laws of the time were ill-equipped to deal with computer-generated creative works.

Reception
The book's publisher Steven Shragis of Carol Group said of the novel, "I'm not going to say this is a great literary work, but it's every bit as good as anything out in this field, and better than an awful lot."The novel received some positive early reviews. In USA Today, novelist Thomas Gifford compared Just This Once to another novel in the same genre, American Star by Jackie Collins. Gifford concluded: "If you do like this stuff, you'd be much, much better off with the one written by the computer." The Dead Jackie Susann Quarterly declared that Susann "would be proud. Lots of money, sleaze, disease, death, oral sex, tragedy and the good girl gone bad."Other reviews were mixed. Publishers Weekly wrote, "If the books of Jacqueline Susann and Harold Robbins seem formulaic, this debut novel of sin and success in Las Vegas outdoes them all. And that, in a way, is the point.... All novelty rests in the conceit of computer authorship, not in the story itself." Library Journal stated "French invested eight years and $50,000 in a scheme to use artificial intelligence to fulfill his authentic, if dubious, desire to generate a trashy novel a la Jacqueline Susann. Shallow, beautiful-people characters are flatly conceived and randomly accessed in a formulaic plot ... a sexy, boring morality tale. Of possible interest to computer buffs for its use of Expert Systems and the virtual promise of more worthy possibilities; others should read Susann." Kirkus Reviews wrote: "The deal here is that author French is not the author, he's just the midwife, having allegedly programmed his computer to write about our times just the way Susann would... almost perfectly capturing glamorous Jackie's turgid but E-Z reading prose style and ultrareliable mix of sex, glitz, dope 'n' despair.... One wonders, though, if French's tale spinning PC will do as well on the talkshows as Jackie did. The computer weenies have been trying to tell us for years, garbage in-garbage out."

See also
Procedural generation
The Policeman's Beard is Half Constructed


== References ==
Keyword extraction is tasked with the automatic identification of terms that best describe the subject of a document.Key phrases, key terms, key segments or just keywords are the terminology which is used for defining the terms that represent the most relevant information contained in the document. Although the terminology is different, function is the same: characterization of the topic discussed in a document. The task of keyword extraction is an important problem in Text Mining, Information Retrieval and Natural Language Processing.

Keyword assignment vs. extraction
Keyword assignment methods can be roughly divided into:

keyword assignment (keywords are chosen from controlled vocabulary or taxonomy) and
keyword extraction (keywords are chosen from words that are explicitly mentioned in original text).Methods for automatic keyword extraction can be supervised, semi-supervised, or unsupervised. Unsupervised methods can be further divided into simple statistics, linguistics or graph-based, or ensemble methods that combine some or most of these methods.

References
Further reading
Journal article: N. Firoozeh, A. Nazarenko, F. Alizon, B. Daille. 2019. Keyword extraction: Issues and methods. Natural Language Engineering, 1-33, doi:10.1017/S1351324919000457, Cambridge University Press
In mathematical logic and computer science, the Kleene star (or Kleene operator or Kleene closure) is a unary operation, either on sets of strings or on sets of symbols or characters. In mathematics
it is more commonly known as the free monoid construction. The application of the Kleene star to a set V is written as V*. It is widely used for regular expressions, which is the context in which it was introduced by Stephen Kleene to characterize certain automata, where it means "zero or more".

If V is a set of strings, then V* is defined as the smallest superset of V that contains the empty string ε and is closed under the string concatenation operation.
If V is a set of symbols or characters, then V* is the set of all strings over symbols in V, including the empty string ε.The set V* can also be described as the set of finite-length strings that can be generated by concatenating arbitrary elements of V, allowing the use of the same element multiple times. If V is either the empty set ∅ or the singleton set {ε}, then V* = {ε}; if V is any other finite set or countably infinite set, then V* is a countably infinite set.The operators are used in rewrite rules for generative grammars.

Definition and notation
Given a set V
define

V0 = {ε} (the language consisting only of the empty string),
V1 = Vand define recursively the set

Vi+1 = { wv : w ∈ Vi and v ∈ V } for each i > 0.If V is a formal language, then Vi, the i-th power of the set V, is a shorthand for the concatenation of set V with itself i times. That is, Vi can be understood to be the set of all strings that can be represented as the concatenation of i strings in V.
The definition of Kleene star on V is

  
    
      
        
          V
          
            ∗
          
        
        =
        
          ⋃
          
            i
            ≥
            0
          
        
        
          V
          
            i
          
        
        =
        
          V
          
            0
          
        
        ∪
        
          V
          
            1
          
        
        ∪
        
          V
          
            2
          
        
        ∪
        
          V
          
            3
          
        
        ∪
        
          V
          
            4
          
        
        ∪
        ⋯
        .
      
    
    {\displaystyle V^{*}=\bigcup _{i\geq 0}V^{i}=V^{0}\cup V^{1}\cup V^{2}\cup V^{3}\cup V^{4}\cup \cdots .}
  Notice that the Kleene star operator is an idempotent unary operator: (V*)* = V* for any set V of strings or characters.

Kleene plus
In some formal language studies, (e.g. AFL theory) a variation on the Kleene star operation called the Kleene plus is used. The Kleene plus omits the V0 term in the above union. In other words, the Kleene plus on V is

  
    
      
        
          V
          
            +
          
        
        =
        
          ⋃
          
            i
            ≥
            1
          
        
        
          V
          
            i
          
        
        =
        
          V
          
            1
          
        
        ∪
        
          V
          
            2
          
        
        ∪
        
          V
          
            3
          
        
        ∪
        ⋯
        .
      
    
    {\displaystyle V^{+}=\bigcup _{i\geq 1}V^{i}=V^{1}\cup V^{2}\cup V^{3}\cup \cdots .}
  For every set L, the Kleene plus of L (denoted L+) equals the concatenation of L with L*; this holds because every element of L+ must either be composed from one element of L and finitely many non-empty terms in L or is just an element of L (where L itself is retrieved by taking L concatenated with ε). 
Conversely, L* = {ε} ∪ L+.

Examples
Example of Kleene star applied to set of strings:

{"ab","c"}* = { ε, "ab", "c", "abab", "abc", "cab", "cc", "ababab", "ababc", "abcab", "abcc", "cabab", "cabc", "ccab", "ccc", ...}.Example of Kleene plus applied to set of characters:

{"a", "b", "c"}+ = { "a", "b", "c", "aa", "ab", "ac", "ba", "bb", "bc", "ca", "cb", "cc", "aaa", "aab", ...}.Kleene star applied to the same character set:

{"a", "b", "c"}* = { ε, "a", "b", "c", "aa", "ab", "ac", "ba", "bb", "bc", "ca", "cb", "cc", "aaa", "aab", ...}.Example of Kleene star applied to the empty set:

∅* = {ε}.Example of Kleene plus applied to the empty set:

∅+ = ∅ ∅* = { } = ∅,where concatenation is an associative and noncommutative product, sharing these properties with the Cartesian product of sets.
Example of Kleene plus and Kleene star applied to the singleton set containing the empty string:

If V = {ε}, then also Vi = {ε} for each i, hence V* = V+ = {ε}.

Generalization
Strings form a monoid with concatenation as the binary operation and ε the identity element.  The Kleene star is defined for any monoid, not just strings.
More precisely, let (M, ⋅) be a monoid, and S ⊆ M.  Then S* is the smallest submonoid of M containing S; that is, S* contains the neutral element of M, the set S, and is such that if x,y ∈ S*, then x⋅y ∈ S*.
Furthermore, the Kleene star is generalized by including the *-operation (and the union) in the algebraic structure itself by the notion of complete star semiring.

See also
Wildcard character
Glob (programming)

References
Further reading
Hopcroft, John E.; Ullman, Jeffrey D. (1979). Introduction to Automata Theory, Languages, and Computation (1st ed.). Addison-Wesley.
Language Computer Corporation (LCC) is a natural language processing research company based in Richardson, Texas.  The company develops a variety of natural language processing products, including software for question answering, information extraction, and automatic summarization.Since its founding in 1995, the low-profile company has landed significant United States Government contracts, with $8,353,476 in contracts in 2006-2008.While the company has focused primarily on the government software market, LCC has also used its technology to spin off three start-up companies.  The first spin off, known as Lymba Corporation, markets the PowerAnswer question answering product originally developed at LCC.  In 2010, LCC's CEO, Andrew Hickl, co-founded two start-ups which made use of the company's technology.  These included Swingly, an automatic question answering start-up, and Extractiv, an information extraction service that was founded in partnership with Houston, Texas-based 80legs.

References
External links
Language Computer Corporation website
Lymba Corporation website
Language engineering involves the creation of natural language processing systems, whose cost and outputs are measurable and predictable. It is a distinct field contrasted to natural language processing and computational linguistics. A recent trend of language engineering is the use of Semantic Web technologies for the creation, archiving, processing, and retrieval of machine processable language data.


== References ==
In natural language processing, language identification or language guessing is the problem of determining which natural language given content is in. Computational approaches to this problem view it as a special case of text categorization, solved with various statistical methods.

Overview
There are several statistical approaches to language identification using different techniques to classify the data.  One technique is to compare the compressibility of the text to the compressibility of texts in a set of known languages.  This approach is known as mutual information based distance measure.  The same technique can also be used to empirically construct family trees of languages which closely correspond to the trees constructed using historical methods.  Mutual information based distance measure is essentially equivalent to more conventional model-based methods and is not generally considered to be either novel or better than simpler techniques.  
Another technique, as described by Cavnar and Trenkle (1994) and Dunning (1994) is to create a language n-gram model from a "training text" for each of the languages.  These models can be based on characters (Cavnar and Trenkle) or encoded bytes (Dunning); in the latter, language identification and character encoding detection are integrated.  Then, for any piece of text needing to be identified, a similar model is made, and that model is compared to each stored language model.  The most likely language is the one with the model that is most similar to the model from the text needing to be identified.  This approach can be problematic when the input text is in a language for which there is no model.  In that case, the method may return another, "most similar" language as its result. Also problematic for any approach are pieces of input text that are composed of several languages, as is common on the Web. 
For a more recent method, see Řehůřek and Kolkus (2009). This method can detect multiple languages in an unstructured piece of text and works robustly on short texts of only a few words: something that the n-gram approaches struggle with.
An older statistical method by Grefenstette was based on the prevalence of certain function words (e.g., "the" in English).

Identifying similar languages
One of the great bottlenecks of language identification systems is to distinguish between closely related languages. Similar languages like Serbian and Croatian or Indonesian and Malay present significant lexical and structural overlap, making it challenging for systems to discriminate between them. 
Recently, the DSL shared task has been organized providing a dataset (Tan et al., 2014) containing 13 different languages (and language varieties) in six language groups: Group A (Bosnian, Croatian, Serbian), Group B (Indonesian, Malaysian), Group C (Czech, Slovak), Group D (Brazilian Portuguese, European Portuguese), Group E (Peninsular Spain, Argentine Spanish), Group F (American English, British English). The best system reached performance of over 95% results (Goutte et al., 2014). Results of the DSL shared task are described in Zampieri et al. 2014.

Software
Apache OpenNLP includes char n-gram based statistical detector and comes with a model that can distinguish 103 languages
Apache Tika contains a language detector for 18 languages

References
Benedetto, D., E. Caglioti and V. Loreto. Language trees and zipping. Physical Review Letters, 88:4 (2002), Complexity theory.
Cavnar, William B. and John M. Trenkle. "N-Gram-Based Text Categorization".  Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval (1994) [1].
Cilibrasi, Rudi and Paul M.B. Vitanyi. "Clustering by compression". IEEE Transactions on Information Theory 51(4), April 2005, 1523-1545.
Dunning, T. (1994) "Statistical Identification of Language". Technical Report MCCS 94-273, New Mexico State University, 1994.
Goodman, Joshua. (2002) Extended comment on "Language Trees and Zipping". Microsoft Research, Feb 21 2002. (This is a criticism of the data compression in favor of the Naive Bayes method.)
Goutte, C.; Leger, S.; Carpuat, M. (2014) The NRC System for Discriminating Similar Languages. Proceedings of the Coling 2014 workshop "Applying NLP Tools to Similar Languages, Varieties and Dialects"
Grefenstette, Gregory. (1995) Comparing two language identification schemes. Proceedings of the 3rd International Conference on the Statistical Analysis of Textual Data (JADT 1995).
Poutsma, Arjen. (2001) Applying Monte Carlo techniques to language identification.  SmartHaven, Amsterdam.  Presented at CLIN 2001.
Tan, L.; Zampieri, M.; Ljubešić, N.; Tiedemann, J. (2014) Merging Comparable Data Sources for the Discrimination of Similar Languages: The DSL Corpus Collection. Proceedings of the 7th Workshop on Building and Using Comparable Corpora (BUCC). Reykjavik, Iceland. p. 6-10
The Economist. (2002) "The elements of style: Analysing compressed data leads to impressive results in linguistics"
Radim Řehůřek and Milan Kolkus. (2009) "Language Identification on the Web: Extending the Dictionary Method" Computational Linguistics and Intelligent Text Processing.
Zampieri, M.; Tan, L.; Ljubešić, N.; Tiedemann, J. (2014) A Report on the DSL Shared Task 2014. Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial). Dublin, Ireland. p. 58-67.

See also
Native Language Identification
Algorithmic information theory
Artificial grammar learning
Family name affixes
Kolmogorov complexity
Language Analysis for the Determination of Origin
Machine translation
Translation


== References ==
Language technology, often called human language technology (HLT), studies methods of how computer programs or electronic devices can analyze, produce, modify or respond to human texts and speech. It consists of natural language processing (NLP) and computational linguistics (CL) on the one hand, and speech technology on the other. It also includes many application oriented aspects of these.  Working with language technology often requires broad knowledge not only about linguistics but also about computer science.
The Globalization and Localization Association (GALA), maintains a directory of language technology and software for translators and localizers.For many of the world's lesser known languages, the foundation of language technology is providing communities with fonts and keyboard setups so their languages can be written on computers or mobile devices.

References
External links
Johns Hopkins University Human Language Technology Center of Excellence
Carnegie Mellon University Language Technologies Institute
Institute for Applied Linguistics (IULA)at Universitat Pompeu Fabra. Barcelona, Spain
German Research Centre for Artificial Intelligence (DFKI) Language Technology Lab
CLT: Centre for Language Technology in Gothenburg, Sweden
Globalization and Localization Association (GALA)
ScriptSource, a reference to the writing systems of the world and the remaining needs for supporting them in the computing realm.
LanguageWare is a natural language processing (NLP) technology developed by IBM, which allows applications to process natural language text. It comprises a set of Java libraries which provide a range of NLP functions: language identification, text segmentation/tokenization, normalization, entity and relationship extraction, and semantic analysis and disambiguation. The analysis engine uses Finite State Machine approach at multiple levels, which aids its performance characteristics, while maintaining a reasonably small footprint.
The behaviour of the system is driven by a set of configurable lexico-semantic resources which describe the characteristics and domain of the processed language. A default set of resources comes as part of LanguageWare and these describe the native language characteristics, such as morphology, and the basic vocabulary for the language. Supplemental resources have been created which capture additional vocabularies, terminologies, rules and grammars, which may be generic to the language or specific to one or more domains.
A set of Eclipse-based customization tooling, LanguageWare Resource Workbench, is available on IBM's alphaWorks site, and allows domain knowledge to be compiled into these resources and thereby incorporated into the analysis process.
LanguageWare can be deployed as a set of UIMA-compliant annotators, Eclipse plug-ins or Web Services.

See also
Data Discovery and Query Builder
Finite state machine
Formal language
IBM Omnifind
Linguistics
Semantic Web
Semantics
Service-oriented architecture
Web services
UIMA

References
External links
IBM LanguageWare Resource Workbench on alphaWorks
IBM LanguageWare Miner for Multidimensional Socio-Semantic Networks on alphaWorks
JumpStart Infocenter for IBM LanguageWare on IBM.com
UIMA Homepage at the Apache Software Foundation
UIMA Framework on SourceForge
IBM OmniFind Yahoo! Edition (FREE enterprise search engine)
Semantic Information Systems and Language Engineering Group
SemanticDesktop.org

Related Papers
Branimir K. Boguraev Annotation-Based Finite State Processing in a Large-Scale NLP Architecture, IBM Research Report, 2004
Alexander Troussov, Mikhail Sogrin, "IBM LanguageWare Ontological Network Miner"
Sheila Kinsella, Andreas Harth, Alexander Troussov, Mikhail Sogrin, John Judge, Conor Hayes, John G. Breslin, "Navigating and Annotating Semantically-Enabled Networks of People and Associated Objects"
Mikhail Kotelnikov, Alexander Polonsky, Malte Kiesel, Max Völkel, Heiko Haller, Mikhail Sogrin, Pär Lannerö, Brian Davis, "Interactive Semantic Wikis"
Sebastian Trüg, Jos van den Oever, Stéphane Laurière, "The Social Semantic Desktop: Nepomuk"
Séamus Lawless, Vincent Wade, "Dynamic Content Discovery, Harvesting and Delivery"
R. Mack, S. Mukherjea, A. Soffer, N. Uramoto, E. Brown, A. Coden, J. Cooper, A. Inokuchi, B. Iyer, Y. Mass, H. Matsuzawa, and L. V. Subramaniam, "Text analytics for life science using the Unstructured Information Management Architecture"
Alex Nevidomsky, "UIMA Framework and Knowledge Discovery at IBM", 4th Text Mining Symposium, Fraunhofer SCAI, 2006
Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.  LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis).  A matrix containing word counts per document (rows represent unique words and columns represent each document) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns.  Documents are then compared by taking the cosine of the angle between the two vectors (or the dot product between the normalizations of the two vectors) formed by any two columns.  Values close to 1 represent very similar documents while values close to 0 represent very dissimilar documents.An information retrieval technique using latent semantic structure was patented in 1988 (US Patent 4,839,853, now expired) by Scott Deerwester, Susan Dumais, George Furnas, Richard Harshman, Thomas Landauer, Karen Lochbaum and Lynn Streeter. In the context of its application to information retrieval, it is sometimes called latent semantic indexing (LSI).

Overview
Occurrence matrix
LSA can use a term-document matrix which describes the occurrences of terms in documents; it is a sparse matrix whose rows correspond to terms and whose columns correspond to documents. A typical example of the weighting of the elements of the matrix is tf-idf (term frequency–inverse document frequency): the weight of an element of the matrix is proportional to the number of times the terms appear in each document, where rare terms are upweighted to reflect their relative importance.
This matrix is also common to standard semantic models, though it is not necessarily explicitly expressed as a matrix, since the mathematical properties of matrices are not always used.

Rank lowering
After the construction of the occurrence matrix, LSA finds a low-rank approximation to the term-document matrix. There could be various reasons for these approximations:

The original term-document matrix is presumed too large for the computing resources; in this case, the approximated low rank  matrix is interpreted as an approximation (a "least and necessary evil").
The original term-document matrix is presumed noisy: for example, anecdotal instances of terms are to be eliminated. From this point of view, the approximated matrix is interpreted as a de-noisified matrix (a better matrix than the original).
The original term-document matrix is presumed overly sparse relative to the "true" term-document matrix.  That is, the original matrix lists only the words actually in each document, whereas we might be interested in all words related to each document—generally a much larger set due to synonymy.The consequence of the rank lowering is that some dimensions are combined and depend on more than one term:

{(car), (truck), (flower)} -->  {(1.3452 * car + 0.2828 * truck), (flower)}This mitigates the problem of identifying synonymy, as the rank lowering is expected to merge the dimensions associated with terms that have similar meanings. It also partially mitigates the problem with polysemy, since components of polysemous words that point in the "right" direction are added to the components of words that share a similar meaning. Conversely, components that point in other directions tend to either simply cancel out, or, at worst, to be smaller than components in the directions corresponding to the intended sense.

Derivation
Let 
  
    
      
        X
      
    
    {\displaystyle X}
   be a matrix where element 
  
    
      
        (
        i
        ,
        j
        )
      
    
    {\displaystyle (i,j)}
   describes the occurrence of term 
  
    
      
        i
      
    
    {\displaystyle i}
   in document 
  
    
      
        j
      
    
    {\displaystyle j}
   (this can be, for example, the frequency). 
  
    
      
        X
      
    
    {\displaystyle X}
   will look like this:

  
    
      
        
          
            
              
              
                
                  
                    
                      d
                    
                  
                  
                    j
                  
                
              
            
            
              
              
                ↓
              
            
            
              
                
                  
                    
                      t
                    
                  
                  
                    i
                  
                  
                    T
                  
                
                →
              
              
                
                  
                    [
                    
                      
                        
                          
                            x
                            
                              1
                              ,
                              1
                            
                          
                        
                        
                          …
                        
                        
                          
                            x
                            
                              1
                              ,
                              j
                            
                          
                        
                        
                          …
                        
                        
                          
                            x
                            
                              1
                              ,
                              n
                            
                          
                        
                      
                      
                        
                          ⋮
                        
                        
                          ⋱
                        
                        
                          ⋮
                        
                        
                          ⋱
                        
                        
                          ⋮
                        
                      
                      
                        
                          
                            x
                            
                              i
                              ,
                              1
                            
                          
                        
                        
                          …
                        
                        
                          
                            x
                            
                              i
                              ,
                              j
                            
                          
                        
                        
                          …
                        
                        
                          
                            x
                            
                              i
                              ,
                              n
                            
                          
                        
                      
                      
                        
                          ⋮
                        
                        
                          ⋱
                        
                        
                          ⋮
                        
                        
                          ⋱
                        
                        
                          ⋮
                        
                      
                      
                        
                          
                            x
                            
                              m
                              ,
                              1
                            
                          
                        
                        
                          …
                        
                        
                          
                            x
                            
                              m
                              ,
                              j
                            
                          
                        
                        
                          …
                        
                        
                          
                            x
                            
                              m
                              ,
                              n
                            
                          
                        
                      
                    
                    ]
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{matrix}&{\textbf {d}}_{j}\\&\downarrow \\{\textbf {t}}_{i}^{T}\rightarrow &{\begin{bmatrix}x_{1,1}&\dots &x_{1,j}&\dots &x_{1,n}\\\vdots &\ddots &\vdots &\ddots &\vdots \\x_{i,1}&\dots &x_{i,j}&\dots &x_{i,n}\\\vdots &\ddots &\vdots &\ddots &\vdots \\x_{m,1}&\dots &x_{m,j}&\dots &x_{m,n}\\\end{bmatrix}}\end{matrix}}}
  Now a row in this matrix will be a vector corresponding to a term, giving its relation to each document:

  
    
      
        
          
            
              t
            
          
          
            i
          
          
            T
          
        
        =
        
          
            [
            
              
                
                  
                    x
                    
                      i
                      ,
                      1
                    
                  
                
                
                  …
                
                
                  
                    x
                    
                      i
                      ,
                      j
                    
                  
                
                
                  …
                
                
                  
                    x
                    
                      i
                      ,
                      n
                    
                  
                
              
            
            ]
          
        
      
    
    {\displaystyle {\textbf {t}}_{i}^{T}={\begin{bmatrix}x_{i,1}&\dots &x_{i,j}&\dots &x_{i,n}\end{bmatrix}}}
  Likewise, a column in this matrix will be a vector corresponding to a document, giving its relation to each term:

  
    
      
        
          
            
              d
            
          
          
            j
          
        
        =
        
          
            [
            
              
                
                  
                    x
                    
                      1
                      ,
                      j
                    
                  
                
              
              
                
                  ⋮
                
              
              
                
                  
                    x
                    
                      i
                      ,
                      j
                    
                  
                
              
              
                
                  ⋮
                
              
              
                
                  
                    x
                    
                      m
                      ,
                      j
                    
                  
                
              
            
            ]
          
        
      
    
    {\displaystyle {\textbf {d}}_{j}={\begin{bmatrix}x_{1,j}\\\vdots \\x_{i,j}\\\vdots \\x_{m,j}\\\end{bmatrix}}}
  Now the dot product 
  
    
      
        
          
            
              t
            
          
          
            i
          
          
            T
          
        
        
          
            
              t
            
          
          
            p
          
        
      
    
    {\displaystyle {\textbf {t}}_{i}^{T}{\textbf {t}}_{p}}
   between two term vectors gives the correlation between the terms over the set of documents. The matrix product 
  
    
      
        X
        
          X
          
            T
          
        
      
    
    {\displaystyle XX^{T}}
   contains all these dot products. Element 
  
    
      
        (
        i
        ,
        p
        )
      
    
    {\displaystyle (i,p)}
   (which is equal to element 
  
    
      
        (
        p
        ,
        i
        )
      
    
    {\displaystyle (p,i)}
  ) contains the dot product 
  
    
      
        
          
            
              t
            
          
          
            i
          
          
            T
          
        
        
          
            
              t
            
          
          
            p
          
        
      
    
    {\displaystyle {\textbf {t}}_{i}^{T}{\textbf {t}}_{p}}
   (
  
    
      
        =
        
          
            
              t
            
          
          
            p
          
          
            T
          
        
        
          
            
              t
            
          
          
            i
          
        
      
    
    {\displaystyle ={\textbf {t}}_{p}^{T}{\textbf {t}}_{i}}
  ). Likewise, the matrix 
  
    
      
        
          X
          
            T
          
        
        X
      
    
    {\displaystyle X^{T}X}
   contains the dot products between all the document vectors, giving their correlation over the terms: 
  
    
      
        
          
            
              d
            
          
          
            j
          
          
            T
          
        
        
          
            
              d
            
          
          
            q
          
        
        =
        
          
            
              d
            
          
          
            q
          
          
            T
          
        
        
          
            
              d
            
          
          
            j
          
        
      
    
    {\displaystyle {\textbf {d}}_{j}^{T}{\textbf {d}}_{q}={\textbf {d}}_{q}^{T}{\textbf {d}}_{j}}
  .
Now, from the theory of linear algebra, there exists a decomposition of 
  
    
      
        X
      
    
    {\displaystyle X}
   such that 
  
    
      
        U
      
    
    {\displaystyle U}
   and 
  
    
      
        V
      
    
    {\displaystyle V}
   are orthogonal matrices and 
  
    
      
        Σ
      
    
    {\displaystyle \Sigma }
   is a diagonal matrix. This is called a singular value decomposition (SVD):

  
    
      
        
          
            
              
                X
                =
                U
                Σ
                
                  V
                  
                    T
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{matrix}X=U\Sigma V^{T}\end{matrix}}}
  The matrix products giving us the term and document correlations then become

  
    
      
        
          
            
              
                X
                
                  X
                  
                    T
                  
                
              
              
                =
              
              
                (
                U
                Σ
                
                  V
                  
                    T
                  
                
                )
                (
                U
                Σ
                
                  V
                  
                    T
                  
                
                
                  )
                  
                    T
                  
                
                =
                (
                U
                Σ
                
                  V
                  
                    T
                  
                
                )
                (
                
                  V
                  
                    
                      T
                      
                        T
                      
                    
                  
                
                
                  Σ
                  
                    T
                  
                
                
                  U
                  
                    T
                  
                
                )
                =
                U
                Σ
                
                  V
                  
                    T
                  
                
                V
                
                  Σ
                  
                    T
                  
                
                
                  U
                  
                    T
                  
                
                =
                U
                Σ
                
                  Σ
                  
                    T
                  
                
                
                  U
                  
                    T
                  
                
              
            
            
              
                
                  X
                  
                    T
                  
                
                X
              
              
                =
              
              
                (
                U
                Σ
                
                  V
                  
                    T
                  
                
                
                  )
                  
                    T
                  
                
                (
                U
                Σ
                
                  V
                  
                    T
                  
                
                )
                =
                (
                
                  V
                  
                    
                      T
                      
                        T
                      
                    
                  
                
                
                  Σ
                  
                    T
                  
                
                
                  U
                  
                    T
                  
                
                )
                (
                U
                Σ
                
                  V
                  
                    T
                  
                
                )
                =
                V
                
                  Σ
                  
                    T
                  
                
                
                  U
                  
                    T
                  
                
                U
                Σ
                
                  V
                  
                    T
                  
                
                =
                V
                
                  Σ
                  
                    T
                  
                
                Σ
                
                  V
                  
                    T
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{matrix}XX^{T}&=&(U\Sigma V^{T})(U\Sigma V^{T})^{T}=(U\Sigma V^{T})(V^{T^{T}}\Sigma ^{T}U^{T})=U\Sigma V^{T}V\Sigma ^{T}U^{T}=U\Sigma \Sigma ^{T}U^{T}\\X^{T}X&=&(U\Sigma V^{T})^{T}(U\Sigma V^{T})=(V^{T^{T}}\Sigma ^{T}U^{T})(U\Sigma V^{T})=V\Sigma ^{T}U^{T}U\Sigma V^{T}=V\Sigma ^{T}\Sigma V^{T}\end{matrix}}}
  Since 
  
    
      
        Σ
        
          Σ
          
            T
          
        
      
    
    {\displaystyle \Sigma \Sigma ^{T}}
   and 
  
    
      
        
          Σ
          
            T
          
        
        Σ
      
    
    {\displaystyle \Sigma ^{T}\Sigma }
   are diagonal we see that 
  
    
      
        U
      
    
    {\displaystyle U}
   must contain the eigenvectors of 
  
    
      
        X
        
          X
          
            T
          
        
      
    
    {\displaystyle XX^{T}}
  , while 
  
    
      
        V
      
    
    {\displaystyle V}
   must be the eigenvectors of 
  
    
      
        
          X
          
            T
          
        
        X
      
    
    {\displaystyle X^{T}X}
  . Both products have the same non-zero eigenvalues, given by the non-zero entries of 
  
    
      
        Σ
        
          Σ
          
            T
          
        
      
    
    {\displaystyle \Sigma \Sigma ^{T}}
  , or equally, by the non-zero entries of 
  
    
      
        
          Σ
          
            T
          
        
        Σ
      
    
    {\displaystyle \Sigma ^{T}\Sigma }
  . Now the decomposition looks like this:

  
    
      
        
          
            
              
              
                X
              
              
              
              
                U
              
              
              
                Σ
              
              
              
                
                  V
                  
                    T
                  
                
              
            
            
              
              
                (
                
                  
                    
                      d
                    
                  
                  
                    j
                  
                
                )
              
              
              
              
              
              
              
              
                (
                
                  
                    
                      
                        
                          d
                        
                        ^
                      
                    
                  
                  
                    j
                  
                
                )
              
            
            
              
              
                ↓
              
              
              
              
              
              
              
              
                ↓
              
            
            
              
                (
                
                  
                    
                      t
                    
                  
                  
                    i
                  
                  
                    T
                  
                
                )
                →
              
              
                
                  
                    [
                    
                      
                        
                          
                            x
                            
                              1
                              ,
                              1
                            
                          
                        
                        
                          …
                        
                        
                          
                            x
                            
                              1
                              ,
                              j
                            
                          
                        
                        
                          …
                        
                        
                          
                            x
                            
                              1
                              ,
                              n
                            
                          
                        
                      
                      
                        
                          ⋮
                        
                        
                          ⋱
                        
                        
                          ⋮
                        
                        
                          ⋱
                        
                        
                          ⋮
                        
                      
                      
                        
                          
                            x
                            
                              i
                              ,
                              1
                            
                          
                        
                        
                          …
                        
                        
                          
                            x
                            
                              i
                              ,
                              j
                            
                          
                        
                        
                          …
                        
                        
                          
                            x
                            
                              i
                              ,
                              n
                            
                          
                        
                      
                      
                        
                          ⋮
                        
                        
                          ⋱
                        
                        
                          ⋮
                        
                        
                          ⋱
                        
                        
                          ⋮
                        
                      
                      
                        
                          
                            x
                            
                              m
                              ,
                              1
                            
                          
                        
                        
                          …
                        
                        
                          
                            x
                            
                              m
                              ,
                              j
                            
                          
                        
                        
                          …
                        
                        
                          
                            x
                            
                              m
                              ,
                              n
                            
                          
                        
                      
                    
                    ]
                  
                
              
              
                =
              
              
                (
                
                  
                    
                      
                        
                          t
                        
                        ^
                      
                    
                  
                  
                    i
                  
                  
                    T
                  
                
                )
                →
              
              
                
                  
                    [
                    
                      
                        
                          
                            
                              [
                              
                                
                                  
                                    
                                  
                                
                                
                                  
                                    
                                  
                                
                                
                                  
                                    
                                      
                                        
                                          u
                                        
                                      
                                      
                                        1
                                      
                                    
                                  
                                
                                
                                  
                                    
                                  
                                
                                
                                  
                                    
                                  
                                
                              
                              ]
                            
                          
                          …
                          
                            
                              [
                              
                                
                                  
                                    
                                  
                                
                                
                                  
                                    
                                  
                                
                                
                                  
                                    
                                      
                                        
                                          u
                                        
                                      
                                      
                                        l
                                      
                                    
                                  
                                
                                
                                  
                                    
                                  
                                
                                
                                  
                                    
                                  
                                
                              
                              ]
                            
                          
                        
                      
                    
                    ]
                  
                
              
              
                ⋅
              
              
                
                  
                    [
                    
                      
                        
                          
                            σ
                            
                              1
                            
                          
                        
                        
                          …
                        
                        
                          0
                        
                      
                      
                        
                          ⋮
                        
                        
                          ⋱
                        
                        
                          ⋮
                        
                      
                      
                        
                          0
                        
                        
                          …
                        
                        
                          
                            σ
                            
                              l
                            
                          
                        
                      
                    
                    ]
                  
                
              
              
                ⋅
              
              
                
                  
                    [
                    
                      
                        
                          
                            
                              [
                              
                                
                                  
                                  
                                  
                                    
                                      
                                        
                                          v
                                        
                                      
                                      
                                        1
                                      
                                    
                                  
                                  
                                  
                                
                              
                              ]
                            
                          
                        
                      
                      
                        
                          ⋮
                        
                      
                      
                        
                          
                            
                              [
                              
                                
                                  
                                  
                                  
                                    
                                      
                                        
                                          v
                                        
                                      
                                      
                                        l
                                      
                                    
                                  
                                  
                                  
                                
                              
                              ]
                            
                          
                        
                      
                    
                    ]
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{matrix}&X&&&U&&\Sigma &&V^{T}\\&({\textbf {d}}_{j})&&&&&&&({\hat {\textbf {d}}}_{j})\\&\downarrow &&&&&&&\downarrow \\({\textbf {t}}_{i}^{T})\rightarrow &{\begin{bmatrix}x_{1,1}&\dots &x_{1,j}&\dots &x_{1,n}\\\vdots &\ddots &\vdots &\ddots &\vdots \\x_{i,1}&\dots &x_{i,j}&\dots &x_{i,n}\\\vdots &\ddots &\vdots &\ddots &\vdots \\x_{m,1}&\dots &x_{m,j}&\dots &x_{m,n}\\\end{bmatrix}}&=&({\hat {\textbf {t}}}_{i}^{T})\rightarrow &{\begin{bmatrix}{\begin{bmatrix}\,\\\,\\{\textbf {u}}_{1}\\\,\\\,\end{bmatrix}}\dots {\begin{bmatrix}\,\\\,\\{\textbf {u}}_{l}\\\,\\\,\end{bmatrix}}\end{bmatrix}}&\cdot &{\begin{bmatrix}\sigma _{1}&\dots &0\\\vdots &\ddots &\vdots \\0&\dots &\sigma _{l}\\\end{bmatrix}}&\cdot &{\begin{bmatrix}{\begin{bmatrix}&&{\textbf {v}}_{1}&&\end{bmatrix}}\\\vdots \\{\begin{bmatrix}&&{\textbf {v}}_{l}&&\end{bmatrix}}\end{bmatrix}}\end{matrix}}}
  The values 
  
    
      
        
          σ
          
            1
          
        
        ,
        …
        ,
        
          σ
          
            l
          
        
      
    
    {\displaystyle \sigma _{1},\dots ,\sigma _{l}}
   are called the singular values, and 
  
    
      
        
          u
          
            1
          
        
        ,
        …
        ,
        
          u
          
            l
          
        
      
    
    {\displaystyle u_{1},\dots ,u_{l}}
   and 
  
    
      
        
          v
          
            1
          
        
        ,
        …
        ,
        
          v
          
            l
          
        
      
    
    {\displaystyle v_{1},\dots ,v_{l}}
   the left and right singular vectors.
Notice the only part of 
  
    
      
        U
      
    
    {\displaystyle U}
   that contributes to 
  
    
      
        
          
            
              t
            
          
          
            i
          
        
      
    
    {\displaystyle {\textbf {t}}_{i}}
   is the 
  
    
      
        i
        
          
            'th
          
        
      
    
    {\displaystyle i{\textrm {'th}}}
   row.
Let this row vector be called 
  
    
      
        
          
            
              
                
                  t
                
                ^
              
            
          
          
            i
          
          
            T
          
        
      
    
    {\displaystyle {\hat {\textrm {t}}}_{i}^{T}}
  .
Likewise, the only part of 
  
    
      
        
          V
          
            T
          
        
      
    
    {\displaystyle V^{T}}
   that contributes to 
  
    
      
        
          
            
              d
            
          
          
            j
          
        
      
    
    {\displaystyle {\textbf {d}}_{j}}
   is the 
  
    
      
        j
        
          
            'th
          
        
      
    
    {\displaystyle j{\textrm {'th}}}
   column, 
  
    
      
        
          
            
              
                
                  d
                
                ^
              
            
          
          
            j
          
        
      
    
    {\displaystyle {\hat {\textrm {d}}}_{j}}
  .
These are not the eigenvectors, but depend on all the eigenvectors.
It turns out that when you select the 
  
    
      
        k
      
    
    {\displaystyle k}
   largest singular values, and their corresponding singular vectors from 
  
    
      
        U
      
    
    {\displaystyle U}
   and 
  
    
      
        V
      
    
    {\displaystyle V}
  , you get the rank 
  
    
      
        k
      
    
    {\displaystyle k}
   approximation to 
  
    
      
        X
      
    
    {\displaystyle X}
   with the smallest error (Frobenius norm). This approximation has a minimal error.  But more importantly we can now treat the term and document vectors as a "semantic space". The row "term" vector 
  
    
      
        
          
            
              
                
                  t
                
                ^
              
            
          
          
            i
          
          
            T
          
        
      
    
    {\displaystyle {\hat {\textbf {t}}}_{i}^{T}}
   then has 
  
    
      
        k
      
    
    {\displaystyle k}
   entries mapping it to a lower-dimensional space dimensions. These new dimensions do not relate to any comprehensible concepts. They are a lower-dimensional approximation of the higher-dimensional space. Likewise, the "document" vector 
  
    
      
        
          
            
              
                
                  d
                
                ^
              
            
          
          
            j
          
        
      
    
    {\displaystyle {\hat {\textbf {d}}}_{j}}
   is an approximation in this lower-dimensional space. We write this approximation as

  
    
      
        
          X
          
            k
          
        
        =
        
          U
          
            k
          
        
        
          Σ
          
            k
          
        
        
          V
          
            k
          
          
            T
          
        
      
    
    {\displaystyle X_{k}=U_{k}\Sigma _{k}V_{k}^{T}}
  You can now do the following:

See how related documents 
  
    
      
        j
      
    
    {\displaystyle j}
   and 
  
    
      
        q
      
    
    {\displaystyle q}
   are in the low-dimensional space by comparing the vectors 
  
    
      
        
          Σ
          
            k
          
        
        
          
            
              
                
                  d
                
                ^
              
            
          
          
            j
          
        
      
    
    {\displaystyle \Sigma _{k}{\hat {\textbf {d}}}_{j}}
   and 
  
    
      
        
          Σ
          
            k
          
        
        
          
            
              
                
                  d
                
                ^
              
            
          
          
            q
          
        
      
    
    {\displaystyle \Sigma _{k}{\hat {\textbf {d}}}_{q}}
   (typically by cosine similarity).
Comparing terms 
  
    
      
        i
      
    
    {\displaystyle i}
   and 
  
    
      
        p
      
    
    {\displaystyle p}
   by comparing the vectors 
  
    
      
        
          Σ
          
            k
          
        
        
          
            
              
                
                  t
                
                ^
              
            
          
          
            i
          
        
      
    
    {\displaystyle \Sigma _{k}{\hat {\textbf {t}}}_{i}}
   and 
  
    
      
        
          Σ
          
            k
          
        
        
          
            
              
                
                  t
                
                ^
              
            
          
          
            p
          
        
      
    
    {\displaystyle \Sigma _{k}{\hat {\textbf {t}}}_{p}}
  . Note that 
  
    
      
        
          
            
              
                t
              
              ^
            
          
        
      
    
    {\displaystyle {\hat {\textbf {t}}}}
   is now a column vector.
Documents and term vector representations can be clustered using traditional clustering algorithms like k-means using similarity measures like cosine.
Given a query, view this as a mini document, and compare it to your documents in the low-dimensional space.To do the latter, you must first translate your query into the low-dimensional space. It is then intuitive that you must use the same transformation that you use on your documents:

  
    
      
        
          
            
              
                
                  d
                
                ^
              
            
          
          
            j
          
        
        =
        
          Σ
          
            k
          
          
            −
            1
          
        
        
          U
          
            k
          
          
            T
          
        
        
          
            
              d
            
          
          
            j
          
        
      
    
    {\displaystyle {\hat {\textbf {d}}}_{j}=\Sigma _{k}^{-1}U_{k}^{T}{\textbf {d}}_{j}}
  Note here that the inverse of the diagonal matrix 
  
    
      
        
          Σ
          
            k
          
        
      
    
    {\displaystyle \Sigma _{k}}
   may be found by inverting each nonzero value within the matrix.
This means that if you have a query vector 
  
    
      
        q
      
    
    {\displaystyle q}
  , you must do the translation 
  
    
      
        
          
            
              
                q
              
              ^
            
          
        
        =
        
          Σ
          
            k
          
          
            −
            1
          
        
        
          U
          
            k
          
          
            T
          
        
        
          
            q
          
        
      
    
    {\displaystyle {\hat {\textbf {q}}}=\Sigma _{k}^{-1}U_{k}^{T}{\textbf {q}}}
   before you compare it with the document vectors in the low-dimensional space. You can do the same for pseudo term vectors:

  
    
      
        
          
            
              t
            
          
          
            i
          
          
            T
          
        
        =
        
          
            
              
                
                  t
                
                ^
              
            
          
          
            i
          
          
            T
          
        
        
          Σ
          
            k
          
        
        
          V
          
            k
          
          
            T
          
        
      
    
    {\displaystyle {\textbf {t}}_{i}^{T}={\hat {\textbf {t}}}_{i}^{T}\Sigma _{k}V_{k}^{T}}
  
  
    
      
        
          
            
              
                
                  t
                
                ^
              
            
          
          
            i
          
          
            T
          
        
        =
        
          
            
              t
            
          
          
            i
          
          
            T
          
        
        
          V
          
            k
          
          
            −
            T
          
        
        
          Σ
          
            k
          
          
            −
            1
          
        
        =
        
          
            
              t
            
          
          
            i
          
          
            T
          
        
        
          V
          
            k
          
        
        
          Σ
          
            k
          
          
            −
            1
          
        
      
    
    {\displaystyle {\hat {\textbf {t}}}_{i}^{T}={\textbf {t}}_{i}^{T}V_{k}^{-T}\Sigma _{k}^{-1}={\textbf {t}}_{i}^{T}V_{k}\Sigma _{k}^{-1}}
  
  
    
      
        
          
            
              
                
                  t
                
                ^
              
            
          
          
            i
          
        
        =
        
          Σ
          
            k
          
          
            −
            1
          
        
        
          V
          
            k
          
          
            T
          
        
        
          
            
              t
            
          
          
            i
          
        
      
    
    {\displaystyle {\hat {\textbf {t}}}_{i}=\Sigma _{k}^{-1}V_{k}^{T}{\textbf {t}}_{i}}

Applications
The new low-dimensional space typically can be used to:

Compare the documents in the low-dimensional space (data clustering, document classification).
Find similar documents across languages, after analyzing a base set of translated documents (cross-language information retrieval).
Find relations between terms (synonymy and polysemy).
Given a query of terms, translate it into the low-dimensional space, and find matching documents (information retrieval).
Find the best similarity between small groups of terms, in a semantic way (i.e. in a context of a knowledge corpus), as for example in multi choice questions MCQ answering model.
Expand the feature space of machine learning / text mining systems 
Analyze word association in text corpus Synonymy and polysemy are fundamental problems in natural language processing: 

Synonymy is the phenomenon where different words describe the same idea. Thus, a query in a search engine may fail to retrieve a relevant document that does not contain the words which appeared in the query. For example, a search for "doctors" may not return a document containing the word "physicians", even though the words have the same meaning.
Polysemy is the phenomenon where the same word has multiple meanings. So a search may retrieve irrelevant documents containing the desired words in the wrong meaning. For example, a botanist and a computer scientist looking for the word "tree" probably desire different sets of documents.

Commercial applications
LSA has been used to assist in performing prior art searches for patents.

Applications in human memory
The use of Latent Semantic Analysis has been prevalent in the study of human memory, especially in areas of free recall and memory search.  There is a positive correlation between the semantic similarity of two words (as measured by LSA) and the probability that the words would be recalled one after another in free recall tasks using study lists of random common nouns. They also noted that in these situations, the inter-response time between the similar words was much quicker than between dissimilar words.  These findings are referred to as the Semantic Proximity Effect.When participants made mistakes in recalling studied items, these mistakes tended to be items that were more semantically related to the desired item and found in a previously studied list.  These prior-list intrusions, as they have come to be called, seem to compete with items on the current list for recall.Another model, termed Word Association Spaces (WAS) is also used in memory studies by collecting free association data from a series of experiments and which includes measures of word relatedness for over 72,000 distinct word pairs.

Implementation
The SVD is typically computed using large matrix methods (for example, Lanczos methods) but may also be computed incrementally and with greatly reduced resources via a neural network-like approach, which does not require the large, full-rank matrix to be held in memory.
A fast, incremental, low-memory, large-matrix SVD algorithm has recently been developed. MATLAB and Python implementations of these fast algorithms are available. Unlike Gorrell and Webb's (2005) stochastic approximation, Brand's algorithm (2003) provides an exact solution.
In recent years progress has been made to reduce the computational complexity of SVD; for instance, by using a parallel ARPACK algorithm to perform parallel eigenvalue decomposition it is possible to speed up the SVD computation cost while providing comparable prediction quality.

Limitations
Some of LSA's drawbacks include:

The resulting dimensions might be difficult to interpret. For instance, in{(car), (truck), (flower)} ↦  {(1.3452 * car + 0.2828 * truck), (flower)}
the (1.3452 * car + 0.2828 * truck) component could be interpreted as "vehicle". However, it is very likely that cases close to
{(car), (bottle), (flower)} ↦  {(1.3452 * car + 0.2828 * bottle), (flower)}
will occur. This leads to results which can be justified on the mathematical level, but have no interpretable meaning in natural language.LSA can only partially capture polysemy (i.e., multiple meanings of a word) because each occurrence of a word is treated as having the same meaning due to the word being represented as a single point in space. For example, the occurrence of "chair" in a document containing "The Chair of the Board" and in a separate document containing "the chair maker" are considered the same.  The behavior results in the vector representation being an average of all the word's different meanings in the corpus, which can make it difficult for comparison.   However, the effect is often lessened due to words having a predominant sense throughout a corpus (i.e. not all meanings are equally likely).
Limitations of bag of words model (BOW), where a text is represented as an unordered collection of words. To address some of the limitation of bag of words model (BOW), multi-gram dictionary can be used to find direct and indirect association as well as higher-order co-occurrences among terms.
The probabilistic model of LSA does not match observed data: LSA assumes that words and documents form a joint Gaussian model (ergodic hypothesis), while a Poisson distribution has been observed.  Thus, a newer alternative is probabilistic latent semantic analysis, based on a multinomial model, which is reported to give better results than standard LSA.

Alternative methods
Semantic hashing
In semantic hashing  documents are mapped to memory addresses by means of a neural network in such a way that semantically similar documents are located at nearby addresses. Deep neural network essentially builds a graphical model of the word-count vectors obtained from a large set of documents. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. This way of extending the efficiency of hash-coding to approximate matching is much faster than locality sensitive hashing, which is the fastest current method.

Latent semantic indexing
Latent semantic indexing (LSI) is an indexing and retrieval method that uses a mathematical technique called singular value decomposition (SVD) to identify patterns in the relationships between the terms and concepts contained in an unstructured collection of text.  LSI is based on the principle that words that are used in the same contexts tend to have similar meanings.  A key feature of LSI is its ability to extract the conceptual content of a body of text by establishing associations between those terms that occur in similar contexts.LSI is also an application of correspondence analysis, a multivariate statistical technique developed by Jean-Paul Benzécri in the early 1970s, to a contingency table built from word counts in documents.
Called "latent semantic indexing" because of its ability to correlate semantically related terms that are latent in a collection of text, it was first applied to text at Bellcore in the late 1980s.   The method, also called latent semantic analysis (LSA), uncovers the underlying latent semantic structure in the usage of words in a body of text and how it can be used to extract the meaning of the text in response to user queries, commonly referred to as concept searches.  Queries, or concept searches, against a set of documents that have undergone LSI will return results that are conceptually similar in meaning to the search criteria even if the results don’t share a specific word or words with the search criteria.

Benefits of LSI
LSI helps overcome synonymy by increasing recall, one of the most problematic constraints of Boolean keyword queries and vector space models.  Synonymy is often the cause of mismatches in the vocabulary used by the authors of documents and the users of information retrieval systems.  As a result, Boolean or keyword queries often return irrelevant results and miss information that is relevant.
LSI is also used to perform automated document categorization.  In fact, several experiments have demonstrated that there are a number of correlations between the way LSI and humans process and categorize text.    Document categorization is the assignment of documents to one or more predefined categories based on their similarity to the conceptual content of the categories.   LSI uses example documents to establish the conceptual basis for each category.  During categorization processing, the concepts contained in the documents being categorized are compared to the concepts contained in the example items, and a category (or categories) is assigned to the documents based on the similarities between the concepts they contain and the concepts that are contained in the example documents.
Dynamic clustering based on the conceptual content of documents can also be accomplished using LSI.  Clustering is a way to group documents based on their conceptual similarity to each other without using example documents to establish the conceptual basis for each cluster.  This is very useful when dealing with an unknown collection of unstructured text.
Because it uses a strictly mathematical approach, LSI is inherently independent of language.  This enables LSI to elicit the semantic content of information written in any language without requiring the use of auxiliary structures, such as dictionaries and thesauri.  LSI can also perform cross-linguistic concept searching and example-based categorization.  For example, queries can be made in one language, such as English, and conceptually similar results will be returned even if they are composed of an entirely different language or of multiple languages.LSI is not restricted to working only with words.  It can also process arbitrary character strings.  Any object that can be expressed as text can be represented in an LSI vector space. For example, tests with MEDLINE abstracts have shown that LSI is able to effectively classify genes based on conceptual modeling of the biological information contained in the titles and abstracts of the MEDLINE citations.LSI automatically adapts to new and changing terminology, and has been shown to be very tolerant of noise (i.e., misspelled words, typographical errors, unreadable characters, etc.).   This is especially important for applications using text derived from Optical Character Recognition (OCR) and speech-to-text conversion.  LSI also deals effectively with sparse, ambiguous, and contradictory data.
Text does not need to be in sentence form for LSI to be effective.  It can work with lists, free-form notes, email, Web-based content, etc.  As long as a collection of text contains multiple terms, LSI can be used to identify patterns in the relationships between the important terms and concepts contained in the text.
LSI has proven to be a useful solution to a number of conceptual matching problems.  The technique has been shown to capture key relationship information, including causal, goal-oriented, and taxonomic information.

LSI timeline
Mid-1960s – Factor analysis technique first described and tested (H. Borko and M. Bernick)
1988 – Seminal paper on LSI technique published 
1989 – Original patent granted 
1992 – First use of LSI to assign articles to reviewers
1994 – Patent granted for the cross-lingual application of LSI (Landauer et al.)
1995 – First use of LSI for grading essays (Foltz, et al., Landauer et al.)
1999 – First implementation of LSI technology for intelligence community for analyzing unstructured text (SAIC).
2002 – LSI-based product offering to intelligence-based government agencies (SAIC)

Mathematics of LSI
LSI uses common linear algebra techniques to learn the conceptual correlations in a collection of text.  In general, the process involves constructing a weighted term-document matrix, performing a Singular Value Decomposition on the matrix, and using the matrix to identify the concepts contained in the text.

Term-document matrix
LSI begins by constructing a term-document matrix, 
  
    
      
        A
      
    
    {\displaystyle A}
  , to identify the occurrences of the 
  
    
      
        m
      
    
    {\displaystyle m}
   unique terms within a collection of 
  
    
      
        n
      
    
    {\displaystyle n}
   documents.  In a term-document matrix, each term is represented by a row, and each document is represented by a column, with each matrix cell, 
  
    
      
        
          a
          
            i
            j
          
        
      
    
    {\displaystyle a_{ij}}
  , initially representing the number of times the associated term appears in the indicated document, 
  
    
      
        
          t
          
            f
            
              i
              j
            
          
        
      
    
    {\displaystyle \mathrm {tf_{ij}} }
  .  This matrix is usually very large and very sparse.
Once a term-document matrix is constructed, local and global weighting functions can be applied to it to condition the data.  The weighting functions transform each cell, 
  
    
      
        
          a
          
            i
            j
          
        
      
    
    {\displaystyle a_{ij}}
   of 
  
    
      
        A
      
    
    {\displaystyle A}
  , to be the product of a local term weight, 
  
    
      
        
          l
          
            i
            j
          
        
      
    
    {\displaystyle l_{ij}}
  , which describes the relative frequency of a term in a document, and a global weight, 
  
    
      
        
          g
          
            i
          
        
      
    
    {\displaystyle g_{i}}
  , which describes the relative frequency of the term within the entire collection of documents.
Some common local weighting functions are defined in the following table.

Some common global weighting functions are defined in the following table.

Empirical studies with LSI report that the Log and Entropy weighting functions work well, in practice, with many data sets.  In other words, each entry 
  
    
      
        
          a
          
            i
            j
          
        
      
    
    {\displaystyle a_{ij}}
   of 
  
    
      
        A
      
    
    {\displaystyle A}
   is computed as:

  
    
      
        
          g
          
            i
          
        
        =
        1
        +
        
          ∑
          
            j
          
        
        
          
            
              
                p
                
                  i
                  j
                
              
              log
              ⁡
              
                p
                
                  i
                  j
                
              
            
            
              log
              ⁡
              n
            
          
        
      
    
    {\displaystyle g_{i}=1+\sum _{j}{\frac {p_{ij}\log p_{ij}}{\log n}}}
  
  
    
      
        
          a
          
            i
            j
          
        
        =
        
          g
          
            i
          
        
         
        log
        ⁡
        (
        
          
            t
            f
          
          
            i
            j
          
        
        +
        1
        )
      
    
    {\displaystyle a_{ij}=g_{i}\ \log(\mathrm {tf} _{ij}+1)}

Rank-reduced singular value decomposition
A rank-reduced, singular value decomposition is performed on the matrix to determine patterns in the relationships between the terms and concepts contained in the text.  The SVD forms the foundation for LSI.   It computes the term and document vector spaces by approximating the single term-frequency matrix, 
  
    
      
        A
      
    
    {\displaystyle A}
  , into three other matrices— an m by r  term-concept vector matrix 
  
    
      
        T
      
    
    {\displaystyle T}
  , an r by r singular values matrix 
  
    
      
        S
      
    
    {\displaystyle S}
  , and a n by r concept-document vector matrix, 
  
    
      
        D
      
    
    {\displaystyle D}
  , which satisfy the following relations:

  
    
      
        A
        ≈
        T
        S
        
          D
          
            T
          
        
      
    
    {\displaystyle A\approx TSD^{T}}
  

  
    
      
        
          T
          
            T
          
        
        T
        =
        
          I
          
            r
          
        
        
        
          D
          
            T
          
        
        D
        =
        
          I
          
            r
          
        
      
    
    {\displaystyle T^{T}T=I_{r}\quad D^{T}D=I_{r}}
  

  
    
      
        
          S
          
            1
            ,
            1
          
        
        ≥
        
          S
          
            2
            ,
            2
          
        
        ≥
        …
        ≥
        
          S
          
            r
            ,
            r
          
        
        >
        0
        
        
          S
          
            i
            ,
            j
          
        
        =
        0
        
        
          where
        
        
        i
        ≠
        j
      
    
    {\displaystyle S_{1,1}\geq S_{2,2}\geq \ldots \geq S_{r,r}>0\quad S_{i,j}=0\;{\text{where}}\;i\neq j}
  
In the formula, A is the supplied m by n weighted matrix of term frequencies in a collection of text where m is the number of unique terms, and n is the number of documents.  T is a computed m by r matrix of term vectors where r is the rank of A—a measure of its unique dimensions ≤ min(m,n).  S is a computed r by r diagonal matrix of decreasing singular values, and D is a computed n by r matrix of document vectors.
The SVD is then truncated to reduce the rank by keeping only the largest k « r diagonal entries in the singular value matrix S,
where k is typically on the order 100 to 300 dimensions.
This effectively reduces the term and document vector matrix sizes to m by k and n by k respectively.  The SVD operation, along with this reduction, has the effect of preserving the most important semantic information in the text while reducing noise and other undesirable artifacts of the original space of A.  This reduced set of matrices is often denoted with a modified formula such as:

A ≈ Ak = Tk Sk DkTEfficient LSI algorithms only compute the first k singular values and term and document vectors as opposed to computing a full SVD and then truncating it.
Note that this rank reduction is essentially the same as doing Principal Component Analysis (PCA) on the matrix A, except that PCA subtracts off the means.  PCA loses the sparseness of the A matrix, which can make it infeasible for large lexicons.

Querying and augmenting LSI vector spaces
The computed Tk and Dk matrices define the term and document vector spaces, which with the computed singular values, Sk, embody the conceptual information derived from the document collection.  The similarity of terms or documents within these spaces is a factor of how close they are to each other in these spaces, typically computed as a function of the angle between the corresponding vectors.
The same steps are used to locate the vectors representing the text of queries and new documents within the document space of an existing LSI index.  By a simple transformation of the A = T S DT equation into the equivalent D = AT T S−1 equation, a new vector, d, for a query or for a new document can be created by computing a new column in A and then multiplying the new column by T S−1.  The new column in A is computed using the originally derived global term weights and applying the same local weighting function to the terms in the query or in the new document.
A drawback to computing vectors in this way, when adding new searchable documents, is that terms that were not known during the SVD phase for the original index are ignored.  These terms will have no impact on the global weights and learned correlations derived from the original collection of text.  However, the computed vectors for the new text are still very relevant for similarity comparisons with all other document vectors.
The process of augmenting the document vector spaces for an LSI index with new documents in this manner is called folding in.  Although the folding-in process does not account for the new semantic content of the new text, adding a substantial number of documents in this way will still provide good results for queries as long as the terms and concepts they contain are well represented within the LSI index to which they are being added.  When the terms and concepts of a new set of documents need to be included in an LSI index, either the term-document matrix, and the SVD, must be recomputed or an incremental update method (such as the one described in ) is needed.

Additional uses of LSI
It is generally acknowledged that the ability to work with text on a semantic basis is essential to modern information retrieval systems.  As a result, the use of LSI has significantly expanded in recent years as earlier challenges in scalability and performance have been overcome.
LSI is being used in a variety of information retrieval and text processing applications, although its primary application has been for concept searching and automated document categorization.   Below are some other ways in which LSI is being used:

Information discovery  (eDiscovery, Government/Intelligence community, Publishing)
Automated document classification (eDiscovery, Government/Intelligence community, Publishing)
Text summarization  (eDiscovery, Publishing)
Relationship discovery  (Government, Intelligence community, Social Networking)
Automatic generation of link charts of individuals and organizations  (Government, Intelligence community)
Matching technical papers and grants with reviewers  (Government)
Online customer support  (Customer Management)
Determining document authorship  (Education)
Automatic keyword annotation of images
Understanding software source code  (Software Engineering)
Filtering spam  (System Administration)
Information visualization
Essay scoring  (Education)
Literature-based discovery
Stock returns prediction
Dream Content Analysis (Psychology) LSI is increasingly being used for electronic document discovery (eDiscovery) to help enterprises prepare for litigation.  In eDiscovery, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis is essential.  Concept-based searching using LSI has been applied to the eDiscovery process by leading providers as early as 2003.

Challenges to LSI
Early challenges to LSI focused on scalability and performance.  LSI requires relatively high computational performance and memory in comparison to other information retrieval techniques.  However, with the implementation of modern high-speed processors and the availability of inexpensive memory, these considerations have been largely overcome.  Real-world applications involving more than 30 million documents that were fully processed through the matrix and SVD computations are common in some LSI applications. A fully scalable (unlimited number of documents, online training) implementation of LSI is contained in the open source gensim software package.Another challenge to LSI has been the alleged difficulty in determining the optimal number of dimensions to use for performing the SVD.  As a general rule, fewer dimensions allow for broader comparisons of the concepts contained in a collection of text, while a higher number of dimensions enable more specific (or more relevant) comparisons of concepts.  The actual number of dimensions that can be used is limited by the number of documents in the collection.  Research has demonstrated that around 300 dimensions will usually provide the best results with moderate-sized document collections (hundreds of thousands of documents) and perhaps 400 dimensions for larger document collections (millions of documents).   However, recent studies indicate that 50-1000 dimensions are suitable depending on the size and nature of the document collection. Checking the proportion of variance retained, similar to PCA or factor analysis, to determine the optimal dimensionality is not suitable for LSI. Using a synonym test or prediction of missing words are two possible methods to find the correct dimensionality.  When LSI topics are used as features in supervised learning methods, one can use prediction error measurements to find the ideal dimensionality.

See also
Coh-Metrix
Compound term processing
Distributional semantics
Explicit semantic analysis
Latent semantic mapping
Latent semantic structure indexing
Principal components analysis
Probabilistic latent semantic analysis
Spamdexing
Word vector
Topic model
Latent Dirichlet allocation

References
Further reading
Landauer, Thomas; Foltz, Peter W.; Laham, Darrell (1998). "Introduction to Latent Semantic Analysis" (PDF). Discourse Processes. 25 (2–3): 259–284. CiteSeerX 10.1.1.125.109. doi:10.1080/01638539809545028.
Deerwester, Scott; Dumais, Susan T.; Furnas, George W.; Landauer, Thomas K.; Harshman, Richard (1990). "Indexing by Latent Semantic Analysis" (PDF). Journal of the American Society for Information Science. 41 (6): 391–407. CiteSeerX 10.1.1.33.2447. doi:10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9. Archived from the original (PDF) on 2012-07-17. Original article where the model was first exposed.
Berry, Michael; Dumais, Susan T.; O'Brien, Gavin W. (1995). "Using Linear Algebra for Intelligent Information Retrieval".  (PDF). Illustration of the application of LSA to document retrieval.
"Latent Semantic Analysis". InfoVis.
Fridolin Wild (November 23, 2005). "An Open Source LSA Package for R". CRAN. Retrieved November 20, 2006.
Thomas Landauer, Susan T. Dumais. "A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge". Retrieved 2007-07-02.

External links
Articles on LSA
Latent Semantic Analysis, a scholarpedia article on LSA written by Tom Landauer, one of the creators of LSA.

Talks and demonstrations
LSA Overview, talk by Prof. Thomas Hofmann describing LSA, its applications in Information Retrieval, and its connections to probabilistic latent semantic analysis.
Complete LSA sample code in C# for Windows. The demo code includes enumeration of text files, filtering stop words, stemming, making a document-term matrix and SVD.

Implementations
Due to its cross-domain applications in Information Retrieval, Natural Language Processing (NLP), Cognitive Science and Computational Linguistics, LSA has been implemented to support many different kinds of applications.

Sense Clusters, an Information Retrieval-oriented perl implementation of LSA
S-Space Package, a Computational Linguistics and Cognitive Science-oriented Java implementation of LSA
Semantic Vectors applies Random Projection, LSA, and Reflective Random Indexing to Lucene term-document matrices
Infomap Project, an NLP-oriented C implementation of LSA (superseded by semanticvectors project)
Text to Matrix Generator, A MATLAB Toolbox for generating term-document matrices from text collections, with support for LSA
Gensim contains a Python implementation of LSA for matrices larger than RAM.
Latent semantic mapping (LSM) is a data-driven framework to model globally meaningful relationships implicit in large volumes of (often textual) data. It is a generalization of latent semantic analysis. In information retrieval, LSA enables retrieval on the basis of conceptual content, instead of merely matching words between queries and documents.
LSM was derived from earlier work on latent semantic analysis.  There are 3 main characteristics of latent semantic analysis: Discrete entities, usually in the form of words and documents, are mapped onto continuous vectors, the mapping involves a form of global correlation pattern, and dimensionality reduction is an important aspect of the analysis process. These constitute generic properties, and have been identified as potentially useful in a variety of different contexts.  This usefulness has encouraged great interest in LSM. The intended product of latent semantic mapping, is a data-driven framework for modeling relationships in large volumes of data.
Mac OS X v10.5 and later includes a framework implementing latent semantic mapping.

See also
Latent semantic analysis

Notes
References
Bellegarda, J.R. (2005). "Latent semantic mapping [information retrieval]". IEEE Signal Processing Magazine. 22 (5): 70–80. Bibcode:2005ISPM...22...70B. doi:10.1109/MSP.2005.1511825.
J. Bellegarda (2006). "Latent semantic mapping: Principles and applications". ICASSP 2006. Archived from the original on 2013-08-24. Retrieved 2013-08-24.
Legal information retrieval is the science of information retrieval applied to legal text, including legislation, case law, and scholarly works. Accurate legal information retrieval is important to provide access to the law to laymen and legal professionals. Its importance has increased because of the vast and quickly increasing amount of legal documents available through electronic means. Legal information retrieval is a part of the growing field of legal informatics.

Overview
In a legal setting, it is frequently important to retrieve all information related to a specific query. However, commonly used boolean search methods (exact matches of specified terms) on full text legal documents have been shown to have an average recall rate as low as 20 percent, meaning that only 1 in 5 relevant documents are actually retrieved. In that case, researchers believed that they had retrieved over 75% of relevant documents. This may result in failing to retrieve important or precedential cases. In some jurisdictions this may be especially problematic, as legal professionals are ethically obligated to be reasonably informed as to relevant legal documents.Legal Information Retrieval attempts to increase the effectiveness of legal searches by increasing the number of relevant documents (providing a high recall rate) and reducing the number of irrelevant documents (a high precision rate). This is a difficult task, as the legal field is prone to jargon, polysemes (words that have different meanings when used in a legal context), and constant change.
Techniques used to achieve these goals generally fall into three categories: boolean retrieval, manual classification of legal text, and natural language processing of legal text.

Problems
Application of standard information retrieval techniques to legal text can be more difficult than application in other subjects. One key problem is that the law rarely has an inherent taxonomy. Instead, the law is generally filled with open-ended terms, which may change over time. This can be especially true in common law countries, where each decided case can subtly change the meaning of a certain word or phrase.Legal information systems must also be programmed to deal with law-specific words and phrases. Though this is less problematic in the context of words which exist solely in law, legal texts also frequently use polysemes, words may have different meanings when used in a legal or common-speech manner, potentially both within the same document. The legal meanings may be dependent on the area of law in which it is applied. For example, in the context of European Union legislation, the term "worker" has four different meanings:
Any worker as defined in Article 3(a) of Directive 89/391/EEC who habitually uses display screen equipment as a significant part of his normal work.
Any person employed by an employer, including trainees and apprentices but excluding domestic servants;
Any person carrying out an occupation on board a vessel, including trainees and apprentices, but excluding port pilots and shore personnel carrying out work on board a vessel at the quayside;
Any person who, in the Member State concerned, is protected as an employee under national employment law and in accordance with national practice;It also has the common meaning: 

A person who works at a specific occupation. 
Though the terms may be similar, correct information retrieval must differentiate between the intended use and irrelevant uses in order to return the correct results.
Even if a system overcomes the language problems inherent in law, it must still determine the relevancy of each result. In the context of judicial decisions, this requires determining the precedential value of the case. Case decisions from senior or superior courts may be more relevant than those from lower courts, even where the lower court's decision contains more discussion of the relevant facts. The opposite may be true, however, if the senior court has only a minor discussion of the topic (for example, if it is a secondary consideration in the case). A information retrieval system must also be aware of the authority of the jurisdiction. A case from a binding authority is most likely of more value than one from a non-binding authority.
Additionally, the intentions of the user may determine which cases they find valuable. For instance, where a legal professional is attempting to argue a specific interpretation of law, he might find a minor court's decision which supports his position more valuable than a senior courts position which does not. He may also value similar positions from different areas of law, different jurisdictions, or dissenting opinions.Overcoming these problems can be made more difficult because of the large number of cases available. The number of legal cases available via electronic means is constantly increasing (in 2003, US appellate courts handed down approximately 500 new cases per day), meaning that an accurate legal information retrieval system must incorporate methods of both sorting past data and managing new data.

Techniques
Boolean searches
Boolean searches, where a user may specify terms such as use of specific words or judgments by a specific court, are the most common type of search available via legal information retrieval systems. They are widely implemented but overcome few of the problems discussed above.
The recall and precision rates of these searches vary depending on the implementation and searches analyzed. One study found a basic boolean search's recall rate to be roughly 20%, and its precision rate to be roughly 79%. Another study implemented a generic search (that is, not designed for legal uses) and found a recall rate of 56% and a precision rate of 72% among legal professionals. Both numbers increased when searches were run by non-legal professionals, to a 68% recall rate and 77% precision rate. This is likely explained because of the use of complex legal terms by the legal professionals.

Manual classification
In order to overcome the limits of basic boolean searches, information systems have attempted to classify case laws and statutes into more computer friendly structures. Usually, this results in the creation of an ontology to classify the texts, based on the way a legal professional might think about them. These attempt to link texts on the basis of their type, their value, and/or their topic areas. Most major legal search providers now implement some sort of classification search, such as Westlaw's “Natural Language” or LexisNexis' Headnote searches. Additionally, both of these services allow browsing of their classifications, via Westlaw's West Key Numbers or Lexis' Headnotes. Though these two search algorithms are proprietary and secret, it is known that they employ manual classification of text (though this may be computer-assisted).These systems can help overcome the majority of problems inherent in legal information retrieval systems, in that manual classification has the greatest chances of identifying landmark cases and understanding the issues that arise in the text. In one study, ontological searching resulted in a precision rate of 82% and a recall rate of 97% among legal professionals. The legal texts included, however, were carefully controlled to just a few areas of law in a specific jurisdiction.The major drawback to this approach is the requirement of using highly skilled legal professionals and large amounts of time to classify texts. As the amount of text available continues to increase, some have stated their belief that manual classification is unsustainable.

Natural language processing
In order to reduce the reliance on legal professionals and the amount of time needed, efforts have been made to create a system to automatically classify legal text and queries. Adequate translation of both would allow accurate information retrieval without the high cost of human classification. These automatic systems generally employ Natural Language Processing (NLP) techniques that are adapted to the legal domain, and also require the creation of a legal ontology. Though multiple systems have been postulated, few have reported results. One system, “SMILE,” which attempted to automatically extract classifications from case texts, resulted in an f-measure (which is a calculation of both recall rate and precision) of under 0.3 (compared to perfect f-measure of 1.0). This is probably much lower than an acceptable rate for general usage.Despite the limited results, many theorists predict that the evolution of such systems will eventually replace manual classification systems.

Citation-Based ranking
In the mid-90s the Room 5 case law retrieval project used citation mining for summaries and ranked its search results based on citation type and count.  This slightly pre-dated the Page Rank algorithm at Stanford which was also a citation-based ranking.  Ranking of results was based as much on jurisdiction as on number of references.

Notes


== References ==
The Lesk algorithm is a classical algorithm for word sense disambiguation introduced by Michael E. Lesk in 1986.

Overview
The Lesk algorithm is based on the assumption that words in a given "neighborhood" (section of text) will tend to share a common topic. A simplified version of the Lesk algorithm is to compare the dictionary definition of an ambiguous word with the terms contained in its neighborhood. Versions have been adapted to use WordNet. An implementation might look like this:

for every sense of the word being disambiguated one should count the amount of words that are in both neighborhood of that word and in the dictionary definition of that sense
the sense that is to be chosen is the sense which has the biggest number of this countA frequently used example illustrating this algorithm is for the context "pine cone".  The following dictionary definitions are used:

PINE 
1. kinds of evergreen tree with needle-shaped leaves
2. waste away through sorrow or illness

CONE 
1. solid body which narrows to a point
2. something of this shape whether solid or hollow
3. fruit of certain evergreen trees

As can be seen, the best intersection is Pine #1 ⋂ Cone #3 = 2.

Simplified Lesk algorithm
In Simplified Lesk algorithm, the correct meaning of each word in a given context is determined individually by locating the sense that overlaps the most between its dictionary definition and the given context. Rather than simultaneously determining the meanings of all words in a given context, this approach tackles each word individually, independent of the meaning of the other words occurring in the same context.
"A comparative evaluation performed by Vasilescu et al. (2004) has shown that the simplified Lesk algorithm can significantly outperform the original definition of the algorithm, both in terms of precision and efficiency. By evaluating the disambiguation algorithms on the Senseval-2 English all words data, they measure a 58% precision using the simplified Lesk algorithm compared to the only 42% under the original algorithm.
Note: Vasilescu et al. implementation considers a back-off strategy for words not covered by the algorithm, consisting of the most frequent sense defined in WordNet. This means that words for which all their possible meanings lead to zero overlap with current context or with other word definitions are by default assigned sense number one in WordNet."Simplified LESK Algorithm with smart default word sense (Vasilescu et al., 2004)
The COMPUTEOVERLAP function returns the number of words in common between two sets, ignoring function words or other words on a stop list. The original Lesk algorithm defines the context in a more complex way.

Criticisms and other Lesk-based methods
Unfortunately, Lesk’s approach is very sensitive to the exact wording of definitions, so the absence of a certain word can radically change the results. Further, the algorithm determines overlaps only among the glosses of the senses being considered. This is a significant limitation in that dictionary glosses tend to be fairly short and do not provide sufficient vocabulary to relate fine-grained sense distinctions.
A lot of work has appeared offering different modifications of this algorithm. These works use other resources for analysis (thesauruses, synonyms dictionaries or morphological and syntactic models): for instance, it may use such information as synonyms, different derivatives, or words from definitions of words from definitions.There are a lot of studies concerning Lesk and its extensions:
Kwong, 2001;
Nastase and Szpakowicz, 2001;
Wilks and Stevenson, 1998, 1999;
Mahesh et al., 1997;
Cowie et al., 1992;
Yarowsky, 1992;
Pook and Catlett, 1988;
Kilgarriff and Rosensweig, 2000;
Gelbukh and Sidorov, 2004.

Lesk variants
Original Lesk (Lesk, 1986)
Adapted/Extended Lesk (Banerjee and Pederson, 2002/2003)  : In adaptive lesk algorithm, A word vector is created corresponds to every content word in the wordnet gloss. Concatenating glosses of related concepts in WordNet can be used to augment this vector. The vector contains the co-occurrence counts of words co-occurring with w in a large corpus. Adding all the word vectors for all the content words in its gloss creates the Gloss vector g for a concept. Relatedness is determined by comparing the gloss vector using the Cosine similarity measure.

See also
Word Sense Disambiguation


== References ==
Lessac Technologies, Inc. (LTI) is an American firm which develops voice synthesis software, licenses technology and sells synthesized novels as MP3 files. The firm currently has seven patents granted and three more pending for its automated methods of converting digital text into human-sounding speech, more accurately recognizing human speech and outputting the text representing the words and phrases of said speech, along with recognizing the speaker’s emotional state. 
The LTI technology is partly based on the work of the late Arthur Lessac, a Professor of Theater at the State University of New York and the creator of Lessac Kinesensic Training, and LTI has licensed exclusive rights to exploit Arthur Lessac's copyrighted works in the fields of speech synthesis and speech recognition. Based on the view that music is speech and speech is music, Lessac's work and books focused on body and speech energies and how they go together. Arthur Lessac's textual annotation system, which was originally developed to assist actors, singers, and orators in marking up scripts to prepare for performance, is adapted in LTI's speech synthesis system as the basic representation of the speech to be synthesized (Lessemes), in contrast to many other systems which use a phonetic representation.LTI’s software has two major components: (1) a linguistic front-end that converts plain text to a sequence of prosodic and phonosensory graphic symbols (Lessemes) based on Arthur Lessac's annotation system, which specify the speech units to be synthesized; (2) a signal-processing back-end that takes the Lessemes as acoustic data and produces human-sounding synthesized speech as output, using unit selection and concatenation.  
LTI’s text-to-speech system came in second in the world-wide Blizzard Challenge 2011 and 2012. The first-place team in 2011 also employed LTI's "front-end" technology, but with its own back-end. The Blizzard Challenge, conducted by the Language Technologies Institute of Carnegie Mellon University, was devised as a way to evaluate speech synthesis techniques by having different research groups build voices from the same voice-actor recordings, and comparing the results through listening tests.  
LTI was founded in 2000 by H. Donald Wilson (chairman), a lawyer, LexisNexis entrepreneur and business associate of Arthur Lessac; and Gary A. Marple (chief inventor), after Marple suggested that Arthur Lessac's  kinesensic voice training might be applicable to computational linguistics. After Wilson’s death in 2006, his nephew John Reichenbach became the firm’s CEO.

References
External links
Company web site
Lexalytics, Inc. provides sentiment and intent analysis to an array of companies using SaaS and cloud based technology. Salience 6, the engine behind Lexalytics, was built as an on-premises, multi-lingual text analysis engine.  It is leased to other companies who use it to power filtering and reputation management programs.  In July, 2015 Lexalytics acquired Semantria to be used as a cloud option for its technology.

History
Lexalytics spun into existence in January 2003 out of a content management startup called Lightspeed. Lightspeed consolidated on America’s West Coast. Jeff Catlin, a Lightspeed General Manager, and Mike Marshall, a Lighstpeed Principle Engineer, convinced investors to give them the East Coast company so as to avoid shutdown costs. Catlin and Marshall renamed the operation Lexalytics.
Catlin took on the role of Chief Executive Officer with Marshall working as Chief Technology Officer.  Lexalytics opted to not accept venture cash.  Instead, the company initially shared sales and marketing expenses with U.K. based document management company Infonic.  The partner companies soon formed a joint venture in July 2008, which was later dissolved. Since then, Lexalytics has worked with many other companies, like Bottlenose, Salesforce, Thomson Reuters, Oracle and DataSift. Relationships with social media monitoring companies like Datasift tend to find Lexalytics’ Salience engine baked into the product itself. Lexalytics is used similarly to monitor sentiment as it relates to stock trading. In December 2014, Lexalytics announced the latest iteration to its sentiment analysis engine, Salience 6. Earlier that year Lexalytics acquired Semantria in a bid to appeal to a wider variety of business models. Created by former Lexalytics Marketing Director Oleg Rogynskyy, Semantria is a SaaS text mining service offered as an API and Excel based plugin that measures sentiment. The goal of the acquisition, which cost Lexalytics less than $10 million USD, was to expand the customer base both within the United States and abroad with multilingual support.The engine that powers Semantria, Salience, is grounded in its deep learning ability.  An example of this is its concept matrix, which allows Salience an understanding of concepts and relationship between concepts based on a detailed reading of the entire repository of Wikipedia. This matrix allows Salience to use Wikipedia for automatic categorization. Along with features like the concept matrix, Salience supports 16 international languages. The engine has earned Lexalytics a spot on EContent’s “Top 100 Companies in the Digital Content Industry” List for 2014-2015. In September 2018, Lexalytics launched document data extraction market using natural language processing (NLP).

See also
Semantria

References
External links
Official website
Document Structuring is a subtask of Natural language generation, which involves deciding the order and grouping (for example into paragraphs) of sentences in a generated text.  It is closely related to the Content determination NLG task.

Example
Assume we have four sentences which we want to include in a generated text

It will rain on Saturday
It will be sunny on Sunday
Max temperature will be 10C on Saturday
Max temperature will be 15C on SundayThere are 24 (4!) orderings of these messages, including

(1234) It will rain on Saturday. It will be sunny on Sunday.  Max temperature will be 10C on Saturday. Max temperature will be 15C on Sunday.
(2341) It will be sunny on Sunday. Max temperature will be 10C on Saturday. Max temperature will be 15C on Sunday.  It will rain on Saturday.
(4321) Max temperature will be 15C on Sunday. Max temperature will be 10C on Saturday. It will be sunny on Sunday. It will rain on Saturday.Some of these orderings are better than others.  For example, of the texts shown above, human readers prefer (1234) over (2314) and (4321).
For any ordering, there are also many ways in which sentences can be grouped into paragraphs and higher-level structures such as sections.  For example, there are 8 (2**3) ways in which the sentences in (1234) can be grouped into paragraphs, including

(12)(34)It will rain on Saturday. It will be sunny on Sunday.
Max temperature will be 10C on Saturday. Max temperature will be 15C on Sunday.(1)(23)(4)It will rain on Saturday.
It will be sunny on Sunday. Max temperature will be 10C on Saturday.
Max temperature will be 15C on Sunday.As with ordering, human readers prefer some groupings over others; for example, (12)(34) is preferred over (1)(23)(4).
The document structuring task is to choose an ordering and grouping of sentences which results in a coherent and well-organised text from the reader's perspective.

Algorithms and Models
There are three basic approaches to document structuring: schemas, corpus-based, and heuristic.
Schemas  are templates which explicitly specify sentence ordering and grouping for a document (as well as Content determination information).  Typically they are constructed by manually analysing a corpus of human-written texts in the target genre, and extracting a document template from these texts.  Schemas work well in practice for texts which are short (5 sentences or less) and/or have a standardised structure, but have problems in generating texts which are longer and do not have a fixed structure.
Corpus-based structuring techniques use statistical corpus analysis techniques to automatically build ordering and/or grouping models.  Such techniques are common in Automatic summarisation, where a computer program automatically generates a summary of a textual document.  In principle they could be applied to text generated from non-linguistic data, but this work is in its infancy; part of the challenge is that texts generated by Natural Language Generation systems are generally expected to be of fairly high quality, which is not always the case for texts generated by automatic summarisation systems.
The final approach is heuristic-based structuring.  Such algorithms perform the structuring task based on heuristic rules, which can come from theories of rhetoric,
psycholinguistic models, and/or a combination of intuition and feedback from pilot experiments with potential users.   Heuristic-based structuring is appealing intellectually, but it can be difficult to get it to work well in practice, in part because heuristics often depend on semantic information (how sentences relate to each other) which is not always available.  On the other hand, heuristic rules can focus on what is best for text readers, whereas the other approaches focus on imitating authors (and many human-authored texts are not well structured).

Narrative
Perhaps the ultimate document structuring challenge is to generate a good narrative—in other words, a text which starts by setting the scene and giving an introduction/overview; then describes a set of events in a clear fashion so readers can easily see how the individual events are related and link together; and concludes with a summary/ending.  Note that narrative in this sense applies to factual texts as well as stories.  Current NLG systems do not do a good job of generating narratives, and this is a major source of user criticism.Generating good narratives is a challenge for all aspects of NLG, but the most fundamental challenge is probably in document structuring.


== References ==
Lexical choice is the subtask of Natural language generation that involves choosing the content words (nouns, verbs, adjectives, and adverbs) in a generated text.  Function words (determiners, for example) are usually chosen during realisation.

Examples
The simplest type of lexical choice involves mapping a domain concept (perhaps represented in an ontology) to a word.  For example, the concept Finger might be mapped to the word finger.
A more complex situation is when a domain concept is expressed using different words in different situations.  For example, the domain concept Value-Change can be expressed in many ways

The temperature rose: the verb rose is used for a Value-Change in temperature which increases the value
The temperature fell: the verb fell is used for a Value-Change in temperature which decreases the value
The rain got heavier: the phrase got heavier is used for a Value-Change in precipitation amount when the precipitation is rain.Sometimes words can communicate additional contextual information, for example

The temperature plummeted: the verb plummeted is used for a Value-Change in temperature which decreases the value, when the change is rapid and largeContextual information is especially significant for vague terms such as tall.  For example, a 2m tall man is tall, but a 2m tall horse is small.

Linguistic perspective
Lexical choice modules must be informed by linguistic knowledge of how the system's input data maps onto words.  This is a question of semantics, but it is also influenced by syntactic factors (such as collocation effects) and pragmatic factors (such as context).
Hence NLG systems need linguistic models
of how meaning is mapped to words in the target domain (genre) of the NLG system.  Genre tends to be very important; for example the verb veer has a very specific meaning in weather forecasts (wind direction is changing in a clockwise direction) which it does not have in general English, and a weather-forecast generator must be aware of this genre-specific meaning.
In some cases there are major differences in how different people use the same word; for example, some people use by evening to mean 6PM and others use it to mean midnight.  Psycholinguists have shown that when people speak to each other, they agree on a common interpretation via lexical alignment; this is not something which NLG systems can yet do.
Ultimately, lexical choice must deal with the fundamental issue of how language relates to the non-linguistic world.  For example, a system which chose colour terms such as red to describe objects in a digital image would need to know which RGB pixel values could generally be described as red; how this was influenced by visual (lighting, other objects in the scene) and linguistic (other objects being discussed) context; what pragmatic connotations were associated with red (for example, when an apple is called red, it is assumed to be ripe as well as have the colour red); and so forth.

Algorithms and models
A number of algorithms and models have been developed for lexical choice in the research community, for example Edmonds developed a model for choosing between near-synonyms (words with similar core meanings but different connotations).  However such algorithms and models have not been widely used in applied NLG systems; such systems have instead often used quite simple computational models, and invested development effort in linguistic analysis instead of algorithm development.


== References ==
Language resource management - Lexical markup framework (LMF; ISO 24613:2008), is the ISO International Organization for Standardization ISO/TC37 standard for natural language processing (NLP) and machine-readable dictionary (MRD) lexicons.
The scope is standardization of principles and methods relating to language resources in the contexts of multilingual communication.

Objectives
The goals of LMF are to provide a common model for the creation and use of lexical resources, to manage the exchange of data between and among these resources, and to enable the merging of large number of individual electronic resources to form extensive global electronic resources.
Types of individual instantiations of LMF can include monolingual, bilingual or multilingual lexical resources. The same specifications are to be used for both small and large lexicons, for both simple and complex lexicons, for both written and spoken lexical representations. The descriptions range from morphology, syntax, computational semantics to computer-assisted translation. The covered languages are not restricted to European languages but cover all natural languages. The range of targeted NLP applications is not restricted. LMF is able to represent most lexicons, including WordNet, EDR and PAROLE lexicons.

History
In the past, lexicon standardization has been studied and developed by a series of projects like GENELEX, EDR, EAGLES, MULTEXT, PAROLE, SIMPLE and ISLE. Then, the ISO/TC37 National delegations decided to address standards dedicated to NLP and lexicon representation.
The work on LMF started in Summer 2003 by a new work item proposal issued by the US delegation. In Fall 2003, the French delegation issued a technical proposition for a data model dedicated to NLP lexicons. In early 2004, the ISO/TC37 committee decided to form a common ISO project with Nicoletta Calzolari (CNR-ILC Italy) as convenor and Gil Francopoulo (Tagmatica France) and Monte George (ANSI USA) as editors.
The first step in developing LMF was to design an overall framework based on the general features of existing lexicons and to develop a consistent terminology to describe the components of those lexicons. The next step was the actual design of a comprehensive model that best represented all of the lexicons in detail. A large panel of 60 experts contributed a wide range of requirements for LMF that covered many types of NLP lexicons. The editors of LMF worked closely with the panel of experts to identify the best solutions and reach a consensus on the design of LMF. Special attention was paid to the morphology in order to provide powerful mechanisms for handling problems in several languages that were known as difficult to handle. 13 versions have been written, dispatched (to the National nominated experts), commented and discussed during various ISO technical meetings. After five years of work, including numerous face-to-face meetings and e-mail exchanges, the editors arrived at a coherent UML model. In conclusion, LMF should be considered a synthesis of the state of the art in NLP lexicon field.

Current stage
The ISO number is 24613. The LMF specification has been published officially as an International Standard on 17 November 2008.

As one of the members of the ISO/TC37 family of standards
The ISO/TC37 standards are currently elaborated as high level specifications and deal with word segmentation (ISO 24614), annotations (ISO 24611 a.k.a. MAF, ISO 24612 a.k.a. LAF, ISO 24615 a.k.a. SynAF, and ISO 24617-1 a.k.a. SemAF/Time), feature structures (ISO 24610), multimedia containers (ISO 24616 a.k.a. MLIF), and lexicons (ISO 24613).
These standards are based on low level specifications dedicated to constants, namely data categories (revision of ISO 12620), language codes (ISO 639), scripts codes (ISO 15924), country codes (ISO 3166) and Unicode (ISO 10646).
The two level organization forms a coherent family of standards with the following common and simple rules:

the high level specification provides structural elements that are adorned by the standardized constants;
the low level specifications provide standardized constants as metadata.

Key standards
The linguistics constants like /feminine/ or /transitive/ are not defined within LMF but are recorded in the Data Category Registry (DCR) that is maintained as a global resource by ISO/TC37 in compliance with ISO/IEC 11179-3:2003. And these constants are used to adorn the high level structural elements.
The LMF specification complies with the modeling principles of Unified Modeling Language (UML) as defined by Object Management Group (OMG). The structure is specified by means of UML class diagrams. The examples are presented by means of UML instance (or object) diagrams.
An XML DTD is given in an annex of the LMF document.

Model structure
LMF is composed of the following components:

The core package that is the structural skeleton which describes the basic hierarchy of information in a lexical entry.
Extensions of the core package which are expressed in a framework that describes the reuse of the core components in conjunction with the additional components required for a specific lexical resource.The extensions are specifically dedicated to morphology, MRD, NLP syntax, NLP semantics, NLP multilingual notations, NLP morphological patterns, multiword expression patterns, and constraint expression patterns.

Example
In the following example, the lexical entry is associated with a lemma clergyman and two inflected forms clergyman and clergymen. The language coding is set for the whole lexical resource. The language value is set for the whole lexicon as shown in the following UML instance diagram.

The elements Lexical Resource, Global Information, Lexicon, Lexical Entry, Lemma, and Word Form define the structure of the lexicon. They are specified within the LMF document.
On the contrary, languageCoding, language, partOfSpeech, commonNoun, writtenForm, grammaticalNumber, singular, plural are data categories that are taken from the Data Category Registry. These marks adorn the structure. The values ISO 639-3, clergyman, clergymen are plain character strings. The value eng is taken from the list of languages as defined by ISO 639-3.
With some additional information like dtdVersion and feat, the same data can be expressed by the following XML fragment:

This example is rather simple, while LMF can represent much more complex linguistic descriptions the XML tagging is correspondingly complex.

Selected publications about LMF
The first publication about the LMF specification as it has been ratified by ISO (this paper became (in 2015) the 9th most cited paper within the Language Resources and Evaluation conferences from LREC papers):

Language Resources and Evaluation LREC-2006/Genoa: Gil Francopoulo, Monte George, Nicoletta Calzolari, Monica Monachini, Nuria Bel, Mandy Pet, Claudia Soria: Lexical Markup Framework (LMF) About semantic representation:

Gesellschaft für linguistische Datenverarbeitung GLDV-2007/Tübingen: Gil Francopoulo, Nuria Bel, Monte George Nicoletta Calzolari, Monica Monachini, Mandy Pet, Claudia Soria: Lexical Markup Framework ISO standard for semantic information in NLP lexicons About African languages:

Traitement Automatique des langues naturelles, Marseille, 2014: Mouhamadou Khoule, Mouhamad Ndiankho Thiam, El Hadj Mamadou Nguer: Toward the establishment of a LMF-based Wolof language lexicon (Vers la mise en place d'un lexique basé sur LMF pour la langue wolof) [in French]About Asian languages:

Lexicography, Journal of ASIALEX, Springer 2014: Lexical Markup Framework: Gil Francopoulo, Chu-Ren Huang: An ISO Standard for Electronic Lexicons and its Implications for Asian Languages DOI 10.1007/s40607-014-0006-zAbout European languages:

COLING 2010: Verena Henrich, Erhard Hinrichs: Standardizing Wordnets in the ISO Standard LMF: Wordnet-LMF for GermaNet 
EACL 2012: Judith Eckle-Kohler, Iryna Gurevych: Subcat-LMF: Fleshing out a standardized format for subcategorization frame interoperability 
EACL 2012: Iryna Gurevych, Judith Eckle-Kohler, Silvana Hartmann, Michael Matuschek, Christian M Meyer, Christian Wirth: UBY - A Large-Scale Unified Lexical-Semantic Resource Based on LMF.About Semitic languages:

Journal of Natural Language Engineering, Cambridge University Press (to appear in Spring 2015): Aida Khemakhem, Bilel Gargouri, Abdelmajid Ben Hamadou, Gil Francopoulo: ISO Standard Modeling of a large Arabic Dictionary.
Proceedings of the seventh Global Wordnet Conference 2014: Nadia B M Karmani, Hsan Soussou, Adel M Alimi: Building a standardized Wordnet in the ISO LMF for aeb language.
Proceedings of the workshop: HLT & NLP within Arabic world, LREC 2008: Noureddine Loukil, Kais Haddar, Abdelmajid Ben Hamadou: Towards a syntactic lexicon of Arabic Verbs.
Traitement Automatique des Langues Naturelles, Toulouse (in French) 2007: Khemakhem A, Gargouri B, Abdelwahed A, Francopoulo G: Modélisation des paradigmes de flexion des verbes arabes selon la norme LMF-ISO 24613.

Dedicated book
There is a book published in 2013: LMF Lexical Markup Framework which is entirely dedicated to LMF. The first chapter deals with the history of lexicon models, the second chapter is a formal presentation of the data model and the third one deals with the relation with the data categories of the ISO-DCR. The other 14 chapters deal with a lexicon or a system, either in the civil or military domain, either within scientific research labs or for industrial applications. These are Wordnet-LMF, Prolmf, DUELME, UBY-LMF, LG-LMF, RELISH, GlobalAtlas (or Global Atlas) and Wordscape.

Related scientific communications
Language Resources and Evaluation LREC-2006/Genoa: The relevance of standards for research infrastructures

See also
Computational lexicology
Lexical semantics
Morphology (linguistics) for explanations concerning paradigms and morphosyntax
Machine translation for a presentation of the different types of multilingual notations (see section Approaches)
Morphological pattern for the difference between a paradigm and a paradigm pattern
WordNet for a presentation of the most famous semantic lexicon for the English language
Universal Terminology eXchange (UTX) for a user-oriented, alternative format for machine-readable dictionaries
Universal Networking Language
UBY-LMF for an application of LMF
OntoLex-Lemon for an LMF-based model for publishing dictionaries as knowledge graphs, in RDF and/or as Linguistic Linked Open Data

References
External links
LMF web site
LIRICS web site
Lexical simplification is a sub-task of text simplification. It can be defined as any lexical substitution task that reduce text complexity.

See also
Lexical substitution

References
Advaith Siddharthan. "Syntactic Simplification and Text Cohesion". In Research on Language and Computation, Volume 4, Issue 1, Jun 2006, Pages 77–109, Springer Science, the Netherlands.
Siddhartha Jonnalagadda, Luis Tari, Joerg Hakenberg, Chitta Baral and Graciela Gonzalez. Towards Effective Sentence Simplification for Automatic Processing of Biomedical Text. In Proc. of the NAACL-HLT 2009, Boulder, USA, June. [1]

External links
Task
Description
Lexical substitution is the task of identifying a substitute for a word in the context of a clause. For instance, given the following text: "After the match, replace any remaining fluid deficit to prevent chronic dehydration throughout the tournament", a substitute of game might be given.
Lexical substitution is strictly related to word sense disambiguation (WSD), in that both aim to determine the meaning of a word. However, while WSD consists of automatically assigning the appropriate sense from a fixed sense inventory, lexical substitution does not impose any constraint on which substitute to choose as the best representative for the word in context. By not prescribing the inventory, lexical substitution overcomes the issue of the granularity of sense distinctions and provides a level playing field for automatic systems that automatically acquire word senses (a task referred to as Word Sense Induction).

Evaluation
In order to evaluate automatic systems on lexical substitution, a task was organized at the Semeval-2007 evaluation competition held in Prague in 2007. A Semeval-2010 task on cross-lingual lexical substitution has also taken place.

Skip-gram model
The skip-gram model takes words with similar meanings into a vector space (collection of objects that can be added together and multiplied by numbers) that are found close to each other in N-dimensions (list of items). A variety of neural networks (computer system modeled after a human brain) are formed together as a result of the vectors and networks that are related together. This all occurs in the dimensions of the vocabulary that has been generated in a network.
The model has been used in lexical substitution automation and prediction algorithms. One such algorithm developed by Oren Melamud, Omer Levy, and Ido Dagan uses the skip-gram model to find a vector for each word and its synonyms. Then, it calculates the cosine distance between vectors to determine which words will be the best substitutes.

Example
In a sentence like "The dog walked at a quick pace" each word has a specific vector in relation to the other. The vector for "The" would be [1,0,0,0,0,0,0] because the 1 is the word vocabulary and the 0s are the words surrounding that vocabulary, which create a vector.

See also
Lexical semantics
Semantic compression
SemEval
Word sense

Bibliography
D. McCarthy, R. Navigli. The English Lexical Substitution Task. Language Resources and Evaluation, 43(2), Springer, 2009, pp. 139–159.
D. McCarthy, R. Navigli. SemEval-2007 Task 10: English Lexical Substitution Task. Proc. of Semeval-2007 Workshop (SEMEVAL), in the 45th Annual Meeting of the Association for Computational Linguistics (ACL 2007), Prague, Czech Republic, 23–24 June 2007, pp. 48–53.
D. McCarthy. Lexical substitution as a task for WSD evaluation. In Proceedings of the ACL workshop on word sense disambiguation: Recent successes and future directions, Philadelphia, USA, 2002, pp. 109–115.
R. Navigli. Word Sense Disambiguation: A Survey, ACM Computing Surveys, 41(2), 2009, pp. 1–69.


== References ==
Lexxe is an internet search engine that applies Natural Language Processing
in its semantic search technology.  Founded in 2005 by Dr. Hong Liang Qiao,
Lexxe is based in Sydney, Australia.  Today, Lexxe's key focus is on sentiment search with the launch of a news sentiment search site at News & Moods (www.newsandmoods.com).
Lexxe has experienced several stages of change of focus in search technology:
Lexxe launched its Alpha version in 2005, featuring Natural Language question
answering (i.e. users could ask questions in English to the search engine apart from keyword searches — this feature has been suspended for redevelopment since 2010).  It used only algorithms to extract answers from web pages, with no question-answer pair databases prepared in advance.
In 2011, Lexxe launched Beta version  with a new search technology called Semantic Key.  Semantic Keys enable users to query with a conceptual keyword
(or a keyword with a special meaning, hence the term Semantic Key) in order
to find instances under the concept, e.g. price → $5.95 or €200, color →
red, yellow, white.  For example, “price: a pound of apples”, “color:
ferrari”.  With initial 500 Semantic Keys at the Beta launch, Lexxe
became the first search engine in the world to offer this unique and useful
search technology to the users.  The cost of building Semantic Keys was too heavy though.
In 2017, Lexxe launched News & Moods (www.newsandmoods.com), an open platform for news sentiment search, a first step towards sentiment search feature for the entire Internet search in Lexxe search engine.  News & Moods also comes with smartphone apps in Android and iOS.

References
External links
Lexxe main web site
News & Moods web site
Following is a list of text corpora in various languages. "Text corpora" is the plural of "text corpus". A text corpus is a large and structured set of texts (nowadays usually electronically stored and processed). Text corpora are used to do statistical analysis and hypothesis testing, checking occurrences or validating linguistic rules within a specific language territory.

English language
Google Books Ngram Corpus
American National Corpus
Bank of English
British National Corpus
Corpus of Contemporary American English (COCA) 425 million words, 1990–2011. Freely searchable online.
Brown Corpus, forming part of the "Brown Family" of corpora, together with LOB, Frown and F-LOB.
GUM corpus, the open source Georgetown University Multilayer corpus, with very many annotation layers
International Corpus of English
Oxford English Corpus
Scottish Corpus of Texts & Speech
Corpus Resource Database (CoRD), more than 80 English language corpora.
RE3D (Relationship and Entity Extraction Evaluation Dataset)

European languages
CETENFolha
The Corpus of Electronic Texts
Google Books Ngram Corpus
The Georgian Language Corpus
Thesaurus Linguae Graecae (Ancient Greek)
Eastern Armenian National Corpus (EANC) 110 million words. Freely searchable online.
Spanish text corpus by Molino de Ideas, which contains 660 million words.
CorALit: the Corpus of Academic Lithuanian Academic texts published in 1999–2009 (approx. 9 million words). Compiled at the University of Vilnius, Lithuania
Reference Corpus of Contemporary Portuguese (CRPC)
Turkish National Corpus
CoRoLa - The Reference Corpus of the Contemporary Romanian Language (Corpus reprezentativ al limbii române contemporane )

Slavic
East Slavic
Belarusian N-korpus
Russian National Corpus
General Internet Corpus of Russian
Ukrainian Language Corpus

South Slavic
Bulgarian National Corpus
Croatian Language Corpus
Croatian National Corpus
Slovenian National Corpus

West Slavic
Czech National Corpus
National Corpus of Polish

German
German Reference Corpus (DeReKo) More than 4 billion words of contemporary written German.
Free corpus of German mistakes from people with dyslexia

Middle Eastern Languages
Hamshahri Corpus (Persian)
Persian in MULTEXT-EAST corpus (Persian)
Amarna letters, (for Akkadian, Egyptian, Sumerogram's, etc.)
TEP: Tehran English-Persian Parallel Corpus
TMC: Tehran Monolingual Corpus, Standard corpus for Persian Language Modeling
Persian Today Corpus: The Most Frequent Words of Today Persian, based on a one-million-word corpus (in Persian: Vāže-hā-ye Porkārbord-e Fārsi-ye Emrūz), Hamid Hassani, Tehran, Iran Language Institute (ILI), 2005, 322 pp. ISBN 964-8699-32-1
Kurdish-corpus.uok.ac.ir (Kurdish-corpus Sorani dialect) University of Kurdistan, Department of English Language and Linguistics
Bijankhan Corpus A Contemporary Persian Corpus for NLP researches, University of Tehran, 2012
Neo-Assyrian Text Corpus Project
Quranic Arabic Corpus (Classical Arabic)
Electronic Text Corpus of Sumerian Literature
Open Richly Annotated Cuneiform Corpus
Asosoft text corpus

Devanagari
Nepali Text Corpus (90+ million running words/6.5+ million sentences)

East Asian Languages
Kotonoha Japanese language corpus
LIVAC Synchronous Corpus (Chinese)

South Asian Languages
SinMin dataset (Sinhala)

Parallel corpora of diverse languages
Europarl Corpus - proceedings of the European Parliament from 1996–2011
EUR-Lex corpus - collection of all official languages of the European Union, created from the EUR-Lex database
OPUS: Open source Parallel Corpus in many many languages
Tatoeba A parallel corpus which contains about 2288000 sentences in 122 languages.
NTU-Multilingual Corpus in 7 languages (ara, eng, ind, jpn, kor, mcn, vie) (legacy repo)
SeedLing corpus - A Seed Corpus for the Human Language Project with 1000+ languages from various sources.
GRALIS parallel texts for various Slavic languages, compiled by the institute for Slavic languages at Graz University (Branko Tošović et al.)
The ACTRES Parallel Corpus (P-ACTRES 2.0) is a bidirectional English-Spanish corpus consisting of original texts in one language and their translation into the other. P-ACTRES 2.0 contains over 6 million words considering both directions together. The JRC-Acquis Multilingual Parallel Corpus of the total body of European Union (EU) law: Acquis Communautaire with 231 language pairs.
European Parliament Proceedings Parallel Corpus 1996-2011
The Opus project aims at collecting freely available parallel corpora
Japanese-English Bilingual Corpus of Wikipedia's Kyoto Articles
COMPARA - Portuguese/English parallel corpora
TERMSEARCH - English/Russian/French parallel corpora (Major international treaties, conventions, agreements, etc.
TradooIT - English/French/Spanish - Free Online tools
Nunavut Hansard - English/Inuktitut parallel corpus
ParaSol - A parallel corpus of Slavic and other languages
Glosbe: Multilanguage parallel corpora with online search interface
InterCorp: A multilingual parallel corpus 20+ languages aligned with Czech, online search interface
myCAT - Olanto, concordancer (open source AGPL) with online search on JCR and UNO corpus
TAUS, with online search interface.
linguatools multilingual parallel corpora, online search interface.
EUR-Lex Corpus - corpus built up of the EUR-Lex database consists of European Union law and other public documents of the European Union
Language Grid - Multilingual service platform that includes parallel text services

Comparable Corpora
WaCky - The Web-As-Corpus Kool Yinitiative Web as Corpus (eng, fre, deu, ita)
Disambiguating Similar Language Corpora Collection (DSLCC) (Bosnian, Croatian, Serbian, Indonesian, Malay, Czech, Slovak, Brazilian Portuguese, European Portuguese, Peninsular Spanish, Argentine Spanish)
Wikipedia Comparable Corpora (41 million aligned Wikipedia articles for 253 language pairs)
The TenTen Corpus Family – comparable web corpora of target size 10 billion words. These corpora are available in the corpus management system Sketch Engine, currently, there exist TenTen corpora for more than 30 languages (such as English TenTen corpus, Arabic TenTen corpus, Spanish TenTen corpus, Russian Tenten corpus,). The overview of existing TenTen corpora can be found at https://www.sketchengine.co.uk/documentation/tenten-corpora/
Timestamped JSI web corpora – web corpora of news articles crawled from a list of RSS feeds. Newsfeed corpora are being prepared in the framework of the project implemented by the Jožef Stefan Institute at Slovenian scientific research institute. and published in Sketch Engine. More information about the project is on the project websites.

L2 Corpora
Cambridge Learner Corpus
Corpus of Academic Written and Spoken English (CAWSE), a collection of Chinese students’ English language samples in academic settings. Freely downloadable online.  
English as a Lingua Franca in Academic Settings (ELFA), an academic ELF corpus.
International Corpus of Learner English (ICLE), a corpus of learner written English.
Louvain International Database of Spoken English Interlanguage (LINDSEI), a corpus of learner spoken English.
Trinity Lancaster Corpus, one of the largest corpus of L2 spoken English.
Vienna-Oxford International Corpus of English (VOICE), an ELF corpus.


== References ==
LIVAC is an uncommon language corpus dynamically maintained since 1995. Different from other existing corpora, LIVAC has adopted a rigorous and regular as well as "Windows" approach in processing and filtering massive media texts from representative Chinese speech communities such as Hong Kong, Macau, Taipei, Singapore, Shanghai, Beijing, as well as Guangzhou, and Shenzhen. The contents are thus deliberately repetitive in most cases, represented by textual samples drawn from editorials, local and international news, cross-Formosan Straits news, as well as news on finance, sports and entertainment. By 2019, 2.7 billion characters of news media texts have been filtered so far, of which 680 million characters have been processed and analyzed and have yielded an expanding Pan-Chinese dictionary of 2.3 million words from the Pan-Chinese printed media. Through rigorous analysis based on computational linguistic methodology, LIVAC has at the same time accumulated a large amount of accurate and meaningful statistical data on the Chinese language and their speech communities in the Pan-Chinese region, and the results show considerable and important variations.The "Windows" approach is the most representative feature of LIVAC and has enabled Pan-Chinese media texts to be quantitatively analyzed according to various attributes such as locations, time and subject domains. Thus, various types of comparative studies and applications in information technology as well as development of often related innovative applications have been possible. Moreover, LIVAC has allowed longitudinal developments to be taken into account, facilitating Key Word in Context (KWIC) and comprehensive study of target words and their underlying concepts as well as linguistic structures over the past 20 years, based on variables such as region, duration and content. Results from the extensive and accumulative data analysis contained in LIVAC have enabled the cultivation of textual databases of proper names, place names, organization names, new words, and bi-weekly and annual rosters of media figures. Related applications have included the establishment of verb and adjective databases, the formulation of sentiment indices, and related opinion mining, to measure and compare the popularity of global media figures in the Chinese media (LIVAC Annual Pan-Chinese Celebrity Rosters, later renamed as the Pan-Chinese Media Personalities Rosters) and construction of monthly new word lexicons (LIVAC Annual Pan-Chinese New Word Rosters). On this basis, the analysis of the emergence, diffusion and transformation of new words, and the publication of dictionaries of neologisms have been made possible.A recent focus is on the relative balance between disyllabic words and growing trisyllabic words in the Chinese language.

Corpus data processing
Accessing media texts, manual input, etc.
Text unification including conversion from simplified to traditional Chinese characters, stored as Big5 and Unicode versions
Automatic word segmentation
Automatic alignment of parallel texts
Manual verification, part-of-speech tagging
Extraction of words and addition to regional sub-corpora
Combination of regional sub-corpora to update the LIVAC corpus, and master lexical database

Labeling for data curation
Categories used include general terms and proper names, such as: general names, surnames, semi titles; geographical, organizations and commercial entities, etc.; time, prepositions, locations, etc.; stack-words; loanwords; case-word; numerals, etc.
Construction of databases of proper names, place names, and specific terms, etc.
Generate rosters: "new word rosters", "celebrity or media personality rosters", "place name rosters", compound words and matched words
Other parts of speech tagging for sub-database, such as common nouns, numerals, numeral classifiers, different types of verbs, and of adjectives, pronouns, adverbs, prepositions, conjunctions, particles marking mood, onomatopoeia, interjection, etc.

Applications
Compilation of Pan-Chinese dictionaries or local dictionaries
Information technology research, such as predictive Chinese text input for mobile phones, automatic speech to text conversion, opinion mining
Comparative studies on linguistic and cultural developments in the Pan-Chinese regions
Language teaching and learning research, and speech-to-text conversion
Customized service on linguistic research and lexical search for international corporations and government agencies

See also
British National Corpus
Oxford English Corpus
Corpus of Contemporary American English (COCA)
語料庫

References
External links
Official website
Linguistic Knowledge Builder (LKB) is a free and open source grammar engineering environment for creating grammars and lexicons of natural languages. Any unification-based grammar can be implemented, but LKB is typically used for grammars with typed feature structures such as HPSG.
It is implemented in Common Lisp, and constitutes one core component of the DELPH-IN collaboration.

External links
DELPH-IN LKB wiki
Logic forms are simple, first-order logic knowledge representations of natural language sentences formed by the conjunction of concept predicates related through shared arguments. Each noun, verb, adjective, adverb, pronoun, preposition and conjunction generates a predicate. Logic forms can be decorated with word senses to disambiguate the semantics of the word. There are two types of predicates: events are marked with e, and entities are marked with x. The shared arguments connect the subjects and objects of verbs and prepositions together. Example input/output might look like this:

Input:  The Earth provides the food we eat every day.
Output: Earth:n_#1(x1) provide:v_#2(e1, x1, x2) food:n_#1(x2) we(x3) eat:v_#1(e2, x3, x2; x4) day:n_#1(x4)

Logic forms are used in some natural language processing techniques, such as question answering, as well as in inference both for database systems and QA systems.

Evaluations
SENSEVAL-3 in 2004 introduced a .

References
Vasile Rus (2002). Logic Form for WordNet Glosses. Ph.D. thesis, Southern Methodist University.
Vasile Rus and Dan Moldovan (September 2002). "High performance logic form transformation". International Journal on Artificial Intelligence Tools. 11 (3): 437–454. doi:10.1142/S0218213002000976.
Dan Moldovan and Vasile Rus (2001). "Logic Form transformation of wordNet and its Applicability to question answering". Proceedings of ACL 2001, Toulouse, France. Archived from the original on 2006-09-13.
Jerry R. Hobbs (1986). "Overview of the TACITUS project". Computational Linguistics. pp. 12(3).
Vasile Rus (2004). "A First Evaluation of Logic Form Identification Systems" (PDF). SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text. Archived from the original (PDF) on 2005-11-03.
The LRE Map (Language Resources and Evaluation) is a freely accessible large database on resources dedicated to Natural language processing. The original feature of LRE Map is that the records are collected during the submission of different major Natural language processing conferences. The records are then cleaned and gathered into a global database called "LRE Map".The LRE Map is intended to be an instrument for collecting information about language resources and to become, at the same time, a community for users, a place to share and discover resources, discuss opinions, provide feedback, discover new trends, etc. It is an instrument for discovering, searching and documenting language resources, here intended in a broad sense, as both data and tools.
The large amount of information contained in the Map can be analyzed in many different ways. For instance, the LRE Map can provide information about the most frequent type of resource, the most represented language, the applications for which resources are used or are being developed, the proportion of new resources vs. already existing ones, or the way in which resources are distributed to the community.

Context
Several institutions worldwide maintain catalogues of language resources
(ELRA, LDC, NICT Universal Catalogue, ACL Data and Code Repository, OLAC, LT World, etc.) However, it has been estimated that only 10% of existing resources are known, either through distribution catalogues or via direct publicity by providers (web sites and the like). The rest remains hidden, the only occasions where it briefly emerges being when a resource is presented in the context of a research paper or report at some conference. Even in this case, nevertheless, it might be that a resource remains in the background simply because the focus of the research is not on the resource per se.

History
The LRE Map originated under the name "LREC Map" during the preparation of LREC 2010 conference. More specifically, the idea was discussed within the FlaReNet project, and in collaboration with ELRA and the Institute of Computational Linguistics of CNR in Pisa, the Map was put in place at LREC 2010. The LREC organizers asked the authors to provide some basic information about all the resources (in a broad sense, i.e. including tools, standards and evaluation packages), either used or created, described in their papers. All these descriptors were then gathered in a global matrix called the LREC Map.
The same methodology and requirements from the authors has been then applied and extended to other conferences, namely COLING-2010, EMNLP-2010, RANLP-2011, LREC 2012, LREC 2014 and LREC 2016.
After this generalization to other conferences, the LREC Map has been renamed as the LRE Map.

Size and content
The size of the database increases over time. The data collected amount to 4776 entries.
Each resource is described according to the following attributes:

Resource type, e.g. lexicon, annotation tool, tagger/parser.
Resource production status, e.g. newly created finished, existing-updated.
Resource availability, e.g. freely available, from data center.
Resource modality, e.g. speech, written, sign language.
Resource use, e.g. named entity recognition, language identification, machine translation.
Resource language, e.g. English, 23 European Union languages, official languages of India.

Uses
The LRE map is a very important tool to chart the NLP field. Compared to other studied based on subjective scorings, the LRE map is made of real facts.
The map has a great potential for many uses, in addition to being an information gathering tool:

It is a great instrument for monitoring the evolution of the field (useful for funders), if applied in different contexts and times.
It can be seen as a huge joint effort, the beginning of an even larger cooperative action not just among few leaders but among all the researchers.
It is also an "educational" means towards the broad acknowledgment of the need of meta-research activities with the active involvement of many.
It is also instrumental in introducing the new notion of "citation of resources" that could provide an award and a means of scholarly recognition for researchers engaged in resource creation.
It is used to help the organization of the conferences of the field like LREC.

Derived matrices
The data were then cleaned and sorted by Joseph Mariani (CNRS-LIMSI IMMI) and Gil Francopoulo (CNRS-LIMSI IMMI + Tagmatica) in order to compute the various matrices of the final FLaReNet reports. One of them, the matrix for written data at LREC 2010 is as follows:

English is the most studied language. Secondly, come French and German languages and then Italian and Spanish.

Future
The LRE Map has been extended to Language Resources and Evaluation Journal and other conferences.

References
External links
LREC Map research page
Luminoso, a Cambridge, MA-based text analytics and artificial intelligence company, spun out of the MIT Media Lab and its crowd-sourced Open Mind Common Sense (OMCS) project.The company has raised $20.6 million in financing and its clients include Sony, Autodesk, Scotts Miracle-Gro, and GlaxoSmithKline.

History
Luminoso was co-founded in 2010 by Dennis Clark, Jason Alonso, Robyn Speer, and Catherine Havasi, a research scientist at MIT in artificial intelligence and computational linguistics. The company builds on the knowledge base of MIT’s Open Mind Common Sense (OMCS) project, co-founded in 1999 by Havasi, who continues to serve as its director. The OCMS knowledge base has since been combined with knowledge from other crowdsourced resources, games with a purpose, and expert-created resources to become ConceptNet. ConceptNet consists of approximately 28 million statements in 304 languages, with full support for 10 languages and moderate support for 77 languages. ConceptNet is a resource for making an AI that understands the meanings of the words people use.During the World Cup in June 2014, the company provided a widely reported real-time sentiment analysis of the U.S. vs. Germany match, analyzing 900,000 posts on Twitter, Facebook and Google+.

Applications
The company uses artificial intelligence, natural language processing, and machine learning to derive insights from unstructured data such as contact center interactions, chatbot and live chat transcripts, product reviews, open-ended survey responses, and email. Luminoso's software identifies and quantifies patterns and relationships in text-based data, including domain-specific or creative language. Rather than human-powered keyword searches of data, the software automates taxonomy creation around concepts, allowing related words and phrases to be dynamically generated and tracked.Commercial applications include analyzing, prioritizing, and routing contact center interactions; identifying consumer complaints before they begin to trend; and tracking sentiment during product launches. The software natively analyzes text in fourteen languages, as well as emoji.

Products
Luminoso's technology can be accessed via two products: Luminoso Daylight and Luminoso Compass. Luminoso Daylight enables a deep-dive analysis into batch or real-time data, whereas Luminoso Compass automates the categorization of real-time data. Both products offer a user interface as well as an API. Luminoso's products can be implemented through either a cloud-based or an on-premises solution.

Research
Luminoso continues to actively conduct research in natural language processing and word embeddings and regularly participates in evaluations such as SemEval. At SemEval 2017, Luminoso participated in Task 2, measuring the semantic similarity of word pairs within and across five languages. Its solution outperformed all competing systems in every language pair tested, with the exception of Persian.

Recognition
Luminoso has been listed as a "Cool Vendor in AI for Marketing" by Gartner, and has also been named a "Boston Artificial Intelligence startup to watch" by BostInno.In May 2017, Luminoso was recognized as having the Best Application for AI in the Enterprise by AI Business, and was also shortlisted as the Best AI Breakthrough and Best Innovation in NLP.

Competitors
Major competitors include Clarabridge and Lexalytics.

Investors
Acadia Woods led a $6.5 million round of funding, with Japan’s Digital Garage, in July 2014. The company previously raised a $1.5 million seed round.The company closed a $10M series B funding in December 2018, led by DVI Equity Partners, with participation from Liberty Global Ventures, DF Enterprises, Raptor Holdco, Acadia Woods Partners, and Accord Ventures, among others.

References
External links
Official website
The sections below give objective criteria for evaluating the usability of machine translation software output.

Stationarity or canonical form
Do repeated translations converge on a single expression in both languages?  I.e. does the translation method show stationarity or produce a canonical form? Does the translation become stationary without losing the original meaning?  This metric has been criticized as not being well correlated with BLEU (BiLingual Evaluation Understudy) scores.

Adaptive to colloquialism, argot or slang
Is the system adaptive to colloquialism, argot or slang? The French language has many rules for creating words in the speech and writing of popular culture.  Two such rules are:  (a) The reverse spelling of words such as femme to meuf.  (This is called verlan.) (b) The attachment of the suffix -ard to a noun or verb to form a proper noun.  For example, the noun faluche means "student hat".  The word faluchard formed from faluche colloquially can mean, depending on context, "a group of students", "a gathering of students" and "behavior typical of a student".  The Google translator as of 28 December 2006 doesn't derive the constructed words as for example from rule (b), as shown here:

Il y a une chorale falucharde mercredi, venez nombreux, les faluchards chantent des paillardes! ==> There is a choral society falucharde Wednesday, come many, the faluchards sing loose-living women!

French argot has three levels of usage:
familier or friendly, acceptable among friends, family and peers but not at work
grossier or swear words, acceptable among friends and peers but not at work or in family
verlan or ghetto slang, acceptable among lower classes but not among middle or upper classesThe United States National Institute of Standards and Technology conducts annual evaluations [1] of machine translation systems based on the BLEU-4 criterion [2].  A combined method called IQmt which incorporates BLEU and additional metrics NIST, GTM, ROUGE and METEOR has been implemented by Gimenez and Amigo [3].

Well-formed output
Is the output grammatical or well-formed in the target language? Using an interlingua should be helpful in this regard, because with a fixed interlingua one should be able to write a grammatical mapping to the target language from the interlingua. Consider the following Arabic language input and English language translation result from the Google translator as of 27 December 2006 [4]. This Google translator output doesn't parse using a reasonable English grammar: 	 

 	 	 
 	وعن حوادث التدافع عند شعيرة رمي الجمرات -التي كثيرا ما يسقط فيها العديد من الضحايا- أشار الأمير نايف إلى إدخال "تحسينات كثيرة في جسر الجمرات ستمنع بإذن الله حدوث أي تزاحم". 	 
	==> 	 
	And incidents at the push Carbuncles-throwing ritual, which often fall where many of the victims - Prince Nayef pointed to the introduction of "many improvements in bridge Carbuncles God would stop the occurrence of any competing."

Semantics preservation
Do repeated re-translations preserve the semantics of the original sentence? For example, consider the following English input passed multiple times into and out of French using the Google translator as of 27 December 2006: 	 

 	 	 
	Better a day earlier than a day late. ==> 	 
	Améliorer un jour plus tôt qu'un jour tard. ==> 	 
 	To improve one day earlier than a day late. ==> 	 
	Pour améliorer un jour plus tôt qu'un jour tard. ==> 	 
	To improve one day earlier than a day late. 	 

	
As noted above and in, this kind of round-trip translation is a very unreliable method of evaluation.

Trustworthiness and security
An interesting peculiarity of Google Translate as of 24 January 2008 (corrected as of 25 January 2008) is the following result when translating from English to Spanish, which shows an embedded joke in the English-Spanish dictionary which has some added poignancy given recent events:

 	 	 
	Heath Ledger is dead ==> 	 
	Tom Cruise está muerto 

	 	
This raises the issue of trustworthiness when relying on a machine translation system embedded in a Life-critical system in which the translation system has input to a Safety Critical Decision Making process.  Conjointly it raises the issue of whether in a given use the software of the machine translation system is safe from hackers.
It is not known whether this feature of Google Translate was the result of a joke/hack or perhaps an unintended consequence of the use of a method such as statistical machine translation.  Reporters from CNET Networks asked Google for an explanation on January 24, 2008; Google said only that it was an "internal issue with Google Translate".  The mistranslation was the subject of much hilarity and speculation on the Internet.If it is an unintended consequence of the use of a method such as statistical machine translation, and not a joke/hack, then this event is a demonstration of a potential source of critical unreliability in the statistical machine translation method.
In human translations, in particular on the part of interpreters, selectivity on the part of the translator in performing a translation is often commented on when one of the two parties being served by the interpreter knows both languages.
This leads to the issue of whether a particular translation could be considered verifiable.  In this case, a converging round-trip translation would be a kind of verification.

See also
Comparison of machine translation applications
Evaluation of machine translation
Round-trip translation
Translation

Notes
References
Gimenez, Jesus and Enrique Amigo.  (2005) IQmt: A framework for machine translation evaluation.
NIST.  Annual machine translation system evaluations and evaluation plan.
Papineni, Kishore, Salim Roukos, Todd Ward and Wei-Jing Zhu. (2002) BLEU: A Method for automatic evaluation of machine translation.  Proc. 40th Annual Meeting of the ACL, July, 2002, pp. 311–318.
Example-based machine translation (EBMT) is a method of machine translation often characterized by its use of a bilingual corpus with parallel texts as its main knowledge base at run-time. It is essentially a translation by analogy and can be viewed as an implementation of a case-based reasoning approach to machine learning.

Translation by analogy
At the foundation of example-based machine translation is the idea of translation by analogy. When applied to the process of human translation, the idea that translation takes place by analogy is a rejection of the idea that people translate sentences by doing deep linguistic analysis. Instead, it is founded on the belief that people translate by first decomposing a sentence into certain phrases, then by translating these phrases, and finally by properly composing these fragments into one long sentence. Phrasal translations are translated by analogy to previous translations. The principle of translation by analogy is encoded to example-based machine translation through the example translations that are used to train such a system.
Other approaches to machine translation, including statistical machine translation, also use bilingual corpora to learn the process of translation.

History
Example-based machine translation was first suggested by Makoto Nagao in 1984. He pointed out that it is especially adapted to translation between two totally different languages, such as English and Japanese. In this case, one sentence can be translated into several well-structured sentences in another language, therefore, it is no use to do the deep linguistic analysis characteristic of rule-based machine translation.

Example
Example-based machine translation systems are trained from bilingual parallel corpora containing sentence pairs like the example shown in the table above. Sentence pairs contain sentences in one language with their translations into another. The particular example shows an example of a minimal pair, meaning that the sentences vary by just one element. These sentences make it simple to learn translations of portions of a sentence. For example, an example-based machine translation system would learn three units of translation from the above example:

How much is that X ? corresponds to Ano X wa ikura desu ka.
red umbrella corresponds to akai kasa
small camera corresponds to chiisai kameraComposing these units can be used to produce novel translations in the future. For example, if we have been trained using some text containing the sentences:
President Kennedy was shot dead during the parade. and The convict escaped on July 15th. We could translate the sentence The convict was shot dead during the parade. by substituting the appropriate parts of the sentences.

Phrasal verbs
Example-based machine translation is best suited for sub-language phenomena like phrasal verbs. Phrasal verbs have highly context-dependent meanings. They are common in English, where they comprise a verb followed by an adverb and/or a preposition, which are called the particle to the verb. Phrasal verbs produce specialized context-specific meanings that may not be derived from the meaning of the constituents. There is almost always an ambiguity during word-to-word translation from source to the target language.
As an example, consider the phrasal verb "put on" and its Hindustani translation. It may be used in any of the following ways:

Ram put on the lights. (Switched on) (Hindustani translation: Jalana)
Ram put on a cap. (Wear) (Hindustani translation: Pahenna)

See also
Programming by example
Translation memory
Natural Language Processing

References
Further reading
Carl, Michael; Way, Andy (2003). Recent Advances in Example-Based Machine Translation. Netherlands: Springer. doi:10.1007/978-94-010-0181-6. ISBN 978-1-4020-1400-0.

External links
Cunei - an open source platform for data-driven machine translation that grew out of research in EBMT, but also includes recent advances from the SMT field
Hybrid machine translation is a method of machine translation that is characterized by the use of multiple machine translation approaches within a single machine translation system. The motivation for developing hybrid machine translation systems stems from the failure of any single technique to achieve a satisfactory level of accuracy. Many hybrid machine translation systems have been successful in improving the accuracy of the translations, and there are several popular machine translation systems which employ hybrid methods. Among these are PROMT, SYSTRAN and Omniscien Technologies (formerly Asia Online).

Approaches
Multi-engine
This approach to hybrid machine translation involves running multiple machine translation systems in parallel. The final output is generated by combining the output of all the sub-systems. Most commonly, these systems use statistical and rule-based translation subsystems, but other combinations have been explored. For example, researchers at Carnegie Mellon University have had some success combining example-based, transfer-based, knowledge-based and statistical translation sub-systems into one machine translation system.

Statistical rule generation
This approach involves using statistical data to generate lexical and syntactic rules. The input is then processed with these rules as if it were a rule-based translator. This approach attempts to avoid the difficult and time-consuming task of creating a set of comprehensive, fine-grained linguistic rules by extracting those rules from the training corpus. This approach still suffers from many problems of normal statistical machine translation, namely that the accuracy of the translation will depend heavily on the similarity of the input text to the text of the training corpus. As a result, this technique has had the most success in domain-specific applications, and has the same difficulties with domain adaptation as many statistical machine translation systems.

Multi-Pass
This approach involves serially processing the input multiple times. The most common technique used in multi-pass machine translation systems is to pre-process the input with a rule-based machine translation system. The output of the rule-based pre-processor is passed to a statistical machine translation system, which produces the final output. This technique is used to limit the amount of information a statistical system need consider, significantly reducing the processing power required. It also removes the need for the rule-based system to  be a complete translation system for the language, significantly reducing the amount of human effort and labor necessary to build the system.

Confidence-Based
This approach differs from the other hybrid approaches in that in most cases only one translation technology is used. A confidence metric is produced for each translated sentence from which a decision can be made whether to try a secondary translation technology or to proceed with the initial translation output. Omniscien Technologies is one company using this approach, with NMT being the primary technology, but falling back to SMT if the confidence score is below a threshold or the sentence length is very short (i.e. 1 or 2 words). SMT is also used when common error patterns such as multiple repeat words appear in sequence, as is common with NMT when the attention mechanism is confused.

See also
Machine translation
Neural machine translation
Natural language processing


== References ==
Rule-based machine translation (RBMT; "Classical Approach" of MT) is machine translation systems based on linguistic information about source and target languages basically retrieved from (unilingual, bilingual or multilingual) dictionaries and grammars covering the main semantic, morphological, and syntactic regularities of each language respectively.  Having input sentences (in some source language), an RBMT system generates them to output sentences (in some target language) on the basis of morphological, syntactic, and semantic analysis of both the source and the target languages involved in a concrete translation task.

History
The first RBMT systems were developed in the early 1970s. The most important steps of this evolution were the emergence of the following RBMT systems:

Systran (http://www.systran.de/)
Japanese MT systems (http://aamt.info/english/mtsys.htm, http://www.wtec.org/loyola/ar93_94/mt.htm)
EUROTRA (Eurotra)Today, other common RBMT systems include:

Apertium
GramTrans

Types of RBMT
There are three different types of rule-based machine translation systems:

Direct Systems (Dictionary Based Machine Translation) map input to output with basic rules.
Transfer RBMT Systems (Transfer Based Machine Translation)  employ morphological and syntactical analysis.
Interlingual RBMT Systems (Interlingua) use an abstract meaning.RBMT systems can also be characterized as the systems opposite to Example-based Systems of Machine Translation (Example Based Machine Translation), whereas Hybrid Machine Translations Systems make use of many principles derived from RBMT.

Basic principles
The main approach of RBMT systems is based on linking the structure of the given input sentence with the structure of the demanded output sentence, necessarily preserving their unique meaning. The following example can illustrate the general frame of RBMT:  

A girl eats an apple. Source Language = English; Demanded Target Language = GermanMinimally, to get a German translation of this English sentence one needs:

A dictionary that will map each English word to an appropriate German word.
Rules representing regular English sentence structure.
Rules representing regular German sentence structure.And finally, we need rules according to which one can relate these two structures together.
Accordingly, we can state the following stages of translation:

1st:  getting basic part-of-speech information of each source word:a = indef.article; girl = noun; eats = verb; an = indef.article; apple = noun2nd: getting syntactic information about the verb "to eat":NP-eat-NP; here: eat – Present Simple, 3rd Person Singular, Active Voice3rd: parsing the source sentence:(NP an apple) = the object of eatOften only partial parsing is sufficient to get to the syntactic structure of the source sentence and to map it onto the structure of the target sentence.

4th: translate English words into Germana (category = indef.article) => ein (category = indef.article)girl (category = noun) => Mädchen (category = noun)eat (category = verb) => essen (category = verb)an (category = indef. article) => ein (category = indef.article)apple (category = noun) => Apfel (category = noun)5th: Mapping dictionary entries into appropriate inflected forms (final generation):A girl eats an apple. => Ein Mädchen isst einen Apfel.

Components
The RBMT system contains:

a SL morphological analyser - analyses a source language word and provides the morphological information;
a SL parser - is a syntax analyser which analyses source language sentences;
a translator - used to translate a source language word into the target language;
a TL morphological generator - works as a generator of appropriate target language words for the given grammatica information;
a TL parser - works as a composer of suitable target language sentences;
Several dictionaries - more specifically a minimum of three dictionaries:a SL dictionary - needed by the source language morphological analyser for morphological analysis,a bilingual dictionary - used by the translator to translate source language words into target language words,a TL dictionary - needed by the target language morphological generator to generate target language words.The RBMT system makes use of the following:

a Source Grammar for the input language which builds syntactic constructions from input sentences;
a Source Lexicon which captures all of the allowable vocabulary in the domain;
Source Mapping Rules which indicate how syntactic heads and grammatical functions in the source language are mapped onto domain concepts and semantic roles in the interlingua;
a Domain Model/Ontology which defines the classes of domain concepts and restricts the fillers of semantic roles for each class;
Target Mapping Rules which indicate how domain concepts and semantic roles in the interlingua are mapped onto syntactic heads and grammatical functions in the target language;
a Target Lexicon which contains appropriate target lexemes for each domain concept;
a Target Grammar for the target language which realizes target syntactic constructions as linearized output sentences.

Advantages
No bilingual texts are required. This makes it possible to create translation systems for languages that have no texts in common, or even no digitized data whatsoever.
Domain independent. Rules are usually written in a domain independent manner, so the vast majority of rules will "just work" in every domain, and only a few specific cases per domain may need rules written for them.
No quality ceiling. Every error can be corrected with a targeted rule, even if the trigger case is extremely rare. This is in contrast to statistical systems where infrequent forms will be washed away by default.
Total control. Because all rules are hand-written, you can easily debug a rule based system to see exactly where a given error enters the system, and why.
Reusability. Because RBMT systems are generally built from a strong source language analysis that is fed to a transfer step and target language generator, the source language analysis and target language generation parts can be shared between multiple translation systems, requiring only the transfer step to be specialized. Additionally, source language analysis for one language can be reused to bootstrap a closely related language analysis.

Shortcomings
Insufficient amount of really good dictionaries. Building new dictionaries is expensive.
Some linguistic information still needs to be set manually.
It is hard to deal with rule interactions in big systems, ambiguity, and idiomatic expressions.
Failure to adapt to new domains. Although RBMT systems usually provide a mechanism to create new rules and extend and adapt the lexicon, changes are usually very costly and the results, frequently, do not pay off.

References
Literature
Arnold, D.J. et al. (1993): Machine Translation: an Introductory Guide
Hutchins, W.J. (1986): Machine Translation: Past, Present, Future

Links
First International Workshop on Free/Open-Source Rule-Based Machine Translation
https://web.archive.org/web/20120306014535/http://www.inf.ed.ac.uk/teaching/courses/mt/lectures/history.pdf
https://web.archive.org/web/20150914205051/http://www.csse.unimelb.edu.au/research/lt/nlp06/materials/Bond/mt-intro.pdf
William C. "Bill" Mann (died August 13, 2004, aged 69) was a computer scientist and computational linguist, the originator of Rhetorical Structure Theory (RST) and a president of the Association for Computational Linguistics (1987–1988). He is especially well known for his work in text generation.
He received a Ph.D. in artificial intelligence and computer science at Carnegie Mellon University under Herbert Simon and Allen Newell.
From the mid-1970s until 1990, he was a researcher at the Information Sciences Institute of the University of Southern California. From 1990 to 1996, he was a consultant with the Summer Institute of Linguistics, based in Nairobi.

Publications
William C. Mann and Sandra A. Thompson, "Rhetorical structure theory: toward a functional theory of text organization", Text 8:243-281 (1988).
Maite Taboada, William C. Mann, "Applications of Rhetorical Structure Theory", Discourse Studies 8:3:567-588 (2006)

Notes
Bibliography
Christian M.I.M. Matthiessen, "Remembering Bill Mann", Computational Linguistics 31:2:161-171

External links
Bibliography of publications and reports by the creators of RST
MASC is a balanced subset of 500K words of written texts and transcribed speech drawn primarily from the Open American National Corpus (OANC). The OANC is a 15 million word (and growing) corpus of American English produced since 1990, all of which is in the public domain or otherwise free of usage and redistribution restrictions.
All of MASC includes manually validated annotations for logical structure (headings, sections, paragraphs, etc.), sentence boundaries, three different tokenizations with associated part of speech tags, shallow parse (noun and verb chunks), named entities (person, location, organization, date and time), and Penn Treebank syntax. Additional manually produced or validated annotations have been produced by the MASC project for portions of the sub-corpus, including full-text annotation for FrameNet frame elements and a 100K+ sentence corpus with WordNet 3.1 sense tags, of which one-tenth are also annotated for FrameNet frame elements. Annotations of all or portions of the sub-corpus for a wide variety of other linguistic phenomena have been contributed by other projects, including PropBank, TimeBank, MPQA opinion, and several others. Co-reference annotations and clause boundaries of the entire MASC corpus are scheduled to be released by the end of 2016.
WordNet sense annotations for all occurrences of 114 words are also included in the MASC distribution, as well as FrameNet annotations for 50-100 occurrences of each of the 114 words. The sentences with WordNet and FrameNet annotations are also distributed as a part of the MASC Sentence Corpus.

Genres
Unlike most freely available corpora including a wide variety of linguistic annotations, MASC contains a balanced selection of texts from a broad range of genres:

Annotations
At present, MASC includes seventeen different types of linguistic annotation (* = in production; ** currently available in original format only):

All MASC annotations, whether contributed or produced in-house, are transduced to the Graph Annotation Format (GrAF) defined by ISO TC37 SC4’s Linguistic Annotation Framework (LAF).
The online tool ANC2Go can transduce annotations over all or parts of MASC to any of several other formats, including CONLL IOB format and formats for use in UIMA and General Architecture for Text Engineering.

Distribution
MASC is an open data resource that can be used by anyone for any purpose. At the same time, it is a collaborative community resource that is sustained by community contributions of annotations and derived data. It is freely downloadable from the MASC download page or through the Linguistic Data Consortium.
MASC is also distributed in part-of-speech-tagged form with the Natural Language Toolkit.

See also
American National Corpus

References
Ide, N., Baker, C., Fellbaum, C., Passonneau, R. (2010). The Manually Annotated Sub-Corpus: A Community Resource For and By the People. Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, Uppsala, Sweden.
Passonneau, R., Baker, C., Fellbaum, C., Ide, N. (2012). The MASC Word Sense Sentence Corpus. Proceedings of the Eighth Language Resources and Evaluation Conference, Istanbul.
Ide, N., Suderman, K., Simms, B. (2010). ANC2Go: A Web Application for Customized Corpus Creation. Proceedings of the Seventh Language Resources and Evaluation Conference (LREC 2010), Valletta, Malta.

External links
MASC website
American National Corpus website
The MAtrixware REsearch Collection (MAREC) is a standardised patent data corpus available for research purposes. MAREC seeks to represent patent documents of several languages in order to answer specific research questions. It consists of 19 million patent documents in different languages, normalised to a highly specific XML schema.
MAREC is intended as raw material for research in areas such as information retrieval, natural language processing or machine translation, which require large amounts of complex documents. The collection contains documents in 19 languages, the majority being English, German and French, and about half of the documents include full text.
In MAREC, the documents from different countries and sources are normalised to a common XML format with a uniform patent numbering scheme and citation format. The standardised fields include dates, countries, languages, references, person names, and companies as well as subject classifications such as IPC codes.MAREC is a comparable corpus, where many documents are available in similar versions in other languages. A comparable corpus can be defined as consisting of texts that share similar topics – news text from the same time period in different countries, while a parallel corpus is defined as a collection of documents with aligned translations from the source to the target language. Since the patent document refers to the same “invention” or “concept of idea” the text is a translation of the invention, but it does not have to be a direct translation of the text itself – text parts could have been removed or added for clarification reasons.
The 19,386,697 XML files measure a total of 621 GB and are hosted by the Information Retrieval Facility. Access and support are free of charge for research purposes.

Use Cases
MAREC is used in the Patent Language Translations Online (PLuTO) project.

References
External links
User guide and statistics
Information Retrieval Facility
MeaningCloud is a Software as a Service product that enables users to embed text analytics and semantic processing in any application or system. It was previously branded as Textalytics.
MeaningCloud extends the concept of semantic API with a cloud-based framework that makes the integration of semantic processing into any system something close to a plug-and-play experience. MeaningCloud is available both in SaaS mode and on-premises.

Functionality
Topic Extraction: identifies appearances of named entities and abstract concepts in the text.
Text Classification: assigns a text to one or several categories in a predefined taxonomy.
Sentiment Analysis: assigns a polarity (positive, negative, neutral) to a document or to the individual topics or attributes appearing in a document (aspect-based sentiment).
Text Clustering: discovers the underlying themes in a document collection and groups these documents according to their similarities and their adherence to those themes.

Integration and customization
Advanced APIs provide a functionality optimized for diverse applications and ease of use. In addition, customization and integration capabilities offer a fast learning curve and a short time to obtain results.

Customized resource management tools allow users to easily incorporate their own semantic resources (dictionaries, taxonomies, sentiment models) to adapt the operation of the system to their needs.
SDKs and plug-ins increase the convenience and integrability of the APIs in the most common environments and platforms. MeaningCloud provides SDKs for Java, Python, PHP, and Visual Basic, and plug-ins for Microsoft Excel and GATE.

About the brand and the company
MeaningCloud is a brand by MeaningCloud LLC, a wholly owned subsidiary of MeaningCloud Europe S.L., previously known as Daedalus. Daedalus was founded in 1998 by Jose C Gonzalez and other colleagues as a spin-off from their Artificial Intelligence research lab at the Technical University of Madrid.

See also
Software as a service
Natural language processing
Computational linguistics
Text mining
Media monitoring
Social media measurement
Semantic publishing
Semantic technology

References
External links
MeaningCloud website
MeCab is an open-source text segmentation library for use with text written in the Japanese language originally developed by the Nara Institute of Science and Technology and currently maintained by Taku Kudou (工藤拓) as part of his work on the Google Japanese Input project. The name derives from the developer's favorite food, mekabu (和布蕪), a Japanese dish made from wakame leaves.
The software was originally based on ChaSen and was developed under the name ChaSenTNG, but now it is developed independently from ChaSen and was rewritten from scratch. MeCab's analysis accuracy is comparable to ChaSen, and its analysis speed is 3–4 times faster on average.
MeCab can analyze and segment a sentence into its parts of speech. There are several dictionaries available for MeCab, but IPADIC is the most commonly used one as with ChaSen.
In 2007, Google used MeCab to generate n-gram data for a large corpus of Japanese text, which it published on its Google Japan blog.MeCab is also used for Japanese input on Mac OS X 10.5 and 10.6, and in iOS since version 2.1.

Example
Input:

ウィキペディア（Ｗｉｋｉｐｅｄｉａ）は誰でも編集できるフリー百科事典です
Results in:

ウィキペディア	名詞,一般,*,*,*,*,*
（	記号,括弧開,*,*,*,*,（,（,（
Ｗｉｋｉｐｅｄｉａ	名詞,固有名詞,組織,*,*,*,*
）	記号,括弧閉,*,*,*,*,）,）,）
は	助詞,係助詞,*,*,*,*,は,ハ,ワ
誰	名詞,代名詞,一般,*,*,*,誰,ダレ,ダレ
でも	助詞,副助詞,*,*,*,*,でも,デモ,デモ
編集	名詞,サ変接続,*,*,*,*,編集,ヘンシュウ,ヘンシュー
できる	動詞,自立,*,*,一段,基本形,できる,デキル,デキル
フリー	名詞,一般,*,*,*,*,フリー,フリー,フリー
百科	名詞,一般,*,*,*,*,百科,ヒャッカ,ヒャッカ
事典	名詞,一般,*,*,*,*,事典,ジテン,ジテン
です	助動詞,*,*,*,特殊・デス,基本形,です,デス,デス
EOS

Besides segmenting the text, MeCab also lists the part of speech of the word, and, if applicable and in the dictionary, its pronunciation. In the above example, the verb できる (dekiru, "to be able to") is classified as an ichidan (一段) verb (動詞) in the infinitive tense (基本形). The word でも (demo) is identified as an adverbial particle (副助詞). As not all columns apply to all words, when a column does not apply to a word, an asterisk is used; this makes it possible to format the information after the word and the tab character as  the comma-separated values.
MeCab also supports several output formats; one of which, chasen, outputs tab-separated values in a format that programs written for ChaSen can use. Another format, yomi (from 読む yomu, to read), outputs the pronunciation of the input text as katakana, as shown below.

ウィキペディア（Ｗｉｋｉｐｅｄｉａ）ハダレデモヘンシュウデキルフリーヒャッカジテンデス

References
External links
 
Official website
METEOR (Metric for Evaluation of Translation with Explicit ORdering) is a metric for the evaluation of machine translation output. The metric is based on the harmonic mean of unigram precision and recall, with recall weighted higher than precision. It also has several features that are not found in other metrics, such as stemming and synonymy matching, along with the standard exact word matching. The metric was designed to fix some of the problems found in the more popular BLEU metric, and also produce good correlation with human judgement at the sentence or segment level. This differs from the BLEU metric in that BLEU seeks correlation at the corpus level.

Results have been presented which give correlation of up to 0.964 with human judgement at the corpus level, compared to BLEU's achievement of 0.817 on the same data set. At the sentence level, the maximum correlation with human judgement achieved was 0.403.[1]

Algorithm
As with BLEU, the basic unit of evaluation is the sentence, the algorithm first creates an alignment (see illustrations) between two sentences, the candidate translation string, and the reference translation string. The alignment is a set of mappings between unigrams. A mapping can be thought of as a line between a unigram in one string, and a unigram in another string. The constraints are as follows; every unigram in the candidate translation must map to zero or one unigram in the reference. Mappings are selected to produce an alignment as defined above. If there are two alignments with the same number of mappings, the alignment is chosen with the fewest crosses, that is, with fewer intersections of two mappings. From the two alignments shown, alignment (a) would be selected at this point. Stages are run consecutively and each stage only adds to the alignment those unigrams which have not been matched in previous stages. Once the final alignment is computed, the score is computed as follows: Unigram precision P is calculated as:

  
    
      
        P
        =
        
          
            m
            
              w
              
                t
              
            
          
        
      
    
    {\displaystyle P={\frac {m}{w_{t}}}}
  Where m is the number of unigrams in the candidate translation that are also found in the reference translation, and 
  
    
      
        
          w
          
            t
          
        
      
    
    {\displaystyle w_{t}}
   is the number of unigrams in the candidate translation. Unigram recall R is computed as:

  
    
      
        R
        =
        
          
            m
            
              w
              
                r
              
            
          
        
      
    
    {\displaystyle R={\frac {m}{w_{r}}}}
  Where m is as above, and 
  
    
      
        
          w
          
            r
          
        
      
    
    {\displaystyle w_{r}}
   is the number of unigrams in the reference translation. Precision and recall are combined using the harmonic mean in the following fashion, with recall weighted 9 times more than precision:

  
    
      
        
          F
          
            m
            e
            a
            n
          
        
        =
        
          
            
              10
              P
              R
            
            
              R
              +
              9
              P
            
          
        
      
    
    {\displaystyle F_{mean}={\frac {10PR}{R+9P}}}
  The measures that have been introduced so far only account for congruity with respect to single words but not with respect to larger segments that appear in both the reference and the candidate sentence. In order to take these into account, longer n-gram matches are used to compute a penalty p for the alignment. The more mappings there are that are not adjacent in the reference and the candidate sentence, the higher the penalty will be. 
In order to compute this penalty, unigrams are grouped into the fewest possible chunks, where a chunk is defined as a set of unigrams that are adjacent in the hypothesis and in the reference. The longer the adjacent mappings between the candidate and the reference, the fewer chunks there are. A translation that is identical to the reference will give just one chunk. The penalty p is computed as follows, 

  
    
      
        p
        =
        0.5
        
          
            (
            
              
                c
                
                  u
                  
                    m
                  
                
              
            
            )
          
          
            3
          
        
      
    
    {\displaystyle p=0.5\left({\frac {c}{u_{m}}}\right)^{3}}
  Where c is the number of chunks, and 
  
    
      
        
          u
          
            m
          
        
      
    
    {\displaystyle u_{m}}
   is the number of unigrams that have been mapped. The final score for a segment is calculated as M below. The penalty has the effect of reducing the 
  
    
      
        
          F
          
            m
            e
            a
            n
          
        
      
    
    {\displaystyle F_{mean}}
   by up to 50% if there are no bigram or longer matches.

  
    
      
        M
        =
        
          F
          
            m
            e
            a
            n
          
        
        (
        1
        −
        p
        )
      
    
    {\displaystyle M=F_{mean}(1-p)}
  To calculate a score over a whole corpus, or collection of segments, the aggregate values for P, R and p are taken and then combined using the same formula. The algorithm also works for comparing a candidate translation against more than one reference translations. In this case the algorithm compares the candidate against each of the references and selects the highest score.

Examples
Score: 0.5000 = Fmean: 1.0000 × (1 − Penalty: 0.5000)
Fmean: 1.0000 = 10 × Precision: 1.0000 × Recall: 1.0000 / （Recall: 1.0000 + 9 × Precision: 1.0000）
Penalty: 0.5000 = 0.5 × (Fragmentation: 1.0000 ^3)
Fragmentation: 1.0000 = Chunks: 6.0000 / Matches: 6.0000

Score: 0.9977 = Fmean: 1.0000 × (1 − Penalty: 0.0023)
Fmean: 1.0000 = 10 × Precision: 1.0000 × Recall: 1.0000 / （Recall: 1.0000 + 9 × Precision: 1.0000）
Penalty: 0.0023 = 0.5 × (Fragmentation: 0.1667 ^3) 
Fragmentation: 0.1667 = Chunks: 1.0000 / Matches: 6.0000

Score: 0.9654 = Fmean: 0.9836 × (1 − Penalty: 0.0185)
Fmean: 0.9836 = 10 × Precision: 0.8571 × Recall: 1.0000 / （Recall: 1.0000 + 9 × Precision: 0.8571）
Penalty: 0.0185 = 0.5 × (Fragmentation: 0.3333 ^3)
Fragmentation: 0.3333 = Chunks: 2.0000 / Matches: 6.0000

See also
BLEU
F-Measure
NIST (metric)
ROUGE (metric)
Word Error Rate (WER)
Noun-Phrase Chunking

Notes
^  Banerjee, S. and Lavie, A. (2005)

References
Banerjee, S. and Lavie, A. (2005) "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments" in Proceedings of Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization at the 43rd Annual Meeting of the Association of Computational Linguistics (ACL-2005), Ann Arbor, Michigan, June 2005
Lavie, A., Sagae, K. and Jayaraman, S. (2004) "The Significance of Recall in Automatic Metrics for MT Evaluation" in Proceedings of AMTA 2004, Washington DC. September 2004

External links
The METEOR Automatic Machine Translation Evaluation System (including link for download)
Minimal recursion semantics (MRS) is a framework for computational semantics. It can be implemented in typed feature structure formalisms such as head-driven phrase structure grammar and lexical functional grammar. It is suitable for computational language parsing and natural language generation. MRS enables a simple formulation of the grammatical constraints on lexical and phrasal semantics, including the principles of semantic composition. This technique is used in machine translation.Early pioneers of MRS include Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan Sag.

See also
DELPH-IN


== References ==
A morphological pattern is a set of associations and/or operations that build the various forms of a lexeme, possibly by inflection, agglutination, compounding or derivation.

Context
The term is used in the domain of lexicons and morphology.

Note
It is important to distinguish the paradigm of a lexeme from a morphological pattern. In the context of an inflecting language, an inflectional morphological pattern is not the explicit list of inflected forms. A morphological pattern usually references a prototypical class of inflectional forms, e.g. ring as per sing. In contrast, the paradigm of a lexeme is the explicit list of the inflected forms of the given lexeme (e.g. to ring, rang, rung). Said in other terms, this is the difference between a description in intension (a morphological pattern) and a description in extension (a paradigm).

See also
lexical markup framework
morphology (linguistics)
Word formation

Sources
Aronoff, Mark (1993). "Morphology by Itself". Cambridge, MA: MIT Press.
Comrie, Bernard. (1989). Language Universals and Linguistic Typology; 2nd ed. Chicago: University of Chicago Press. ISBN 0-226-11433-3 (pbk).
Matthews, Peter. (1991). Morphology; 2nd ed. Cambridge: Cambridge University Press. ISBN 0-521-41043-6 (hb). ISBN 0-521-42256-6 (pbk).
Mel'čuk, Igor A. (1993-2000). Cours de morphologie générale, vol. 1-5. Montreal: Presses de l'Université de Montréal.
Stump, Gregory T. (2001). Inflectional Morphology: a theory of paradigm structure. (Cambridge Studies in Linguistics; no. 93.) Cambridge: Cambridge University Press. ISBN 0-521-78047-0 (hbk).
Multi-document summarization is an automatic procedure aimed at extraction of information from multiple texts written about the same topic. The resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents. In such a way, multi-document summarization systems are complementing the news aggregators performing the next step down the road of coping with information overload.

Key benefits
Multi-document summarization creates information reports that are both concise and comprehensive.
With different opinions being put together & outlined, every topic is described from multiple perspectives within a single document.
While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required.
Automatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased.

Technological challenges
The multi-document summarization task is more complex than summarizing a single document, even a long one. The difficulty arises from thematic diversity within a large set of documents. A good summarization technology aims to combine the main themes with completeness, readability, and concision. The Document Understanding Conferences, conducted annually by NIST, have developed sophisticated evaluation criteria for techniques accepting the multi-document summarization challenge.
An ideal multi-document summarization system not only shortens the source texts, but also presents information organized around the key aspects to represent diverse views. Success produces an overview of a given topic. Such text compilations should also basic requirements for an overview text compiled by a human. The multi-document summary quality criteria are as follows:

clear structure, including an outline of the main content, from which it is easy to navigate to the full text sections
text within sections is divided into meaningful paragraphs
gradual transition from more general to more specific thematic aspects
good readability.The latter point deserves an additional note. Care is taken to ensure that the automatic overview shows:

no paper-unrelated "information noise" from the respective documents (e.g., web pages)
no dangling references to what is not mentioned or explained in the overview
no text breaks across a sentence
no semantic redundancy.

Real-life systems
The multi-document summarization technology is now coming of age - a view supported by a choice of advanced web-based systems that are currently available.

Ultimate Research Assistant - performs text mining on Internet search results to help summarize and organize them and make it easier for the user to perform online research. Specific text mining techniques used by the tool include concept extraction, text summarization, hierarchical concept clustering (e.g., automated taxonomy generation), and various visualization techniques, including tag clouds and mind maps.
iResearch Reporter - Commercial Text Extraction and Text Summarization system, free demo site accepts user-entered query, passes it on to Google search engine, retrieves multiple relevant documents, produces categorized, easily  readable natural language summary reports covering multiple documents in retrieved set, all extracts linked to original documents on the Web, post-processing, entity extraction, event and relationship extraction, text extraction, extract clustering, linguistic analysis, multi-document, full text, natural language processing, categorization rules, clustering, linguistic analysis, text summary construction tool set.
Newsblaster is a system that helps users find news that is of the most interest to them. The system automatically collects, clusters, categorizes, and summarizes news from several sites on the web (CNN, Reuters, Fox News, etc.) on a daily basis, and it provides users an interface to browse the results.
NewsInEssence may be used to retrieve and summarize a cluster of articles from the web. It can start from a URL and retrieve documents that are similar, or it can retrieve documents that match a given set of keywords. NewsInEssence also downloads news articles daily and produces news clusters from them.
NewsFeed Researcher is a news portal performing continuous automatic summarization of documents initially clustered by the news aggregators (e.g., Google News). NewsFeed Researcher is backed by a free online engine covering major events related to business, technology, U.S. and international news. This tool is also available in on-demand mode allowing a user to build a summaries on selected topics.
Scrape This is like a search engine, but instead of providing links to the most relevant websites based on a query, it scrapes the pertinent information off of the relevant websites and provides the user with a consolidated multi-document summary, along with dictionary definitions, images, and videos.
JistWeb is a query specific multiple document summariser.As auto-generated multi-document summaries increasingly resemble the overviews written by a human, their use of extracted text snippets may one day face copyright issues in relation to the fair use copyright concept.

Bibliography
Günes Erkan; Dragomir R. Radev (1 December 2004), "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization", Journal of Artificial Intelligence Research, 22: 457–479, doi:10.1613/JAIR.1523, Wikidata  Q81312697 
Dragomir R. Radev, Hongyan Jing, Malgorzata Styś, and Daniel Tam. Centroid-based summarization of multiple documents. Information Processing and Management, 40:919–938, December 2004. [5]
Kathleen R. McKeown and Dragomir R. Radev. Generating summaries of multiple news articles. In Proceedings, ACM Conference on Research and Development in Information Retrieval SIGIR'95, pages 74–82, Seattle, Washington, July 1995. [6]
C.-Y. Lin, E. Hovy, "From single to multi-document summarization: A prototype system and its evaluation", In "Proceedings of the ACL", pp. 457–464, 2002
Kathleen McKeown, Rebecca J. Passonneau, David K. Elson, Ani Nenkova, Julia Hirschberg, "Do Summaries Help? A Task-Based Evaluation of Multi-Document Summarization", SIGIR’05, Salvador, Brazil, August 15–19, 2005 [7]
R. Barzilay, N. Elhadad, K. R. McKeown, "Inferring strategies for sentence ordering in multidocument news summarization", Journal of Artificial Intelligence Research, v. 17, pp. 35–55, 2002
M. Soubbotin, S. Soubbotin, "Trade-Off Between Factors Influencing Quality of the Summary", Document Understanding Workshop (DUC), Vancouver, B.C., Canada, October 9–10, 2005 [8]
C Ravindranath Chowdary, and P. Sreenivasa Kumar. "Esum: an efficient system for query-specific multi-document summarization." In ECIR (Advances in Information Retrieval), pp. 724–728. Springer Berlin Heidelberg, 2009.

See also
Automatic summarization
Text mining
News aggregators

References
External links
Document Understanding Conferences
Columbia NLP Projects
NewsInEssence: Web-based News Summarization
A multilingual notation is a representation in a lexical resource that allows the translation between two or more words.

UML diagrams
For instance, within LMF, a multilingual notation could be as presented in the following diagram, for English / French translation. In this diagram, two intermediate SenseAxis instances are used in order to represent a near match between fleuve in French and river in English. The SenseAxis instance on the bottom is not linked directly to any English sense because this notion does not exist in English.

A more complex situation is when more than two languages are concerned, as in the following diagram dealing with English, Italian and Spanish.

Number of languages considerations
Within the context of a multilingual database comprising more than two languages, usually the multilingual notations are factorized, in order to save the number of links. In other terms, the multilingual notations are interlingual nodes that are shared among the language descriptions.
But in the specific context of a lexical resource that is a bilingual lexicon, the term bilingual link is usually preferred.

Other terminology
Let us note that instead of translation (that has a rather broad meaning), some authors prefer equivalence between words, with different notions like dynamic and formal equivalences.

Context of use
This term is mainly used in the context of Machine translation and NLP lexicons. The term is not used in the context of translation dictionary that concerns mainly hand-held electronic translators.

See also
lexical markup framework

External links
workshop on multilingual language resources
Multimodal sentiment analysis is a new dimension of the traditional text-based sentiment analysis, which goes beyond the analysis of texts, and includes other modalities such as audio and visual data. It can be bimodal, which includes different combinations of two modalities, or trimodal, which incorporates three modalities. With the extensive amount of social media data available online in different forms such as videos and images, the conventional text-based sentiment analysis has evolved into more complex models of multimodal sentiment analysis, which can be applied in the development of  virtual assistants, analysis of YouTube movie reviews, analysis of news videos, and emotion recognition (sometimes known as emotion detection) such as depression monitoring, among others.
Similar to the traditional sentiment analysis, one of the most basic task in multimodal sentiment analysis is sentiment classification, which classifies different sentiments into categories such as positive, negative, or neutral. The complexity of analyzing text, audio, and visual features to perform such a task requires the application of different fusion techniques, such as feature-level, decision-level, and hybrid fusion. The performance of these fusion techniques and the classification algorithms applied, are influenced by the type of textual, audio, and visual features employed in the analysis.

Features
Feature engineering, which involves the selection of features that are fed into machine learning algorithms, plays a key role in the sentiment classification performance. In multimodal sentiment analysis, a combination of different textual, audio, and visual features are employed.

Textual features
Similar to the conventional text-based sentiment analysis, some of the most commonly used textual features in multimodal sentiment analysis are unigrams and n-grams, which are basically a sequence of words in a given textual document. These features are applied using bag-of-words or bag-of-concepts feature representations, in which words or concepts are represented as vectors in a suitable space.

Audio features
Sentiment and emotion characteristics are prominent in different phonetic and prosodic properties contained in audio features. Some of the most important audio features employed in multimodal sentiment analysis are  mel-frequency cepstrum (MFCC), spectral centroid, spectral flux, beat histogram, beat sum, strongest beat, pause duration, and pitch. OpenSMILE and Praat are popular open-source toolkits for extracting such audio features.

Visual features
One of the main advantages of analyzing videos with respect to texts alone, is the presence of rich sentiment cues in visual data. Visual features include facial expressions, which are of paramount importance in capturing sentiments and emotions, as they are a main channel of forming a person's present state of mind. Specifically, smile, is considered to be one of the most predictive visual cues in multimodal sentiment analysis. OpenFace is an open-source facial analysis toolkit available for extracting and understanding such visual features.

Fusion techniques
Unlike the traditional text-based sentiment analysis, multimodal sentiment analysis undergo a fusion process in which data from different modalities (text, audio, or visual) are fused and analyzed together. The existing approaches in multimodal sentiment analysis data fusion can be grouped into three main categories: feature-level, decision-level, and hybrid fusion, and the performance of the sentiment classification depends on which type of fusion technique is employed.

Feature-level fusion
Feature-level fusion (sometimes known as early fusion) gathers all the features from each modality (text, audio, or visual) and joins them together into a single feature vector, which is eventually fed into a classification algorithm. One of the difficulties in implementing this technique is the integration of the heterogeneous features.

Decision-level fusion
Decision-level fusion (sometimes known as late fusion), feeds data from each modality (text, audio, or visual) independently into its own classification algorithm, and obtains the final sentiment classification results by fusing each result into a single decision vector. One of the advantages of this fusion technique is that it eliminates the need to fuse heterogeneous data, and each modality can utilize its most appropriate classification algorithm.

Hybrid fusion
Hybrid fusion is a combination of feature-level and decision-level fusion techniques, which exploits complementary information from both methods during the classification process. It usually involves a two-step procedure wherein feature-level fusion is initially performed between two modalities, and decision-level fusion is then applied as a second step, to fuse the initial results from the feature-level fusion, with the remaining modality.

Applications
Similar to text-based sentiment analysis, multimodal sentiment analysis can be applied in the development of different forms of recommender systems such as in the analysis of user-generated videos of movie reviews and general product reviews, to predict the sentiments of customers, and subsequently create product or service recommendations. Multimodal sentiment analysis also plays an important role in the advancement of virtual assistants through the application of natural language processing (NLP) and machine learning techniques. In the healthcare domain, multimodal sentiment analysis can be utilized to detect certain medical conditions such as stress, anxiety, or depression. Multimodal sentiment analysis can also be applied in understanding the sentiments contained in video news programs, which is considered as a complicated and challenging domain, as sentiments expressed by reporters tend to be less obvious or neutral.


== References ==
In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application.  The n-grams typically are collected from a text or speech corpus. When the items are words, n-grams may also be called shingles.Using Latin numerical prefixes, an n-gram of size 1 is referred to as a "unigram"; size 2 is a "bigram" (or, less commonly, a "digram"); size 3 is a "trigram". English cardinal numbers are sometimes used, e.g., "four-gram", "five-gram", and so on.    In computational biology, a polymer or oligomer of a known size is called a k-mer instead of an n-gram, with specific names using Greek numerical prefixes such as "monomer", "dimer", "trimer", "tetramer", "pentamer", etc., or English cardinal numbers, "one-mer", "two-mer", "three-mer", etc.

Applications
An n-gram model is a type of probabilistic language model for predicting the next item in such a sequence in the form of a (n − 1)–order Markov model. n-gram models are now widely used in probability, communication theory, computational linguistics (for instance, statistical natural language processing), computational biology (for instance, biological sequence analysis), and data compression. Two benefits of n-gram models (and algorithms that use them) are simplicity and scalability – with larger n, a model can store more context with a well-understood space–time tradeoff, enabling small experiments to scale up efficiently.

Examples
Figure 1 shows several example sequences and the corresponding 1-gram, 2-gram and 3-gram sequences.
Here are further examples; these are word-level 3-grams and 4-grams (and counts of the number of times they appeared) from the Google n-gram corpus.3-grams

ceramics collectables collectibles (55)
ceramics collectables fine (130)
ceramics collected by (52)
ceramics collectible pottery (50)
ceramics collectibles cooking (45)4-grams

serve as the incoming (92)
serve as the incubator (99)
serve as the independent (794)
serve as the index (223)
serve as the indication (72)
serve as the indicator (120)

n-gram models
An n-gram model models sequences, notably natural languages, using the statistical properties of n-grams.
This idea can be traced to an experiment by Claude Shannon's work in information theory.  Shannon posed the question: given a sequence of letters (for example, the sequence "for ex"), what is the likelihood of the next letter?  From training data, one can derive a probability distribution for the next letter given a history of size 
  
    
      
        n
      
    
    {\displaystyle n}
  :  a = 0.4, b = 0.00001, c = 0, ....; where the probabilities of all possible "next-letters" sum to 1.0.
More concisely, an n-gram model predicts 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   based on 
  
    
      
        
          x
          
            i
            −
            (
            n
            −
            1
            )
          
        
        ,
        …
        ,
        
          x
          
            i
            −
            1
          
        
      
    
    {\displaystyle x_{i-(n-1)},\dots ,x_{i-1}}
  . In probability terms, this is 
  
    
      
        P
        (
        
          x
          
            i
          
        
        ∣
        
          x
          
            i
            −
            (
            n
            −
            1
            )
          
        
        ,
        …
        ,
        
          x
          
            i
            −
            1
          
        
        )
      
    
    {\displaystyle P(x_{i}\mid x_{i-(n-1)},\dots ,x_{i-1})}
  . When used for language modeling, independence assumptions are made so that each word depends only on the last n − 1 words.  This Markov model is used as an approximation of the true underlying language.  This assumption is important because it massively simplifies the problem of estimating the language model from data.  In addition, because of the open nature of language, it is common to group words unknown to the language model together.
Note that in a simple n-gram language model, the probability of a word, conditioned on some number of previous words (one word in a bigram model, two words in a trigram model, etc.) can be described as following a categorical distribution (often imprecisely called a "multinomial distribution").
In practice, the probability distributions are smoothed by assigning non-zero probabilities to unseen words or n-grams; see smoothing techniques.

Applications and considerations
n-gram models are widely used in statistical natural language processing.  In speech recognition, phonemes and sequences of phonemes are modeled using a n-gram distribution.  For parsing, words are modeled such that each n-gram is composed of n words.  For language identification, sequences of characters/graphemes (e.g., letters of the alphabet) are modeled for different languages.  For sequences of characters, the 3-grams (sometimes referred to as "trigrams") that can be generated from "good morning" are "goo", "ood", "od ", "d m", " mo", "mor" and so forth, counting the space character as a gram (sometimes the beginning and end of a text are modeled explicitly, adding "_ ⁠_g", "_go", "ng_", and "g_ ⁠_").  For sequences of words,  the trigrams (shingles) that can be generated from "the dog smelled like a skunk" are "# the dog", "the dog smelled", "dog smelled like", "smelled like a", "like a skunk" and "a skunk #".
Practitioners more interested in multiple word terms might preprocess strings to remove spaces. Many simply collapse whitespace to a single space while preserving paragraph marks, because the whitespace is frequently either an element of writing style or introduces layout or presentation not required by the prediction and deduction methodology. Punctuation is also commonly reduced or removed by preprocessing and is frequently used to trigger functionality.
n-grams can also be used for sequences of words or almost any type of data. For example, they have been used for extracting features for clustering large sets of satellite earth images and for determining what part of the Earth a particular image came from.  They have also been very successful as the first pass in genetic sequence search and in the identification of the species from which short sequences of DNA originated.n-gram models are often criticized because they lack any explicit representation of long range dependency. This is because the only explicit dependency range is (n − 1) tokens for an n-gram model, and since natural languages incorporate many cases of unbounded dependencies (such as wh-movement), this means that an n-gram model cannot in principle distinguish unbounded dependencies from noise (since long range correlations drop exponentially with distance for any Markov model). For this reason, n-gram models have not made much impact on linguistic theory, where part of the explicit goal is to model such dependencies.
Another criticism that has been made is that Markov models of language, including n-gram models, do not explicitly capture the performance/competence distinction.  This is because n-gram models are not designed to model linguistic knowledge as such, and make no claims to being (even potentially) complete models of linguistic knowledge; instead, they are used in practical applications.
In practice, n-gram models have been shown  to be extremely effective in modeling language data, which is a core component in modern statistical language applications.
Most modern applications that rely on n-gram based models, such as machine translation applications, do not rely exclusively on such models; instead, they typically also incorporate Bayesian inference. Modern statistical models are typically made up of two parts, a prior distribution describing the inherent likelihood of a possible result and a likelihood function used to assess the compatibility of a possible result with observed data. When a language model is used, it is used as part of the prior distribution (e.g. to gauge the inherent "goodness" of a possible translation), and even then it is often not the only component in this distribution.
Handcrafted features of various sorts are also used, for example variables that represent the position of a word in a sentence or the general topic of discourse. In addition, features based on the structure of the potential result, such as syntactic considerations, are often used. Such features are also used as part of the likelihood function, which makes use of the observed data. Conventional linguistic theory can be incorporated in these features (although in practice, it is rare that features specific to generative or other particular theories of grammar are incorporated, as computational linguists tend to be "agnostic" towards individual theories of grammar).

Out-of-vocabulary words
An issue when using n-gram language models are out-of-vocabulary (OOV) words. They are encountered in computational linguistics and natural language processing when the input includes words which were not present in a system's dictionary or database during its preparation. By default, when a language model is estimated, the entire observed vocabulary is used. In some cases, it may be necessary to estimate the language model with a specific fixed vocabulary. In such a scenario, the n-grams in the corpus that contain an out-of-vocabulary word are ignored. The n-gram probabilities are smoothed over all the words in the vocabulary even if they were not observed.Nonetheless, it is essential in some cases to explicitly model the probability of out-of-vocabulary words by introducing a special token (e.g. <unk>) into the vocabulary. Out-of-vocabulary words in the corpus are effectively replaced with this special <unk> token before n-grams counts are cumulated. With this option, it is possible to estimate the transition probabilities of n-grams involving out-of-vocabulary words.

n-grams for approximate matching
n-grams can also be used for efficient approximate matching.  By converting a sequence of items to a set of n-grams, it can be embedded in a vector space, thus allowing the sequence to be compared to other sequences in an efficient manner. For example, if we convert strings with only letters in the English alphabet into single character 3-grams, we get a 
  
    
      
        
          26
          
            3
          
        
      
    
    {\displaystyle 26^{3}}
  -dimensional space (the first dimension measures the number of occurrences of "aaa", the second "aab", and so forth for all possible combinations of three letters). Using this representation, we lose information about the string. For example, both the strings "abc" and "bca" give rise to exactly the same 2-gram "bc" (although {"ab", "bc"} is clearly not the same as {"bc", "ca"}). However, we know empirically that if two strings of real text have a similar vector representation (as measured by cosine distance) then they are likely to be similar. Other metrics have also been applied to vectors of n-grams with varying, sometimes better, results. For example, z-scores have been used to compare documents by examining how many standard deviations each n-gram differs from its mean occurrence in a large collection, or text corpus, of documents (which form the "background" vector).  In the event of small counts, the g-score (also known as g-test) may give better results for comparing alternative models.
It is also possible to take a more principled approach to the statistics of n-grams, modeling similarity as the likelihood that two strings came from the same source directly in terms of a problem in Bayesian inference.
n-gram-based searching can also be used for plagiarism detection.

Other applications
n-grams find use in several areas of computer science, computational linguistics, and applied mathematics.
They have been used to:

design kernels that allow machine learning algorithms such as support vector machines to learn from string data
find likely candidates for the correct spelling of a misspelled word
improve compression in compression algorithms where a small area of data requires n-grams of greater length
assess the probability of a given word sequence appearing in text of a language of interest in pattern recognition systems, speech recognition, OCR (optical character recognition), Intelligent Character Recognition (ICR), machine translation and similar applications
improve retrieval in information retrieval systems when it is hoped to find similar "documents" (a term for which the conventional meaning is sometimes stretched, depending on the data set) given a single query document and a database of reference documents
improve retrieval performance in genetic sequence analysis as in the BLAST family of programs
identify the language a text is in or the species a small sequence of DNA was taken from
predict letters or words at random in order to create text, as in the dissociated press algorithm.
cryptanalysis

Space required for an n-gram
Consider an n-gram where the units are characters and a text with t characters. The space this n-gram requires is exponential:

  
    
      
        n
        (
        t
        −
        2
        (
        n
        −
        1
        )
        )
        +
        
          ∑
          
            i
            =
            1
          
          
            n
            −
            1
          
        
        2
        i
        
        n
        ,
        t
        ∈
        
          
            N
          
        
      
    
    {\displaystyle n(t-2(n-1))+\sum _{i=1}^{n-1}2i\qquad n,t\in {\mathcal {N}}}
  
A parabola can be fitted through each discrete data point by obtaining three pairs of coordinates and solving a linear system with three variables, which leads to the general formula:

  
    
      
        −
        
          n
          
            2
          
        
        +
        (
        t
        +
        1
        )
        n
      
    
    {\displaystyle -n^{2}+(t+1)n}

Bias-versus-variance trade-off
To choose a value for n in an n-gram model, it is necessary to find the right trade-off between the stability of the estimate against its appropriateness. This means that trigram (i.e. triplets of words) is a common choice with large training corpora (millions of words), whereas a bigram is often used with smaller ones.

Smoothing techniques
There are problems of balance weight between infrequent grams (for example, if a proper name appeared in the training data) and frequent grams.   Also, items not seen in the training data will be given a probability of 0.0 without smoothing. For unseen but plausible data from a sample, one can introduce pseudocounts.  Pseudocounts are generally motivated on Bayesian grounds.
In practice it is necessary to smooth the probability distributions by also assigning non-zero probabilities to unseen words or n-grams.  The reason is that models derived directly from the n-gram frequency counts have severe problems when confronted with any n-grams that have not explicitly been seen before – the zero-frequency problem.  Various smoothing methods are used, from simple "add-one" (Laplace) smoothing (assign a count of 1 to unseen n-grams; see Rule of succession) to more sophisticated models, such as Good–Turing discounting or back-off models.  Some of these methods are equivalent to assigning a prior distribution to the probabilities of the n-grams and using Bayesian inference to compute the resulting posterior n-gram probabilities.  However, the more sophisticated smoothing models were typically not derived in this fashion, but instead through independent considerations.

Linear interpolation (e.g., taking the weighted mean of the unigram, bigram, and trigram)
Good–Turing discounting
Witten–Bell discounting
Lidstone's smoothing
Katz's back-off model (trigram)
Kneser–Ney smoothing

Skip-gram
In the field of computational linguistics, in particular language modeling, skip-grams are a generalization of n-grams in which the components (typically words) need not be consecutive in the text under consideration, but may leave gaps that are skipped over. They provide one way of overcoming the data sparsity problem found with conventional n-gram analysis.
Formally, an n-gram is a consecutive subsequence of length n of some sequence of tokens w1 … wn. A k-skip-n-gram is a length-n subsequence where the components occur at distance at most k from each other.
For example, in the input text:

the rain in Spain falls mainly on the plainthe set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences

the in, rain Spain, in falls, Spain mainly, falls on, mainly the, and on plain.

Syntactic n-grams
Syntactic n-grams are n-grams defined by paths in syntactic dependency or constituent trees rather than the linear structure of the text. For example, the sentence "economic news has little effect on financial markets" can be transformed to syntactic n-grams following the tree structure of its dependency relations: news-economic, effect-little, effect-on-markets-financial.Syntactic n-grams are intended to reflect syntactic structure more faithfully than linear n-grams, and have many of the same applications, especially as features in a Vector Space Model. Syntactic n-grams for certain tasks gives better results than the use of standard n-grams, for example, for authorship attribution.Another type of syntactic n-grams are part-of-speech n-grams, defined as fixed-length contiguous overlapping subsequences that are extracted from part-of-speech sequences of text. Part-of-speech n-grams have several applications, most commonly in information retrieval.

See also
Collocation
Hidden Markov model
n-tuple
String kernel
MinHash
Feature extraction
Longest common substring problem
The RiTa Toolkit

References
Christopher D. Manning, Hinrich Schütze, Foundations of Statistical Natural Language Processing, MIT Press: 1999.  ISBN 0-262-13360-1.
White, Owen; Dunning, Ted; Sutton, Granger; Adams, Mark; Venter, J.Craig; Fields, Chris (1993). "A quality control algorithm for dna sequencing projects". Nucleic Acids Research. 21 (16): 3829–3838. doi:10.1093/nar/21.16.3829. PMC 309901. PMID 8367301.
Frederick J. Damerau, Markov Models and Linguistic Theory.  Mouton.  The Hague, 1971.
Figueroa, Alejandro; Atkinson, John (2012). "Contextual Language Models For Ranking Answers To Natural Language Definition Questions". Computational Intelligence. 28 (4): 528–548. doi:10.1111/j.1467-8640.2012.00426.x.
Brocardo, Marcelo Luiz; Issa Traore; Sherif Saad; Isaac Woungang (2013). Authorship Verification for Short Messages Using Stylometry (PDF). IEEE Intl. Conference on Computer, Information and Telecommunication Systems (CITS).

External links
Google's Google Book n-gram viewer and Web n-grams database (September 2006)
Microsoft's web n-grams service
STATOPERATOR N-grams Project Weighted n-gram viewer for every domain in Alexa Top 1M
1,000,000 most frequent 2,3,4,5-grams from the 425 million word Corpus of Contemporary American English
Peachnote's music ngram viewer
Stochastic Language Models (n-Gram) Specification (W3C)
Michael Collin's notes on n-Gram Language Models
OpenRefine: Clustering In Depth
Naive semantics is an approach used in computer science for representing basic knowledge about a specific domain, and has been used in applications such as the representation of the meaning of natural language sentences in artificial intelligence applications. In a general setting the term has been used to refer to the use of a limited store of generally understood knowledge about a specific domain in the world, and has been applied to fields such as the knowledge based design of data schemas.In natural language understanding, naive semantics involves the use of a lexical theory which maps each word sense to a simple theory (or set of assertions) about the objects or events of reference. In this sense, naive semantic theory is based upon a particular language, its syntax and its word senses. For instance the word "water" and the assertion water(X) may be associated with the three predicates clear(X), liquid(X) and tasteless(X).

References
Naive semantics for natural language understanding by Kathleen Dahlgren 1988 ISBN 0-89838-287-4


== Notes ==
In information extraction, a named entity is a real-world object, such as persons, locations, organizations, products, etc., that can be denoted with a proper name. It can be abstract or have a physical existence. Examples of named entities include Barack Obama, New York City, Volkswagen Golf, or anything else that can be named. Named entities can simply be viewed as entity instances (e.g., New York City is an instance of a city).
From a historical perspective, the term Named Entity was coined during the MUC-6 evaluation campaign and contained ENAMEX (entity name expressions e.g. persons, locations and organizations) and NUMEX (numerical expression).
A more formal definition can be derived from the rigid designator by Saul Kripke. In the expression "Named Entity", the word "Named" aims to restrict the possible set of entities to only those for which one or many rigid designators stands for the referent. A designator is rigid when it designates the same thing in every possible world. On the contrary, flaccid designators may designate different things in different possible worlds.
As an example, consider the sentence, "Trump is the president of the United States". Both "Trump" and the "United States" are named entities since they refer to specific objects (Donald Trump  and United States). However, "president" is not a named entity since it can be used to refer to many different objects in different worlds (in different presidential periods referring to different persons, or even in different countries or organizations referring to different people). Rigid designators usually include proper names as well as certain natural terms like biological species and substances.
There is also a general agreement in the Named Entity Recognition community to consider as named entities temporal and numerical expressions such as amounts of money and other types of units, which may violate the rigid designator perspective.
The task of recognizing named entities in text is Named Entity Recognition while the task of determining the identity of the named entities mentioned in text is called Named Entity Disambiguation. Both tasks require dedicated algorithms and resources to be addressed.

See also
Named-entity recognition (also referred to as entity identification, entity chunking and entity extraction)
Entity linking (also referred to as named entity linking (NEL), named entity disambiguation (NED), named entity recognition and disambiguation (NERD) or named entity normalization)
Information extraction
Knowledge extraction
Text mining (also referred to as text data mining)
Truecasing
Apache OpenNLP
spaCy
General Architecture for Text Engineering
Natural Language Toolkit


== References ==
Paco Nathan (born 1962) is an American computer scientist, author, and performance art show producer from San Luis Obispo, California, who established much of his career in Austin, Texas.

Early life
Nathan studied mathematics and computer science at Stanford University, specializing in user interface design and artificial intelligence, with Douglas Lenat as graduate advisor.He received a teaching fellowship during 1984-1986, under the direction of Stuart Reges, to create a course called CS1E, as a peer-teaching introduction to using the Internet, informally called "PCs for Poets". It has since grown to become the popular Residential Computing program on campus.

Career
Nathan collaborated with Robby Garner and the Italian researcher Luigi Caputo, President of Alma Research Centre, on one of the first web chatterbots, named Barry DeFacto, in 1995.The three have worked together on several related projects, including the JFRED open source project for developing Java-based chat bots. They used JFRED in BBC Television's "Tomorrow's World MegaLab Experiment" and attained a 17% Turing percentage during what was the largest online Turing test at the time.He was a co-founder (with Jon Lebkowsky) and president of FringeWare, Inc., and the editor of FringeWare Review. FringeWare, founded in 1992, was one of the early commercial sites on the Internet. It experimented with mixing subcultural analysis and ecommerce, hence the name "fringe" plus "ware". Through work at FringeWare in support of small press publishers and fringe subcultures, Nathan also helped produce a series of performance art shows during 1997-1999, including events for Robert Anton Wilson, Survival Research Laboratories,Church of the Subgenius, RTMark, and Negativland. FringeWare was later used as a pattern for part of the organization of the Viridian design movement.Nathan has written for several other publications including O'Reilly Net, Wired, Whole Earth Review, Mondo 2000, and was a contributing editor for Boing Boing during the early 1990s.His first article for Mondo 2000 about the IBVA brainwave interface system was credited as inspiration for the song Hi-Tech Hippies by Yellow Magic Orchestra.Other popular writings have included a parody (nEurorAncid) of the cyberpunk novel Neuromancer, and court-room reporting on behalf of a newly launched Wired during the federal trial of Steve Jackson Games v. US Secret Service.More recent work has focused on applying aspects of systems theory for computer network applications. Nathan led an engineering team at Symbiot to develop software for monitoring and visualizing risk metrics of complex network security systems. That work received an Apple Design Award in 2004, was cited as a source for the United Nations UNCTAD Information Economy Report in 2005, and spun off as an open source project called OpenSIMS. During his period at Symbiot, Nathan helped pioneer a controversial "hands on" college program in network security at Austin Community College, for which he received a NISOD Award for Teaching Excellence in 2003.Some of the technology at Symbiot emerged from an earlier project created by Nathan, called The Ceteri Institute, which used complex systems modeling to analyze aspects of multinational corporations. That effort followed from several years of writing, speaking, and political organizing on behalf of anti-corporate activism. During the period of 1999-2002, he summarized that material in a series of papers and lectures about "corporate metabolism".Nathan currently works as the technical director for HeadCase Humanufacturing, combining previous experience in chat bots and ecommerce.

See also
Tiffany Lee Brown
Wiley Wiggins

Notes and references
External links
Paco Nathan blog ("Hugo Junot" alias) ceteri.org
Spock profile spock.com
JFRED open source project mac.com
Austin SRL 1997 show archive srl.org
FringeWare content archives web.archive.org
OpenSIMS open source project sourceforge.net
HeadCase Humanufacturing home page headcaselabs.com
ClaimID verified web pages claimid.com
In neuropsychology, linguistics, and the philosophy of language, a natural language or ordinary language is any language that has evolved naturally in humans through use and repetition without conscious planning or premeditation. Natural languages can take different forms, such as speech or signing. They are distinguished from constructed and formal languages such as those used to program computers or to study logic.

Defining natural language
Though the exact definition varies between scholars, natural language can broadly be defined in contrast to artificial or constructed languages (such as computer programming languages and international auxiliary languages) and to other communication systems in nature. Examples of such communication systems include bees' waggle dance and whale song, to which researchers have found or applied the linguistic cognates of dialect and even syntax. However, classification of animal communication systems as languages is controversial.All language varieties of world languages are natural languages, although some varieties are subject to greater degrees of published prescriptivism or language regulation than others. Thus nonstandard dialects can be viewed as a wild type in comparison with standard languages. But even an official language with a regulating academy, such as Standard French with the French Academy, is classified as a natural language (for example, in the field of natural language processing), as its prescriptive points do not make it either constructed enough to be classified as a constructed language or controlled enough to be classified as a controlled natural language.

Controlled languages
Controlled natural languages are subsets of natural languages whose grammars and dictionaries have been restricted in order to reduce or eliminate both ambiguity and complexity (for instance, by cutting down on rarely used superlative or adverbial forms or irregular verbs). The purpose behind the development and implementation of a controlled natural language typically is to aid non-native speakers of a natural language in understanding it, or to ease computer processing of a natural language. An example of a widely used controlled natural language is Simplified English, which was originally developed for aerospace industry maintenance manuals.

Constructed languages and international auxiliary languages
Constructed international auxiliary languages such as Esperanto and Interlingua (even those that have native speakers) are not generally considered natural languages. Natural languages have been used to communicate and have evolved in a natural way, whereas Esperanto was designed by L. L. Zamenhof selecting elements from natural languages, not grown from natural fluctuations in vocabulary and syntax. Some natural languages have become naturally "standardized" by children's natural tendency to correct for illogical grammatical structures in their parents' speech, which can be seen in the development of pidgin languages into creole languages (as explained by Steven Pinker in The Language Instinct), but this is not the case in many languages, including constructed languages such as Esperanto, where strict rules are in place as an attempt to consciously remove such irregularities. The possible exception to this are true native speakers of such languages. More substantive basis for this designation is that the vocabulary, grammar, and orthography of Interlingua are natural; they have been standardized and presented by a linguistic research body, but they predated it and are not themselves considered a product of human invention. Most experts, however, consider Interlingua to be naturalistic rather than natural. Latino sine flexione, a second naturalistic auxiliary language, is also naturalistic in content but is no longer widely spoken.

See also
Language acquisition
Origin of language
Formal semantics of natural languages
Jargon

Notes


== References ==
Natural Language Engineering is a bimonthly peer-reviewed academic journal published by Cambridge University Press which covers research and software in natural language processing. Its aim is to "bridge the gap between traditional computational linguistics research and the implementation of practical applications with potential real-world use". Other than original publication on theoretical and applied aspects of computational linguistics, the journal also contains Industry Watch and Emerging Trends columns tracking developments in the field. The editor-in-chief is  Ruslan Mitkov from University of Wolverhampton. According to the Journal Citation Reports, the journal has a 2016 impact factor of 1.065.

References
External links
Official website
Native-language identification (NLI) is the task of determining an author's native language (L1) based only on their writings in a second language (L2). NLI works through identifying language-usage patterns that are common to specific L1 groups and then applying this knowledge to predict the native language of previously unseen texts. This is motivated in part by applications in second-language acquisition, language teaching and forensic linguistics, amongst others.

Overview
NLI works under the assumption that an author's L1 will dispose them towards particular language production patterns in their L2, as influenced by their native language. This relates to cross-linguistic influence (CLI), a key topic in the field of second-language acquisition (SLA) that analyzes transfer effects from the L1 on later learned languages.
Using large-scale English data, NLI methods achieve over 80% accuracy in predicting the native language of texts written by authors from 11 different L1 backgrounds. This can be compared to a baseline of 9% for choosing randomly.

Applications
Pedagogy and language transfer
This identification of L1-specific features has been used to study language transfer effects in second-language acquisition. This is useful for developing pedagogical material, teaching methods, L1-specific instructions and generating learner feedback that is tailored to their native language.

Forensic linguistics
NLI methods can also be applied in forensic linguistics as a method of performing authorship profiling in order to infer the attributes of an author, including their linguistic background.
This is particularly useful in situations where a text, e.g. an anonymous letter, is the key piece of evidence in an investigation and clues about the native language of a writer can help investigators in identifying the source.
This has already attracted interest and funding from intelligence agencies.

Methodology
Natural language processing methods are used to extract and identify language usage patterns common to speakers of an L1-group. This is done using language learner data, usually from a learner corpus. Next, machine learning is applied to train classifiers, like support vector machines, for predicting the L1 of unseen texts.
A range of ensemble based systems have also been applied to the task and shown to improve performance over single classifier systems.Various linguistic feature types have been applied for this task. These include syntactic features such as constituent parses, grammatical dependencies and part-of-speech tags.
Surface level lexical features such as character, word and lemma n-grams have also been found to be quite useful for this task. However, it seems that character n-grams are the single best feature for the task.

2013 shared task
The Building Educational Applications (BEA) workshop at NAACL 2013 hosted the inaugural NLI shared task. The competition resulted in 29 entries from teams across the globe, 24 of which also published a paper describing their systems and approaches.

See also


== References ==
The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language. It was developed by Steven Bird and Edward Loper in the Department of Computer and Information Science at the University of Pennsylvania. NLTK includes graphical demonstrations and sample data. It is accompanied by a book that explains the underlying concepts behind the language processing tasks supported by the toolkit, plus a cookbook.NLTK is intended to support research and teaching in NLP or closely related areas, including empirical linguistics, cognitive science, artificial intelligence, information retrieval, and machine learning.
NLTK has been used successfully as a teaching tool, as an individual study tool, and as a platform for prototyping and building research systems. There are 32 universities in the US and 25 countries using NLTK in their courses. NLTK supports classification, tokenization, stemming, tagging, parsing, and semantic reasoning functionalities.

Library highlights
Lexical analysis: Word and text tokenizer
n-gram and collocations
Part-of-speech tagger
Tree model and Text chunker for capturing
Named-entity recognition

See also
SpaCy

References
External links
Official website
Natural-language understanding (NLU) or natural-language interpretation (NLI) is a subtopic  of natural-language processing in artificial intelligence that deals with machine reading comprehension. Natural-language understanding is considered an AI-hard problem.There is considerable commercial interest in the field because of its application to automated reasoning, machine translation, question answering, news-gathering, text categorization, voice-activation, archiving, and large-scale content analysis.

History
The program STUDENT, written in 1964 by Daniel Bobrow for his PhD dissertation at MIT is one of the earliest known attempts at natural-language understanding by a computer.  Eight years after John McCarthy coined the term artificial intelligence, Bobrow's dissertation (titled Natural Language Input for a Computer Problem Solving System) showed how a computer could understand simple natural language input to solve algebra word problems.
A year later, in 1965, Joseph Weizenbaum at MIT wrote ELIZA, an interactive program that carried on a dialogue in English on any topic, the most popular being psychotherapy. ELIZA worked by simple parsing and substitution of key words into canned phrases and Weizenbaum sidestepped the problem of giving the program a database of real-world knowledge or a rich lexicon. Yet ELIZA gained surprising popularity as a toy project and can be seen as a very early precursor to current commercial systems such as those used by Ask.com.In 1969 Roger Schank at Stanford University introduced the conceptual dependency theory for natural-language understanding. This model, partially influenced by the work of Sydney Lamb, was extensively used by Schank's students at Yale University, such as Robert Wilensky, Wendy Lehnert, and Janet Kolodner.
In 1970, William A. Woods introduced the augmented transition network (ATN) to represent natural language input. Instead of phrase structure rules ATNs used an equivalent set of finite state automata that were called recursively. ATNs and their more general format called "generalized ATNs" continued to be used for a number of years.
In 1971 Terry Winograd finished writing SHRDLU for his PhD thesis at MIT. SHRDLU could understand simple English sentences in a restricted world of children's blocks to direct a robotic arm to move items. The successful demonstration of SHRDLU provided significant momentum for continued research in the field. Winograd continued to be a major influence in the field with the publication of his book Language as a Cognitive Process. At Stanford, Winograd would later be the adviser for Larry Page, who co-founded Google.
In the 1970s and 1980s the natural language processing group at SRI International continued research and development in the field. A number of commercial efforts based on the research were undertaken, e.g., in 1982 Gary Hendrix formed Symantec Corporation originally as a company for developing a natural language interface for database queries on personal computers. However, with the advent of mouse driven, graphic user interfaces Symantec changed direction. A number of other commercial efforts were started around the same time, e.g., Larry R. Harris at the Artificial Intelligence Corporation and Roger Schank and his students at Cognitive Systems corp. In 1983, Michael Dyer developed the BORIS system at Yale which bore similarities to the work of Roger Schank and W. G. Lehnert.The third millennium saw the introduction of systems using machine learning for text classification, such as the IBM Watson. However, it is debated how much "understanding" such systems demonstrate, e.g. according to John Searle, Watson did not even understand the questions.John Ball, cognitive scientist and inventor of Patom Theory supports this assessment. Natural language processing has made inroads for applications to support human productivity in service and ecommerce but this has largely been made possible by narrowing the scope of the application. There are thousands of ways to request something in a human language which still defies conventional natural language processing. "To have a meaningful conversation with machines is only possible when we match every word to the correct meaning based on the meanings of the other words in the sentence – just like a 3-year-old does without guesswork" Patom Theory

Scope and context
The umbrella term "natural-language understanding" can be applied to a diverse set of computer applications, ranging from small, relatively simple tasks such as short commands issued to robots, to highly complex endeavors such as the full comprehension of newspaper articles or poetry passages. Many real world applications fall between the two extremes, for instance text classification for the automatic analysis of emails and their routing to a suitable department in a corporation does not require in depth understanding of the text, but needs to deal with a much larger vocabulary and more diverse syntax than the management of simple queries to database tables with fixed schemata.
Throughout the years various attempts at processing natural language or English-like sentences presented to computers have taken place at varying degrees of complexity. Some attempts have not resulted in systems with deep understanding, but have helped overall system usability. For example, Wayne Ratliff originally developed the Vulcan program with an English-like syntax to mimic the English speaking computer in Star Trek. Vulcan later became the dBase system whose easy-to-use syntax effectively launched the personal computer database industry. Systems with an easy to use or English like syntax are, however, quite distinct from systems that use a rich lexicon and include an internal representation (often as first order logic) of the semantics of natural language sentences.
Hence the breadth and depth of "understanding" aimed at by a system determine both the complexity of the system (and the implied challenges) and the types of applications it can deal with.  The "breadth" of a system is measured by the sizes of its vocabulary and grammar.  The "depth" is measured by the degree to which its understanding approximates that of a fluent native speaker.  At the narrowest and shallowest, English-like command interpreters require minimal complexity, but have a small range of applications.  Narrow but deep systems explore and model mechanisms of understanding, but they still have limited application.  Systems that attempt to understand the contents of a document such as a news release beyond simple keyword matching and to judge its suitability for a user are broader and require significant complexity, but they are still somewhat shallow.  Systems that are both very broad and very deep are beyond the current state of the art.

Components and architecture
Regardless of the approach used, most natural-language-understanding systems share some common components. The system needs a lexicon of the language and a parser and grammar rules to break sentences into an internal representation. The construction of a rich lexicon with a suitable ontology requires significant effort, e.g., the Wordnet lexicon required many person-years of effort.The system also needs theory from semantics to guide the comprehension. The interpretation capabilities of a language-understanding system depend on the semantic theory it uses. Competing semantic theories of language have specific trade-offs in their suitability as the basis of computer-automated semantic interpretation. These range from naive semantics or stochastic semantic analysis to the use of pragmatics to derive meaning from context. Semantic parsers convert natural-language texts into formal meaning representations.Advanced applications of natural-language understanding also attempt to incorporate logical inference within their framework. This is generally achieved by mapping the derived meaning into a set of assertions in predicate logic, then using logical deduction to arrive at conclusions. Therefore, systems based on functional languages such as Lisp need to include a subsystem to represent logical assertions, while logic-oriented systems such as those using the language Prolog generally rely on an extension of the built-in logical representation framework.The management of context in natural-language understanding can present special challenges. A large variety of examples and counter examples have resulted in multiple approaches to the formal modeling of context, each with specific strengths and weaknesses.

See also
Computational semantics
Computational linguistics
Discourse representation theory
Deep linguistic processing
History of natural language processing
Information extraction
Mathematica
Natural language programming
Natural language user interface
Siri (software)
Wolfram Alpha
Open information extraction
Part-of-speech tagging
Speech recognition


== Notes ==
Natural-language user interface (LUI or NLUI) is a type of computer human interface where linguistic phenomena such as verbs, phrases and clauses act as UI controls for creating, selecting and modifying data in software applications.
In interface design, natural-language interfaces are sought after for their speed and ease of use, but most suffer the challenges to understanding wide varieties of ambiguous input.
Natural-language interfaces are an active area of study in the field of natural-language processing and computational linguistics. An intuitive general natural-language interface is one of the active goals of the Semantic Web.
Text interfaces are "natural" to varying degrees. Many formal (un-natural) programming languages incorporate idioms of natural human language. Likewise, a traditional keyword search engine could be described as a "shallow" natural-language user interface.

Overview
A natural-language search engine would in theory find targeted answers to user questions (as opposed to keyword search). For example, when confronted with a question of the form 'which U.S. state has the highest income tax?', conventional search engines ignore the question and instead search on the keywords 'state', 'income' and 'tax'. Natural-language search, on the other hand, attempts to use natural-language processing to understand the nature of the question and then to search and return a subset of the web that contains the answer to the question. If it works, results would have a higher relevance than results from a keyword search engine.

History
Prototype Nl interfaces had already appeared in the late sixties and early seventies.
SHRDLU, a natural-language interface that manipulates blocks in a virtual "blocks world"
Lunar, a natural-language interface to a database containing chemical analyses of Apollo-11 moon rocks by William A. Woods.
Chat-80 transformed English questions into Prolog expressions, which were evaluated against the Prolog database.  The code of Chat-80 was circulated widely, and formed the basis of several other experimental Nl interfaces. An online demo is available on the LPA website.
ELIZA, written at MIT by Joseph Weizenbaum between 1964 and 1966, mimicked a psychotherapist and was operated by processing users' responses to scripts. Using almost no information about human thought or emotion, the DOCTOR script sometimes provided a startlingly human-like interaction. An online demo is available on the LPA website.
Janus is also one of the few systems to support temporal questions.
Intellect from Trinzic (formed by the merger of AICorp and Aion).
BBN’s Parlance built on experience from the development of the Rus  and Irus systems.
IBM Languageaccess
Q&A from Symantec.
Datatalker from Natural Language Inc.
Loqui  from BIM Systems.
English Wizard from Linguistic Technology Corporation.
iAskWeb from Anserity Inc. fully implemented in Prolog was providing interactive recommendations in NL to users in tax and investment domains in 1999-2001

Challenges
Natural-language interfaces have in the past led users to anthropomorphize the computer, or at least to attribute more intelligence to machines than is warranted. On the part of the user, this has led to unrealistic expectations of the capabilities of the system. Such expectations will make it difficult to learn the restrictions of the system if users attribute too much capability to it, and will ultimately lead to disappointment when the system fails to perform as expected as was the case in the AI winter of the 1970s and 80s.
A 1995 paper titled 'Natural Language Interfaces to Databases – An Introduction', describes some challenges:
Modifier attachment
The request "List all employees in the company with a driving licence" is ambiguous unless you know that companies can't have driving licences.
Conjunction and disjunction
"List all applicants who live in California and Arizona" is ambiguous unless you know that a person can't live in two places at once.
Anaphora resolution
resolve what a user means by 'he', 'she' or 'it', in a self-referential query.Other goals to consider more generally are the speed and efficiency of the interface, in all algorithms these two points are the main point that will determine if some methods are better than others and therefore have greater success in the market. In addition, localisation across multiple language sites requires extra consideration - this is based on differing sentence structure and language syntax variations between most languages.
Finally, regarding the methods used, the main problem to be solved is creating a general algorithm that can recognize the entire spectrum of different voices, while disregarding nationality, gender or age. The significant differences between the extracted features - even from speakers who says the same word or phrase - must be successfully overcome.

Uses and applications
The natural-language interface gives rise to technology used for many different applications.
Some of the main uses are:

Dictation, is the most common use for automated speech recognition (ASR) systems today. This includes medical transcriptions, legal and business dictation, and general word processing. In some cases special vocabularies are used to increase the accuracy of the system.
Command and control, ASR systems that are designed to perform functions and actions on the system are defined as command and control systems. Utterances like "Open Netscape" and "Start a new xterm" will do just that.
Telephony, some PBX/Voice Mail systems allow callers to speak commands instead of pressing buttons to send specific tones.
Wearables, because inputs are limited for wearable devices, speaking is a natural possibility.
Medical, disabilities, many people have difficulty typing due to physical limitations such as repetitive strain injuries (RSI), muscular dystrophy, and many others. For example, people with difficulty hearing could use a system connected to their telephone to convert a caller's speech to text.
Embedded applications, some new cellular phones include C&C speech recognition that allow utterances such as "call home". This may be a major factor in the future of automatic speech recognition and Linux.
Software development: An integrated development environment can embed natural-language interfaces to help developers.Below are named and defined some of the applications that use natural-language recognition, and so have integrated utilities listed above.

Ubiquity
Ubiquity, an add-on for Mozilla Firefox, is a collection of quick and easy natural-language-derived commands that act as mashups of web services, thus allowing users to get information and relate it to current and other webpages.

Wolfram Alpha
Wolfram Alpha is an online service that answers factual queries directly by computing the answer from structured data, rather than providing a list of documents or web pages that might contain the answer as a search engine would. It was announced in March 2009 by Stephen Wolfram, and was released to the public on May 15, 2009.

Siri
Siri is an intelligent personal assistant application integrated with operating system iOS. The application uses natural language processing to answer questions and make recommendations.
Siri's marketing claims include that it adapts to a user's individual preferences over time and personalizes results, and performs tasks such as making dinner reservations while trying to catch a cab.

Others
Ask.com – The original idea behind Ask Jeeves (Ask.com) was traditional keyword searching with an ability to get answers to questions posed in everyday, natural language. The current Ask.com still supports this, with added support for math, dictionary, and conversion questions.
Braina – Braina is a natural language interface for Windows OS that allows to type or speak English language sentences to perform a certain action or find information.
GNOME Do – Allows for quick finding miscellaneous artifacts of GNOME environment (applications, Evolution and Pidgin contacts, Firefox bookmarks, Rhythmbox artists and albums, and so on) and execute the basic actions on them (launch, open, email, chat, play, etc.).
hakia – hakia was an Internet search engine. The company invented an alternative new infrastructure to indexing that used SemanticRank algorithm, a solution mix from the disciplines of ontological semantics, fuzzy logic, computational linguistics, and mathematics. hakia closed in 2014.
Lexxe – Lexxe was an Internet search engine that used natural-language processing for queries (semantic search). Searches could be made with keywords, phrases, and questions, such as "How old is Wikipedia?" Lexxe closed its search engine services in 2015.
Pikimal – Pikimal used natural-language tied to user preference to make search recommendations by template. Pikimal closed in 2015.
Powerset – On May 11, 2008, the company unveiled a tool for searching a fixed subset of Wikipedia using conversational phrases rather than keywords. On July 1, 2008, it was purchased by Microsoft.
Q-go – The Q-go technology provides relevant answers to users in response to queries on a company’s internet website or corporate intranet, formulated in natural sentences or keyword input alike. Q-go was acquired by RightNow Technologies in 2011.
Yebol – Yebol is a vertical "decision" search engine that had developed a knowledge-based, semantic search platform. Yebol's artificial intelligence human intelligence-infused algorithms automatically cluster and categorize search results, web sites, pages and content that it presents in a visually indexed format that is more aligned with initial human intent. Yebol uses association, ranking and clustering algorithms to analyze related keywords or web pages. Yebol integrates natural-language processing, metasynthetic-engineered open complex systems, and machine algorithms with human knowledge for each query to establish a web directory that actually 'learns', using correlation, clustering and classification algorithms to automatically generate the knowledge query, which is retained and regenerated forward.

See also
Conversational user interface
Natural user interface
Natural-language programming
Voice user interface
Chatterbot, a computer program that simulates human conversations
Noisy text
Question answering
Selection-based search
Semantic search
Semantic query
Semantic Web


== References ==
Natural-language programming (NLP) is an ontology-assisted way of programming in terms of natural-language sentences, e.g. English. A structured document with Content, sections and subsections for explanations of sentences forms a NLP document, which is actually a computer program. Natural languages and natural-language user interfaces include Inform7, a natural programming language for making interactive fiction, Ring, a general-purpose language, Shakespeare, an esoteric natural programming language in the style of the plays of William Shakespeare, and Wolfram Alpha, a computational knowledge engine, using natural-language input. Some methods for program synthesis are based on natural-language programming.

Interpretation
The smallest unit of statement in NLP is a sentence.  Each sentence is stated in terms of concepts from the underlying ontology, attributes in that ontology and named objects in capital letters.  In an NLP text every sentence unambiguously compiles into a procedure call in the underlying high-level programming language such as MATLAB, Octave, SciLab, Python, etc.
Symbolic languages such as Wolfram Language are capable of interpreted processing of queries by sentences. This can allow interactive requests such as that implemented in Wolfram Alpha. The difference between these and NLP is that the latter builds up a single program or a library of routines that are programmed through natural language sentences using an ontology that defines the available data structures in a high level programming language.
An example text from an English language natural-language program is as follows:

 If  U_ is 'smc01-control',  then do the following. Define surface weights Alpha as "[0.5, 0.5]".
 Initialise matrix Phi as a 'unit matrix'. Define J as the 'inertia matrix' of Spc01. Compute
 matrix J2 as the inverse of J.  Compute position velocity error Ve and angular velocity error
 Oe from dynamical state X, guidance reference Xnow.   Define the joint sliding surface G2
 from the position velocity error Ve and angular velocity error Oe using the surface weights
 Alpha. Compute the smoothed sign function SG2 from the joint sliding surface G2 with sign
 threshold 0.01.  Compute special dynamical force F from dynamical state X and surface
 weights Alpha.  Compute control torque T and control force U from matrix J2, surface weights
 Alpha, special dynamical force F, smoothed sign function SG2.  Finish conditional actions.

that defines a feedback control scheme using a sliding mode control method.

Software paradigm
Natural-language programming is a top-down method of writing software. Its stages are as follows:

Definition of an ontology –  taxonomy –  of concepts needed to describe tasks in the topic addressed. Each concept and all their attributes are defined in natural-language words. This ontology will define the data structures the NLP can use in sentences.
Definition of one or more top-level sentences in terms of concepts from the ontology. These sentences are later used to invoke the most important activities in the topic.
Defining of each of the top-level sentences in terms of a sequence of sentences.
Defining each of the lower-level sentences in terms of other sentences or by a simple sentence of the form  Execute code "...".  where ... stands for a code in terms of the associated high-level programming language.
Repeating the previous step until you have no sentences left undefined. During this process each of sentences can be classified to belong to a section of the document to be produced in HTML or Latex format to form the final natural-language program.
Testing the meaning of each sentence by executing its code using testing objects.
Providing a library of procedure calls (in the underlying high-level language) which are needed in the code definitions of some low-level-sentence meanings.
Providing a title, author data and compiling the sentences into an HTML or LaTex file.
Publishing the natural-language program as a webpage on the Internet or as a PDF file compiled from the LaTex document.

Publication value of natural-language programs and documents
A natural-language program is a precise formal description of some procedure that its author created. It is human readable and it can also be read by a suitable software agent.  For example, a web page in an NLP format can be read by a software personal assistant agent to a person and she or he can ask the agent to execute some sentences, i.e. carry out some task or answer a question. There is a reader agent available for English interpretation of HTML based NLP documents that a person can run on her personal computer .

Contribution of natural-language programs to machine knowledge
An ontology class in a natural-language program that is not a concept in the sense as humans use concepts. Concepts in an NLP are examples (samples) of generic human concepts. Each sentence in a natural-language program is either (1) stating a relationship in a world model or (2) carries out an action in the environment or (3) carries out a computational procedure or (4) invokes an answering mechanism in response to a question.
A set of NLP sentences, with associated ontology defined, can also be used as a pseudo code that does not provide the details in any underlying high level programming language. In such an application the sentences used become high level abstractions (conceptualisations) of computing procedures that are computer language and machine independent.

See also
Controlled natural language
Context-free language
Domain-specific language (or DSL)
End-user programming
Knowledge representation
Natural-language processing
Source-code generation
Very high-level programming languageProgramming languages with English-like syntax
AppleScript
Attempto Controlled English
COBOL
ClearTalk
FLOW-MATIC
HyperTalk
Inform 7
JOSS
Software AG
Transcript
Structured Query Language (or SQL)
xTalk

References
Bibliography
Books
Natural Language Programming of Agents and Robotic Devices:  publishing for agents and humans in sEnglish by S M Veres, ISBN 978-0-9558417-0-5, London, June 2008.
Dijkstra, Edsger W. (1979). "On the foolishness of "natural language programming"". Program Construction. Lecture Notes in Computer Science. 69. pp. 51–53. doi:10.1007/bfb0014656. ISBN 3-540-09251-X.
Lieberman, Henry; Liu, Hugo (2006). "Feasibility Studies for Programming in Natural Language". End User Development. Human-Computer Interaction Series. 9. pp. 459–473. doi:10.1007/1-4020-5386-x_20. ISBN 978-1-4020-4220-1.
Halpern, Mark (1990). "Natural Language and Redundancy in Programming". Binding Time: Six Studies in Programming Technology & Milieu. Ablex series in computational science. Intellect Books. ISBN 9780893916916.CS1 maint: ref=harv (link)Papers at conferences
Veres, S.M.; Molnar, L. (2010). "Documents for Intelligent Agents in English". Artificial Intelligence and Applications. doi:10.2316/p.2010.674-122. ISBN 978-0-88986-817-5.
Sliding mode control of autonomous spacecraft. (half written in sEnglish) by S M Veres an N K Lincoln, Proc. TAROS’2008, Towards Autonomous Robotic Systems, Edinburgh, 1–3 September 2008.
Veres, Sandor M. (2010). "Mission Capable Autonomous Control Systems in the Oceans, in the Air and in Space". Brain-Inspired Information Technology. Studies in Computational Intelligence. 266. pp. 1–10. doi:10.1007/978-3-642-04025-2_1. ISBN 978-3-642-04024-5.
Programming Spatial Algorithms in Natural Language, by Boris Galitsky, Daniel Usikov, in the AAAI Workshop on Spatial and Temporal Reasoning 2008, AAAI Technical report, https://www.aaai.org/Library/Workshops/ws08-11.php.
Pulido-Prieto, Oscar; Juárez-Martínez, Ulises (2017). "A Survey of Naturalistic Programming Technologies". ACM Computing Surveys. 50 (5): 1–35. doi:10.1145/3109481.Program synthesis from natural language specificationsRaza, Mohammad, Sumit Gulwani, and Natasa Milic-Frayling. "Compositional Program Synthesis from Natural Language and Examples." IJCAI. 2015.
Green, Cordell. "A Summary of the PSI Program Synthesis System." IJCAI. Vol. 5. 1977.

External links
English Script
SEMPRE
sEnglish –  explaining "system English"
sysbrain.org
How natural should a natural interface be? –  thoughts on how "natural" the Ubiquity interface should attempt to be
Tool turns English to code
Computer knowledge representation format, system, methods, and applications –  US patent
NetOwl is a suite of multilingual text and identity analytics products that analyze big data in the form of text data – reports, web, social media, etc. – as well as structured entity data about people, organizations, places, and things.
NetOwl utilizes artificial intelligence (AI)-based approaches, including natural language processing (NLP), machine learning (ML), and computational linguistics, to extract entities, relationships, and events; to perform sentiment analysis; to assign latitude/longitude to geographical references in text; to translate names written in foreign languages; and to perform name matching and identity resolution.
NetOwl's uses include semantic search and discovery, geospatial analysis, intelligence analysis, content enrichment, compliance monitoring, cyber threat monitoring, risk management, and bioinformatics.

Products
The NetOwl suite includes, among others, the following text and entity analytics products:

Text Analytics
NetOwl Extractor performs entity extraction from unstructured texts using natural language processing (NLP), machine learning (ML), and computational linguistics.  Extractor also performs semantic relationship and event extraction as well as geotagging of text. It is used for a variety of data sources including both traditional sources (e.g., news, reports, web pages, email) and social media (e.g., Twitter, Facebook, chats, blogs). It runs on a variety of Big Data analytics platforms, including Apache Hadoop and LexisNexis’s High-Performance Computer Cluster (HPCC) technology. It has been integrated with a number of 3rd party analytical tools such as Esri ArcGIS and Google Earth/Maps.

Identity Analytics
NetOwl NameMatcher and EntityMatcher perform name matching and identity resolution for large multicultural and multilingual entity databases using machine learning (ML) and computational linguistics approaches.  They are used for applications such as anti-money laundering (AML), watch lists, regulatory compliance, fraud detection, etc.

History
The first NetOwl product was NetOwl Extractor, which was initially released in 1996.  Since then, Extractor has added many new capabilities, including relationship and event extraction, categorization, name translation, geotagging, and sentiment analysis, as well as entity extraction in other languages.  Other products were added later to the NetOwl suite, namely TextMiner, NameMatcher, and EntityMatcher.
NetOwl has participated in several 3rd party-sponsored text and entity analytics software benchmarking events. NetOwl Extractor was the top-scoring named entity extraction system at the DARPA-sponsored Message Understanding Conference MUC-6 and the top-scoring link and event extraction system in MUC-7. It was also the top-scoring system at several of the NIST-sponsored Automatic Content Extraction (ACE) evaluation tasks.  NetOwl NameMatcher was the top-scoring system at the MITRE Challenge for Multicultural Person Name Matching.

References
External links
NetOwl website

See also
Knowledge extraction
Text mining
Data Mining
Computational linguistics
Named entity recognition
Unstructured data
Document classification
News analysis refers to the measurement of the various qualitative and quantitative attributes of textual (unstructured data) news stories. Some of these attributes are: sentiment, relevance, and novelty. Expressing news stories as numbers and metadata permits the manipulation of everyday information in a mathematical and statistical way. This data is often used in financial markets as part of a trading strategy or by businesses to judge market sentiment and make better business decisions.
News analytics are usually derived through automated text analysis and applied to digital texts using elements from natural language processing and machine learning such as latent semantic analysis, support vector machines, "bag of words" among other techniques.

Applications and strategies
The application of sophisticated linguistic analysis to news and social media has grown from an area of research to mature product solutions since 2007. News analytics and news sentiment calculations are now routinely used by both buy-side and sell side in alpha generation, trading execution, risk management, and market surveillance and compliance. There is however a good deal of variation in the quality, effectiveness and completeness of currently available solutions.
A large number of companies use news analysis to help them make better business decisions. Academic researchers have become interested in news analysis especially with regards to predicting stock price movements, volatility and traded volume.  Provided a set of values such as sentiment and relevance as well as the frequency of news arrivals, it is possible to construct news sentiment scores  for multiple asset classes such as equities, Forex, fixed income, and commodities. Sentiment scores can be constructed at various horizons to meet the different needs and objectives of high and low frequency trading strategies, whilst characteristics such as direction and volatility of asset returns as well as the traded volume may be addressed more directly via the construction of tailor-made sentiment scores. Scores are generally constructed as a range of values. For instance, values may range between 0 and 100, where values above and below 50 convey positive and negative sentiment, respectively. Based on such sentiment scores, it should be possible to generate a set of strategies useful for instance within investing, hedging, and order execution.

Absolute return strategies
The objective of absolute return strategies is absolute (positive) returns regardless of the direction of the financial market. To meet this objective, such strategies typically involve opportunistic long and short positions in selected instruments with zero or limited market exposure. In statistical terms, absolute return strategies should have very low correlation with the market return. Typically, hedge funds tend to employ absolute return strategies. Below, a few examples show how news analysis can be applied in the absolute return strategy space with the purpose to identify alpha opportunities applying a market neutral strategy or  based on volatility trading.
Example 1
Scenario: The gap between the news sentiment scores for direction, 
  
    
      
        S
      
    
    {\displaystyle S}
  , of Company 
  
    
      
        X
      
    
    {\displaystyle X}
   and Market 
  
    
      
        Y
      
    
    {\displaystyle Y}
   has moved beyond 
  
    
      
        +
        20
      
    
    {\displaystyle +20}
  . That is, 
  
    
      
        
          S
          
            X
          
        
        −
        
          S
          
            Y
          
        
      
    
    {\displaystyle S_{X}-S_{Y}}
   ≥ 
  
    
      
        20
      
    
    {\displaystyle 20}
  .
Action: Buy the stock on Company 
  
    
      
        X
      
    
    {\displaystyle X}
   and short the future on Market 
  
    
      
        Y
      
    
    {\displaystyle Y}
  .
Exit Strategy: When the gap in the news sentiment scores for direction of Company 
  
    
      
        X
      
    
    {\displaystyle X}
   and Market 
  
    
      
        Y
      
    
    {\displaystyle Y}
   has disappeared, 
  
    
      
        
          S
          
            X
          
        
        −
        
          S
          
            Y
          
        
      
    
    {\displaystyle S_{X}-S_{Y}}
   = 
  
    
      
        0
      
    
    {\displaystyle 0}
  , sell the stock on Company 
  
    
      
        X
      
    
    {\displaystyle X}
   and go long the future on Market 
  
    
      
        Y
      
    
    {\displaystyle Y}
   to close the positions.
Example 2
Scenario: The news sentiment score for volatility of Company 
  
    
      
        X
      
    
    {\displaystyle X}
   goes above 
  
    
      
        70
      
    
    {\displaystyle 70}
   out of 
  
    
      
        100
      
    
    {\displaystyle 100}
   indicating an expected volatility above the option implied volatility.
Action: Buy a short-dated straddle (the purchase of both a put and a call) on the stock of Company 

  
    
      
        X
      
    
    {\displaystyle X}
  .
Exit Strategy: Keep the straddle on Company 
  
    
      
        X
      
    
    {\displaystyle X}
   until expiry or until a certain profit target has been reached.

Relative return strategies
The objective of relative return strategies is to either replicate (passive management) or outperform (active management) a theoretical passive reference portfolio or benchmark. To meet these objectives such strategies typically involve long positions in selected instruments. In statistical terms, relative return strategies often have high correlation with the market return. Typically, mutual funds tend to employ relative return strategies. Below, a few examples show how news analysis can be applied in the relative return strategy space with the purpose to outperform the market applying a stock picking strategy and by making tactical tilts to ones asset allocation model.
Example 1
Scenario: The news sentiment score for direction of Company 
  
    
      
        X
      
    
    {\displaystyle X}
   goes above 
  
    
      
        70
      
    
    {\displaystyle 70}
   out of 
  
    
      
        100
      
    
    {\displaystyle 100}
  .
Action: Buy the stock on Company 
  
    
      
        X
      
    
    {\displaystyle X}
  .
Exit Strategy: When the news sentiment score for direction of Company 
  
    
      
        X
      
    
    {\displaystyle X}
   falls below 
  
    
      
        60
      
    
    {\displaystyle 60}
  , sell the stock on Company 
  
    
      
        X
      
    
    {\displaystyle X}
   to close the position.
Example 2
Scenario: The news sentiment score for direction of Sector 
  
    
      
        Z
      
    
    {\displaystyle Z}
   goes above 
  
    
      
        70
      
    
    {\displaystyle 70}
   out of 
  
    
      
        100
      
    
    {\displaystyle 100}
  .
Action: Include Sector 
  
    
      
        Z
      
    
    {\displaystyle Z}
   as a tactical bet in the asset allocation model.
Exit Strategy: When the news sentiment score for direction of Sector 
  
    
      
        Z
      
    
    {\displaystyle Z}
   falls below 
  
    
      
        60
      
    
    {\displaystyle 60}
  , remove the tactical bet for Sector 
  
    
      
        Z
      
    
    {\displaystyle Z}
   from the asset allocation model.

Financial risk management
The objective of financial risk management is to create economic value in a firm or to maintain a certain risk profile of an investment portfolio by using financial instruments to manage risk exposures, particularly credit risk and market risk. Other types include Foreign exchange, Shape, Volatility, Sector, Liquidity, Inflation risks, etc. As a specialization of risk management, financial risk management focuses on when and how to hedge using financial instruments to manage costly exposures to risk. Below, a few examples show how news analysis can be applied in the financial risk management space with the purpose to either arrive at better risk estimates in terms of Value at Risk (VaR) or to manage the risk of a portfolio to meet ones portfolio mandate.
Example 1
Scenario: The bank operates a VaR model to manage the overall market risk of its portfolio.
Action: Estimate the portfolio covariance matrix taking into account the development of the news sentiment score for volume. Implement the relevant hedges to bring the VaR of the bank in line with the desired levels.
Example 2
Scenario: A portfolio manager operates his portfolio towards a certain desired risk profile.
Action: Estimate the portfolio covariance matrix taking into account the development of the news sentiment score for volume. Scale the portfolio exposure according to the targeted risk profile.

Computer algorithms using news analytics
Within 0.33 seconds, computer algorithms using news analytics can notify subscribers

which company the news is about,
if the news article sentiment is positive or negative,
if the news is ranked as high or low relative importance … relative relevance.
the stock price reaction and the increase in trade volume is concentrated in the first 5 seconds after an news article is released.

Algorithmic order execution
The objective of algorithmic order execution, which is part of the concept of algorithmic trading, is to reduce trading costs by optimizing on the timing of a given order.  It is widely used by hedge funds, pension funds, mutual funds, and other institutional traders to divide up large trades into several smaller trades to manage market impact, opportunity cost, and risk more effectively. The example below shows how news analysis can be applied in the algorithmic order execution space with the purpose to arrive at more efficient algorithmic trading systems.
Example 1
Scenario: A large order needs to be placed in the market for the stock on Company 
  
    
      
        X
      
    
    {\displaystyle X}
  .
Action: Scale the daily volume distribution for Company 
  
    
      
        X
      
    
    {\displaystyle X}
   applied in the algorithmic trading system, thus taking into account the news sentiment score for volume. This is followed by the creation of the desired trading distribution forcing greater market participation during the periods of the day when volume is expected to be heaviest.

Effects
Being able to express news stories as numbers permits the manipulation of everyday information in a statistical way that allows computers not only to make decisions once made only by humans, but to do so more efficiently. Since market participants are always looking for an edge, the speed of computer connections and the delivery of news analysis, measured in milliseconds, have become essential.

See also
Computational linguistics
Sentiment analysis
Text mining
Trading the news
Unstructured data
Natural language processing
Information asymmetry
Algorithmic trading


== References ==
Niki is an artificial intelligence company headquartered in Bangalore, Karnataka. It was founded in May 2015 by IIT Kharagpur graduates Sachin Jaiswal, Keshav Prawasi, Shishir Modi and Nitin Babel.
The Niki android app was launched for a limited beta in June, 2015, then released for public during YourStory's TechSparks 2015, and is a Tech30 company. The company raised an undisclosed amount in seed funding from Unilazer Ventures, a Mumbai-based VC firm founded by Ronnie Screwvala, in October 2015. This was followed by another seed funding round by Ratan Tata in May 2016. The company then raised US$2 million in Series A round of funding from SAP.iO, existing investors and some US and Germany-based investors, among others.

Product
The product is an artificial intelligence-powered chatbot which works as an intelligent personal assistant, named Niki. Leveraging natural language processing and machine learning, Niki presents a chat-based natural language user interface to the users where they can interact with Niki in their natural language. Niki understands how users chat in India, deciphers the words, in the context of product/services that they would like to purchase, and comes up with apt recommendations.
Initially it was only available on the Android platform as a mobile app. The company has expanded its operations to the Facebook messenger and Apple iOS platforms and has introduced a better user experience and natural language processing technology. The company aims to soon be present on more messaging platforms like Slack and WhatsApp.
The company currently provides 20+ services to over 2 million consumers, covering a wide spectrum ranging from utility services like mobile recharge, bill payments, travel services like cabs, buses, hotels and entertainment services like movies and events. Services such as flights and healthcare are also planned.

Partnerships
In September 2017, Infosys Finacle joined with Niki.ai to provide chat-based service to banking customers.In August 2017, Niki partnered with LazyPay to enable a 'buy now, pay later' feature for its users.

References
External links
Official website 
Blog
Medium
Noisy text analytics is a process of information extraction whose goal is to automatically extract structured or semistructured information from noisy unstructured text data. While Text analytics is a growing and mature field that has great value because of the huge amounts of data being produced, processing of noisy text is gaining in importance because a lot of common applications produce noisy text data. Noisy unstructured text data is found in informal settings such as online chat, text messages, e-mails, message boards, newsgroups, blogs, wikis and web pages. Also, text produced by processing spontaneous speech using automatic speech recognition and printed or handwritten text using optical character recognition contains processing noise. Text produced under such circumstances is typically highly noisy containing spelling errors, abbreviations, non-standard words, false starts, repetitions, missing punctuations, missing letter case information, pause filling words such as “um” and “uh” and other texting and speech disfluencies. Such text can be seen in large amounts in contact centers, chat rooms, optical character recognition (OCR) of text documents, short message service (SMS) text, etc. Documents with historical language can also be considered noisy with respect to today's knowledge about the language. Such text contains important historical, religious, ancient medical knowledge that is useful. The nature of the noisy text produced in all these contexts warrants moving beyond traditional text analysis techniques.

Techniques for noisy text analysis
Missing punctuation and the use of non-standard words can often hinder standard natural language processing tools such as part-of-speech tagging
and parsing. Techniques to both learn from the noisy data and then to be able to process the noisy data are only now being developed.

Possible source of noisy text
World wide web: Poorly written text is found in web pages, online chat, blogs, wikis, discussion forums, newsgroups. Most of these data are unstructured and the style of writing is very different from, say, well-written news articles. Analysis for the web data is important because they are sources for market buzz analysis, market review, trend estimation, etc. Also, because of the large amount of data, it is necessary to find efficient methods of information extraction, classification, automatic summarization and analysis of these data.
Contact centers: This is a general term for help desks, information lines and customer service centers operating in domains ranging from computer sales and support to mobile phones to apparels. On an average a person in the developed world interacts at least once a week with a contact center agent. A typical contact center agent handles over a hundred calls per day. They operate in various modes such as voice, online chat and E-mail. The contact center industry produces gigabytes of data in the form of E-mails, chat logs, voice conversation transcriptions, customer feedback, etc. A bulk of the contact center data is voice conversations. Transcription of these using state of the art automatic speech recognition results in text with 30-40% word error rate. Further, even written modes of communication like online chat between customers and agents and even the interactions over email tend to be noisy. Analysis of contact center data is essential for customer relationship management, customer satisfaction analysis, call modeling, customer profiling, agent profiling, etc., and it requires sophisticated techniques to handle poorly written text.
Printed Documents: Many libraries, government organizations and national defence organizations have vast repositories of hard copy documents. To retrieve and process the content from such documents, they need to be processed using Optical Character Recognition. In addition to printed text, these documents may also contain handwritten annotations. OCRed text can be highly noisy depending on the font size, quality of the print etc. It can range from 2-3% word error rates to as high as 50-60% word error rates. Handwritten annotations can be particularly hard to decipher, and error rates can be quite high in their presence.
Short Messaging Service (SMS): Language usage over computer mediated discourses, like chats, emails and SMS texts, significantly differs from the standard form of the language. An urge towards shorter message length facilitating faster typing and the need for semantic clarity, shape the structure of this non-standard form known as the texting language.

See also
Text analytics
Information extraction
Computational linguistics
Natural language processing
Named entity recognition
Text mining
Automatic summarization
Statistical classification
Data quality

References
"Wong, W., Liu, W. & Bennamoun, M. Enhanced Integrated Scoring for Cleaning Dirty Texts. In: IJCAI Workshop on Analytics for Noisy Unstructured Text Data (AND), 2007; Hyderabad, India.".
"L. V. Subramaniam, S. Roy, T. A. Faruquie, S. Negi, A survey of types of text noise and techniques to handle noisy text. In: Third Workshop on Analytics for Noisy Unstructured Text Data (AND), 2009".
Ontology learning (ontology extraction, ontology generation, or ontology acquisition) is the automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms and the relationships between the concepts that these terms represent from a corpus of natural language text, and encoding them with an ontology language for easy retrieval. As building ontologies manually is extremely labor-intensive and time-consuming, there is great motivation to automate the process.
Typically, the process starts by extracting terms and concepts or noun phrases from plain text using linguistic processors such as part-of-speech tagging and phrase chunking. Then statistical 
or symbolic
techniques are used to extract relation signatures, often based on pattern-based or definition-based hypernym extraction techniques.

Procedure
Ontology learning (OL) is used to (semi-)automatically extract whole ontologies from natural language text. The process is usually split into the following eight tasks, which are not all necessarily applied in every ontology learning system.

Domain terminology extraction
During the domain terminology extraction step, domain-specific terms are extracted, which are used in the following step (concept discovery) to derive concepts. Relevant terms can be determined e. g. by calculation of the TF/IDF values or by application of the C-value / NC-value method. The resulting list of terms has to be filtered by a domain expert. In the subsequent step, similarly to coreference resolution in information extraction, the OL system determines synonyms, because they share the same meaning and therefore correspond to the same concept. The most common methods therefore are clustering and the application of statistical similarity measures.

Concept discovery
In the concept discovery step, terms are grouped to meaning bearing units, which correspond to an abstraction of the world and therefore to concepts. The grouped terms are these domain-specific terms and their synonyms, which were identified in the domain terminology extraction step.

Concept hierarchy derivation
In the concept hierarchy derivation step, the OL system tries to arrange the extracted concepts in a taxonomic structure. This is mostly achieved by unsupervised hierarchical clustering methods. Because the result of such methods is often noisy, a supervision, e. g. by evaluation by the user, is integrated. A further method for the derivation of a concept hierarchy exists in the usage of several patterns, which should indicate a sub- or supersumption relationship. Patterns like “X, that is a Y” or “X is a Y” indicate, that X is a subclass of Y. Such pattern can be analyzed efficiently, but they occur too infrequent, to extract enough sub- or supersumption relationships. Instead bootstrapping methods are developed, which learn these patterns automatically and therefore ensure a higher coverage.

Learning of non-taxonomic relations
At the learning of non-taxonomic relations step, relationships are extracted, which do not express any sub- or supersumption. Such relationships are e.g. works-for or located-in. There are two common approaches to solve this subtask. The first one is based upon the extraction of anonymous associations, which are named appropriately in a second step. The second approach extracts verbs, which indicate a relationship between the entities, represented by the surrounding words. But the result of both approaches has to be evaluated by an ontologist.

Rule discovery
During rule discovery, axioms (formal description of concepts) are generated for the extracted concepts. This can be achieved for example by analyzing the syntactic structure of a natural language definition and the application of transformation rules on the resulting dependency tree. The result of this process is a list of axioms, which is afterwards comprehended to a concept description. This one has to be evaluated by an ontologist.

Ontology population
At this step, the ontology is augmented with instances of concepts and properties. For the augmentation with instances of concepts, methods based on the matching of lexico-syntactic patterns are used. Instances of properties are added by application of bootstrapping methods, which collect relation tuples.

Concept hierarchy extension
In this step, the OL system tries to extend the taxonomic structure of an existing ontology with further concepts. This can be realized supervised by a trained classifier or unsupervised by the application of similarity measures.

Frame and event detection
During frame/event detection, the OL system tries to extract complex relationships from text, e.g. who departed from where to what place and when. Approaches range from applying SVM with kernel methods to semantic role labeling (SRL) to deep semantic parsing techniques.

Tools
Dog4Dag (Dresden Ontology Generator for Directed Acyclic Graphs) is an ontology generation plugin for Protégé 4.1 and OBOEdit 2.1. It allows for term generation, sibling generation, definition generation, and relationship induction. Integrated into Protégé 4.1 and OBO-Edit 2.1, DOG4DAG allows ontology extension for all common ontology formats (e.g., OWL and OBO). Limited largely to EBI and Bio Portal lookup service extensions.

See also
Automatic taxonomy construction
Computational linguistics
Domain ontology
Information extraction
Natural language understanding
Semantic Web
Text mining

Bibliography
P. Buitelaar, P. Cimiano (Eds.). Ontology Learning and Population: Bridging the Gap between Text and Knowledge, Series information for Frontiers in Artificial Intelligence and Applications, IOS Press, 2008.
P. Buitelaar, P. Cimiano, and B. Magnini (Eds.). Ontology Learning from Text: Methods, Evaluation and Applications, Series information for Frontiers in Artificial Intelligence and Applications, IOS Press, 2005.
Wong, W. (2009), "Learning Lightweight Ontologies from Text across Different Domains using the Web as Background Knowledge". Doctor of Philosophy thesis, University of Western Australia.
Wong, W., Liu, W. & Bennamoun, M. (2012), "Ontology Learning from Text: A Look back and into the Future". ACM Computing Surveys, Volume 44, Issue 4, Pages 20:1-20:36.
Thomas Wächter, Götz Fabian, Michael Schroeder: DOG4DAG: semi-automated ontology generation in OBO-Edit and Protégé. SWAT4LS London, 2011. doi:10.1145/2166896.2166926


== References ==
In natural language processing, open information extraction (OIE) is the task of generating a structured, machine-readable representation of the information in text, usually in the form of triples or n-ary propositions.

Overview
A proposition can be understood as truth-bearer, a textual expression of a potential fact (e.g., "Dante wrote the Divine Comedy"), represented in an amenable structure for computers [e.g., ("Dante", "wrote", "Divine Comedy")]. An OIE extraction normally consists of a relation and a set of arguments. For instance, ("Dante", "passed away in" "Ravenna") is a proposition formed by the relation "passed away in" and the arguments "Dante" and "Ravenna". The first argument is usually referred as the subject while the second is considered to be the object.The extraction is said to be a textual representation of a potential fact because its elements are not linked to a knowledge base. Furthermore, the factual nature of the proposition has not yet been established. In the above example, transforming the extraction into a full fledged fact would first require linking, if possible, the relation and the arguments to a knowledge base. Second, the truth of the extraction would need to be determined. In computer science transforming OIE extractions into ontological facts is known as relation extraction.
In fact, OIE can be seen as the first step to a wide range of deeper text understanding tasks such as relation extraction, knowledge-base construction, question answering, semantic role labeling. The extracted propositions can also be directly used for end-user applications such as structured search (e.g., retrieve all propositions with "Dante" as subject).
OIE was first introduced by TextRunner developed at the University of Washington Turing Center headed by Oren Etzioni. Other methods introduced later such as Reverb, OLLIE, ClausIE or CSD helped to shape the OIE task by characterizing some of its aspects. At a high level, all of these approaches make use of a set of patterns to generate the extractions. Depending on the particular approach, these patterns are either hand-crafted or learned.

OIE systems and contributions
Reverb suggested the necessity to produce meaningful relations to more accurately capture the information in the input text. For instance, given the sentence "Faust made a pact with the devil", it would be erroneous to just produce the extraction ("Faust", "made", "a pact") since it would not be adequately informative. A more precise extraction would be ("Faust", "made a pact with", "the devil"). Reverb also argued against the generation of overspecific relations.
OLLIE stressed two important aspects for OIE. First, it pointed to the lack of factuality of the propositions. For instance, in a sentence like "If John studies hard, he will pass the exam", it would be inaccurate to consider ("John", "will pass", "the exam") as a fact. Additionally, the authors indicated that an OIE system should be able to extract non-verb mediated relations, which account for significant portion of the information expressed in natural language text. For instance, in the sentence "Obama, the former US president, was born in Hawaii", an OIE system should be able to recognize a proposition ("Obama", "is", "former US president").
ClausIE introduced the connection between grammatical clauses, propositions, and OIE extractions. The authors stated that as each grammatical clause expresses a proposition, each verb mediated proposition can be identified by solely recognizing the set of clauses expressed in each sentence. This implies that to correctly recognize the set of propositions in an input sentence, it is necessary to understand its grammatical structure. The authors studied the case in the English language that only admits seven clause types, meaning that the identification of each proposition only requires defining seven grammatical patterns.
The finding also established a separation between the recognition of the propositions and its materialization. In a first step, the proposition can be identified without any consideration of its final form, in a domain-independent and unsupervised way, mostly based on linguistic principles. In a second step, the information can be represented according to the requirements of the underlying application, without conditioning the identification phase.
Consider the sentence "Albert Einstein was born in Ulm and died in Princeton". The first step will recognize the two propositions ("Albert Einstein", "was born", "in Ulm") and ("Albert Einstein", "died", "in Princeton"). Once the information has been correctly identified, the propositions can take the particular form required by the underlying application [e.g., ("Albert Einstein", "was born in", "Ulm") and ("Albert Einstein", "died in", "Princeton")].
CSD introduced the idea of minimality in OIE. It considers that computers can make better use of the extractions if they are expressed in a compact way. This is especially important in sentences with subordinate clauses. In these cases, CSD suggests the generation of nested extractions. For example, consider the sentence "The Embassy said that 6,700 Americans were in Pakistan". CSD generates two extractions [i] ("6,700 Americans", "were", "in Pakistan") and [ii] ("The Embassy", "said", "that [i]). This is usually known as reification.


== References ==
People Pattern is a digital marketing company based in Austin, Texas. The company's text analytics software applications extract facts, relationships and sentiment from unstructured data, which comprise approximately 85% of the information companies store electronically.Corporations that use or have used People Pattern software include McDonald's, Charles Schwab, Campbell's Soup, Discover, Wal-Mart, Nintendo, The University of Texas, and Cisco Systems.

History
People Pattern was founded in 2013. An early investor in People Pattern was Mohr Davidow Ventures,.

See also
Text mining

References
External links
Official website
The term phrase structure grammar was originally introduced by Noam Chomsky as the term for grammar studied previously by Emil Post and Axel Thue (Post canonical systems). Some authors, however, reserve the term for more restricted grammars in the Chomsky hierarchy: context-sensitive grammars or context-free grammars. In a broader sense, phrase structure grammars are also known as constituency grammars. The defining trait of phrase structure grammars is thus their adherence to the constituency relation, as opposed to the dependency relation of dependency grammars.

Constituency relation
In linguistics, phrase structure grammars are all those grammars that are based on the constituency relation, as opposed to the dependency relation associated with dependency grammars; hence, phrase structure grammars are also known as constituency grammars.  Any of several related theories for the parsing of natural language qualify as constituency grammars, and most of them have been developed from Chomsky's work, including

Government and binding theory
Generalized phrase structure grammar
Head-driven phrase structure grammar
Lexical functional grammar
The minimalist program
NanosyntaxFurther grammar frameworks and formalisms also qualify as constituency-based, although they may not think of themselves as having spawned from Chomsky's work, e.g.

Arc pair grammar, and
Categorial grammar.The fundamental trait that these frameworks all share is that they view sentence structure in terms of the constituency relation. The constituency relation derives from the subject-predicate division of Latin and Greek grammars that is based on term logic and reaches back to Aristotle in antiquity. Basic clause structure is understood in terms of a binary division of the clause into subject (noun phrase NP) and predicate (verb phrase VP).
The binary division of the clause results in a one-to-one-or-more correspondence. For each element in a sentence, there are one or more nodes in the tree structure that one assumes for that sentence. A two word sentence such as Luke laughed necessarily implies three (or more) nodes in the syntactic structure: one for the noun Luke (subject NP), one for the verb laughed (predicate VP), and one for the entirety Luke laughed (sentence S). The constituency grammars listed above all view sentence structure in terms of this one-to-one-or-more correspondence.

Dependency relation
By the time of Gottlob Frege, a competing understanding of the logic of sentences had arisen. Frege rejected the binary division of the sentence and replaced it with an understanding of sentence logic in terms of logical predicates and their arguments. On this alternative conception of sentence logic, the binary division of the clause into subject and predicate was not possible. It therefore opened the door to the dependency relation (although the dependency relation had also existed in a less obvious form in traditional grammars long before Frege). The dependency relation was first acknowledged concretely and developed as the basis for a comprehensive theory of syntax and grammar by Lucien Tesnière in his posthumously published work Éléments de syntaxe structurale (Elements of Structural Syntax).The dependency relation is a one-to-one correspondence: for every element (word or morph) in a sentence, there is just one node in the syntactic structure. The distinction is thus a graph-theoretical distinction. The dependency relation restricts the number of nodes in the syntactic structure of a sentence to the exact number of syntactic units (usually words) that that sentence contains. Thus the two word sentence Luke laughed implies just two syntactic nodes, one for Luke and one for laughed. Some prominent dependency grammars are listed here:

Recursive categorical syntax, sometimes called algebraic syntax
Functional generative description
Lexicase
Link grammar
Meaning-text theory
Operator grammar
Word grammarSince these grammars are all based on the dependency relation, they are by definition NOT phrase structure grammars.

Non-descript grammars
Other grammars generally avoid attempts to group syntactic units into clusters in a manner that would allow classification in terms of the constituency vs. dependency distinction. In this respect, the following grammar frameworks do not come down solidly on either side of the dividing line:

Construction grammar
Cognitive grammar

See also
Notes


== References ==
Powerset was an American company based in San Francisco, California, that, in 2006, was developing a natural language search engine for the Internet. On July 1, 2008, Powerset was acquired by Microsoft for an estimated $100 million.Powerset was working on building a  natural language search engine that could find targeted answers to user questions (as opposed to keyword based search). For example, when confronted with a question like "Which U.S. state has the highest income tax?", conventional search engines ignore the question phrasing and instead do a search on the keywords "state", "highest", "income", and "tax". Powerset on the other hand, attempts to use natural language processing to understand the nature of the question and return pages containing the answer.
The company was in the process of "building a natural language search engine that reads and understands every sentence on the Web". The company has licensed natural language technology from PARC, the former Xerox Palo Alto Research Center.On May 11, 2008, the company unveiled a tool for searching a fixed subset of English Wikipedia using conversational phrases rather than keywords.

Powerlabs
In a form of beta testing, Powerset opened an online community called Powerlabs on September 17, 2007. Business Week said: "The company hopes the site will marshal thousands of people to help build and improve its search engine before it goes public next year." Said The New York Times: "[Powerset Labs] goes far beyond the 'alpha' or 'beta' testing involved in most software projects, when users put a new product through rigorous testing to find its flaws. Powerset doesn’t have a product yet, but rather a collection of promising natural language technologies, which are the fruit of years of research at Xerox PARC."Powerlabs' initial search results are taken from Wikipedia.

People
Barney Pell (born March 18, 1968, in Hollywood, California) was co-founder and CEO of Powerset. Pell received his bachelor of science degree in symbolic systems from Stanford University in 1989, where he graduated Phi Beta Kappa and was a National Merit Scholar. Pell received a PhD in computer science from Cambridge University in 1993, where he was a Marshall Scholar. He has worked at NASA, as chief strategist and vice president of business development at StockMaster.com (acquired by Red Herring in March, 2000) and at Whizbang! Labs. Prior to joining Powerset, Pell was an Entrepreneur-in-Residence at Mayfield Fund, a venture capital firm in Silicon Valley.
Pell is also a founder of Moon Express, Inc., a U.S. company awarded a $10M commercial lunar contract by NASA and a competitor in the Google Lunar X PRIZE.Steve Newcomb was the COO and co-founder of Powerset.  Prior to joining Powerset, he was a co-founder of Loudfire, General Manager at Promptu, and was on the board of directors at Jaxtr.  He left Powerset in October 2007 to form Virgance, a social startup incubator.
Lorenzo Thione (born in Como, Italy) was the product architect and co-founder of Powerset. Prior to joining Powerset, he worked at FXPAL in natural language processing and related research fields. Thione earned his master's degree in software engineering from the University of Texas at Austin.Ronald Kaplan, former manager of research in Natural Language Theory and Technology at PARC, served as the company's CTO and CSO.Ryan Ferrier is a member of the founding team of Powerset. He managed personnel and internal operations. After 2008 he went on to co-found Serious Business, which made Facebook applications and was later bought by Zynga.
Another Powerset alumnus, Alex Le, became CTO of Serious Business and went on to become an executive producer at Zynga when it bought the company. Siqi Chen founded a stealth startup in mobile computing after leaving Powerset.

Investors
Powerset attracted a wide range of investors, many of whom had considerable experience in the venture capital field. The company received $12.5 million in Series A funding during November 2007, co-led by the venture capital firms Foundation Capital and The Founders Fund.Among the better-known investors:

Esther Dyson, founding chairman of ICANN, founder of the newsletter Release 1.0 and editor at Cnet
Peter Thiel, founder and former CEO of PayPal
Luke Nosek, founder of PayPal
Reid Hoffman, executive vice president of PayPal and founder of LinkedIn
First Round Capital, seed-stage venture firm

See also
Bing (search engine)

References
External links
Powerset main web site - redirects to Bing
Predictive text is an input technology used where one key or button represents many letters, such as on the numeric keypads of mobile phones and in accessibility technologies. Each key press results in a  prediction rather than repeatedly sequencing through the same group of "letters" it represents, in the same, invariable order.  Predictive text could allow for an entire word to be input by single keypress.  Predictive text makes efficient use of fewer device keys to input writing into a text message, an e-mail, an address book, a calendar, and the like.
The most widely used, general, predictive text systems are T9, iTap, eZiText, and LetterWise/WordWise.  There are many ways to build a device that predicts text, but all predictive text systems have initial linguistic settings that offer predictions that are re-prioritized to adapt to each user.  This learning adapts, by way of the device memory, to a user's disambiguating feedback that results in corrective key presses, such as pressing a "next" key to get to the intention. Most predictive text systems have a user database to facilitate this process.
Theoretically the number of keystrokes required per desired character in the finished writing is, on average, comparable to using a keyboard.  This is approximately true providing that all words used are in its database, punctuation is ignored, and no input mistakes are made typing or spelling.
The theoretical keystrokes per character, KSPC, of a keyboard is KSPC=1.00, and of multi-tap is KSPC=2.03. Eatoni's LetterWise is a predictive multi-tap hybrid, which when operating on a standard telephone keypad achieves KSPC=1.15 for English.
The choice of which predictive text system is the best to use involves matching the user's preferred interface style, the user's level of learned ability to operate predictive text software, and the user's efficiency goal. There are various levels of risk in predictive text systems, versus multi-tap systems, because the predicted text that is automatically written that provide the speed and mechanical efficiency benefit, could, if the user is not careful to review, result in transmitting misinformation.  Predictive text systems take time to learn to use well, and so generally, a device's system has user options to set up the choice of multi-tap or of any one of several schools of predictive text methods.

Background
Short message service (SMS) permits a mobile phone user to send text messages (also called messages, SMSes, texts, and txts) as a short message.  The most common system of SMS text input is referred to as "multi-tap". Using multi-tap, a key is pressed multiple times to access the list of letters on that key. For instance, pressing the "2" key once displays an "a", twice displays a "b" and three times displays a "c". To enter two successive letters that are on the same key, the user must either pause or hit a "next" button. A user can type by pressing an alphanumeric keypad without looking at the electronic equipment display. Thus, multi-tap is easy to understand, and can be used without any visual feedback. However, multi-tap is not very efficient, requiring potentially many keystrokes to enter a single letter.
In ideal predictive text entry, all words used are in the dictionary, punctuation is ignored, no spelling mistakes are made, and no typing mistakes are made.  The ideal dictionary would include all slang, proper nouns, abbreviations, URLs, foreign-language words and other user-unique words.  This ideal circumstance gives predictive text software the reduction in the number of key strokes a user is required to enter a word. The user presses the number corresponding to each letter and, as long as the word exists in the predictive text dictionary, or is correctly disambiguated by non-dictionary systems, it will appear.  For instance, pressing "4663" will typically  be interpreted as the word good, provided that a linguistic database in English is currently in use, though alternatives such as home, hood and hoof are also valid interpretations of the sequence of key strokes.
The most widely used systems of predictive text are Tegic's T9, Motorola's iTap, and the Eatoni Ergonomics' LetterWise and WordWise.  T9 and iTap use dictionaries, but Eatoni Ergonomics' products uses a disambiguation process, a set of statistical rules to recreate words from keystroke sequences.  All predictive text systems require a linguistic database for every supported input language.

Dictionary vs. non-dictionary systems
Traditional disambiguation works by referencing a dictionary of commonly used words, though Eatoni offers a dictionaryless disambiguation system.
In dictionary-based systems, as the user presses the number buttons, an algorithm searches the dictionary for a list of possible words that match the keypress combination, and offers up the most probable choice. The user can then confirm the selection and move on, or use a key to cycle through the possible combinations.
A non-dictionary system constructs words and other sequences of letters from the statistics of word parts. To attempt predictions of the intended result of keystrokes not yet entered, disambiguation may be combined with a word completion facility.
Either system (disambiguation or predictive) may include a user database, which can be further classified as a "learning" system when words or phrases are entered into the user database without direct user intervention.  The user database is for storing words or phrases which are not well disambiguated by the pre-supplied database. Some disambiguation systems further attempt to correct spelling, format text or perform other automatic rewrites, with the risky effect of either enhancing or frustrating user efforts to enter text.

History
The actuating keys of the Chinese typewriter created by Lin Yutang in the 1940s included suggestions for the characters following the one selected.  In 1951, the Chinese typesetter Zhang Jiying arranged Chinese characters in associative clusters, a precursor of modern predictive text entry, and broke speed records by doing so.  Predictive entry of text from a telephone keypad has been known at least since the 1970s (Smith and Goodwin, 1971). Aspects of predictive text have been patented for instance by Kondraske (1985), while a fully functional keypad to text system for communicating with deaf people via phone was patented in 1988 by Roy Feinson (#4,754,474) that included most of the features of modern predictive text systems including disambiguation and local dictionary storage. Predictive text was mainly used to look up names in directories over the phone, until mobile phone text messaging came into widespread use.

Example
On a typical phone keypad, if users wished to type the in a "multi-tap" keypad entry system, they would need to:

Press 8 (tuv) once to select t.
Press 4 (ghi) twice to select h.
Press 3 (def) twice to select e.Meanwhile, in a phone with predictive text, they need only:

Press 8 once to select the (tuv) group for the first character.
Press 4 once to select the (ghi) group for the second character.
Press 3 once to select the (def) group for the third character.The system updates the display as each keypress is entered, to show the most probable entry. In this example, prediction reduced the number of button presses from five to three. The effect is even greater with longer words and those composed of letters later in each key's sequence.
A dictionary-based predictive system is based on hope that the desired word is in the dictionary. That hope may be misplaced if the word differs in any way from common usage—in particular, if the word is not spelled or typed correctly, is slang, or is a proper noun.  In these cases, some other mechanism must be used to enter the word. Furthermore, the simple dictionary approach fails with agglutinative languages, where a single word does not necessarily represent a single semantic entity.

Companies and products
Predictive text is developed and marketed in a variety of competing products, such as Nuance Communications's T9. Other products include Motorola's iTap,  Eatoni Ergonomic's LetterWise (character, rather than word-based prediction), WordWise (word-based prediction without a dictionary), EQ3 (a QWERTY-like layout compatible with regular telephone keypads); Prevalent Devices's Phraze-It; Xrgomics' TenGO (a six-key reduced QWERTY keyboard system); Adaptxt (considers language, context, grammar and semantics); Lightkey (a predictive typing software for Windows); Clevertexting (statistical nature of the language, dictionaryless, dynamic key allocation); and  Oizea Type (temporal ambiguity); Intelab's Tauto; WordLogic's Intelligent Input Platform™ (patented, layer-based advanced text prediction, includes multi-language dictionary, spell-check, built-in Web search).

Textonyms
Words produced by the same combination of keypresses have been called "textonyms"; also "txtonyms"; or "T9onyms" (pronounced "tynonyms"), though they are not specific to T9. Selecting the wrong textonym can occur with no misspelling or typo, if the wrong textonym is selected by default or user error. As mentioned above, the key sequence 4663 on a telephone keypad, provided with a linguistic database in English, will generally be disambiguated as the word good. However, the same key sequence also corresponds to other words, such as home, gone, hoof, hood and so on.  For example, "Are you home?" could be rendered as "Are you good?" if the user neglects to alter the default 4663 word. This can lead to misunderstandings; for example sequence 735328 might correspond to either select or its antonym reject.  A 2010 row that led to manslaughter was sparked by a textonym error. Predictive text choosing a default different from that which the user expects has similarities with the Cupertino effect, by which spell-check software changes a spelling to that of an unintended word.
Textonyms have been used as Generation Y slang; for example, the use of the word book to mean cool, since book is the default in those predictive text systems that assume it is more frequent than cool. This is related to cacography.

Disambiguation failure and misspelling
Textonyms in which a disambiguation system gives more than one dictionary word for a single sequence of keystrokes are not the only issue, or even the most important issue, limiting the effectiveness of predictive text implementations. More important, according to the above references, are words for which the disambiguation produces a single, incorrect response.  The system may, for example, respond with Blairf upon input of 252473, when the intended word was Blaise or Claire, both of which correspond to the keystroke sequence, but are not, in this example, found by the predictive text system. When typos or misspellings  occur, they are very unlikely to be recognized correctly by a disambiguation system, though error correction mechanisms may mitigate that effect.

See also
Concepts
Multi-tap
Assistive technology
Autocomplete
Text entry interface
Input method editor
Text messaging
SMS language
Speech-to-Text Reporter

Products
T9 (predictive text)
ITap
LetterWise
Q9 input method
Adaptxt

Devices
iPhone
LG VX9400
Nokia 5510
Avigo 10

References
Further reading
Smith, Sidney L.; Goodwin, Nancy C. (1971). "Alphabetic Data Entry Via the Touch-Tone Pad: A Comment". Human Factors. 13 (2): 189–190. doi:10.1177/001872087101300212.

External links
New Scientist article on textonyms
An Australian newspaper article on textonyms
Technical notes on iTap (including lists of textonyms)
A production or production rule in computer science is a rewrite rule specifying a symbol substitution that can be recursively performed to generate new symbol sequences.  A finite set of productions 
  
    
      
        P
      
    
    {\displaystyle P}
   is the main component in the specification of a formal grammar (specifically a generative grammar). The other components are a finite set 
  
    
      
        N
      
    
    {\displaystyle N}
   of nonterminal symbols, a finite set (known as an alphabet) 
  
    
      
        Σ
      
    
    {\displaystyle \Sigma }
   of terminal symbols that is disjoint from 
  
    
      
        N
      
    
    {\displaystyle N}
   and a distinguished symbol 
  
    
      
        S
        ∈
        N
      
    
    {\displaystyle S\in N}
   that is the start symbol.
In an unrestricted grammar, a production is of the form 
  
    
      
        u
        →
        v
      
    
    {\displaystyle u\to v}
   where 
  
    
      
        u
      
    
    {\displaystyle u}
   and 
  
    
      
        v
      
    
    {\displaystyle v}
   are arbitrary strings of terminals and nonterminals however 
  
    
      
        u
      
    
    {\displaystyle u}
   may not be the empty string. If 
  
    
      
        v
      
    
    {\displaystyle v}
   is the empty string, this is denoted by the symbol 
  
    
      
        ϵ
      
    
    {\displaystyle \epsilon }
  , or 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
   (rather than leave the right-hand side blank). So productions are members of the cartesian product

  
    
      
        
          V
          
            ∗
          
        
        N
        
          V
          
            ∗
          
        
        ×
        
          V
          
            ∗
          
        
        =
        (
        
          V
          
            ∗
          
        
        ∖
        
          Σ
          
            ∗
          
        
        )
        ×
        
          V
          
            ∗
          
        
      
    
    {\displaystyle V^{*}NV^{*}\times V^{*}=(V^{*}\setminus \Sigma ^{*})\times V^{*}}
  ,where 
  
    
      
        V
        :=
        N
        ∪
        Σ
      
    
    {\displaystyle V:=N\cup \Sigma }
   is the vocabulary, 
  
    
      
        
          

          
          
            ∗
          
        
      
    
    {\displaystyle {}^{*}}
   is the Kleene star operator, 
  
    
      
        
          V
          
            ∗
          
        
        N
        
          V
          
            ∗
          
        
      
    
    {\displaystyle V^{*}NV^{*}}
   indicates concatenation, 
  
    
      
        ∪
      
    
    {\displaystyle \cup }
   denotes set union, and 
  
    
      
        ∖
      
    
    {\displaystyle \setminus }
   denotes set minus or set difference. If we do not allow the start symbol to occur in 
  
    
      
        v
      
    
    {\displaystyle v}
   (the word on the right side), we have to replace 
  
    
      
        
          V
          
            ∗
          
        
      
    
    {\displaystyle V^{*}}
   by 
  
    
      
        (
        V
        ∖
        {
        S
        }
        
          )
          
            ∗
          
        
      
    
    {\displaystyle (V\setminus \{S\})^{*}}
   on the right side of the cartesian product symbol.The other types of formal grammar in the Chomsky hierarchy impose additional restrictions on what constitutes a production. Notably in a context-free grammar, the left-hand side of a production must be a single nonterminal symbol. So productions are of the form:

  
    
      
        N
        →
        (
        N
        ∪
        Σ
        
          )
          
            ∗
          
        
      
    
    {\displaystyle N\to (N\cup \Sigma )^{*}}

Grammar generation
To generate a string in the language, one begins with a string consisting of only a single start symbol, and then successively applies the rules (any number of times, in any order) to rewrite this string.  This stops when we obtain a string containing only terminals. The language consists of all the strings that can be generated in this manner.  Any particular sequence of legal choices taken during this rewriting process yields one particular string in the language. If there are multiple different ways of generating this single string, then the grammar is said to be ambiguous.
For example, assume the alphabet consists of 
  
    
      
        a
      
    
    {\displaystyle a}
   and 
  
    
      
        b
      
    
    {\displaystyle b}
  , with the start symbol 
  
    
      
        S
      
    
    {\displaystyle S}
  , and we have the following rules:

1. 
  
    
      
        S
        →
        a
        S
        b
      
    
    {\displaystyle S\rightarrow aSb}
  
2. 
  
    
      
        S
        →
        b
        a
      
    
    {\displaystyle S\rightarrow ba}
  then we start with 
  
    
      
        S
      
    
    {\displaystyle S}
  , and can choose a rule to apply to it. If we choose rule 1, we replace 
  
    
      
        S
      
    
    {\displaystyle S}
   with 
  
    
      
        a
        S
        b
      
    
    {\displaystyle aSb}
   and obtain the string 
  
    
      
        a
        S
        b
      
    
    {\displaystyle aSb}
  . If we choose rule 1 again, we replace 
  
    
      
        S
      
    
    {\displaystyle S}
   with 
  
    
      
        a
        S
        b
      
    
    {\displaystyle aSb}
   and obtain the string 
  
    
      
        a
        a
        S
        b
        b
      
    
    {\displaystyle aaSbb}
  . This process is repeated until we only have symbols from the alphabet (i.e., 
  
    
      
        a
      
    
    {\displaystyle a}
   and 
  
    
      
        b
      
    
    {\displaystyle b}
  ). If we now choose rule 2, we replace 
  
    
      
        S
      
    
    {\displaystyle S}
   with 
  
    
      
        b
        a
      
    
    {\displaystyle ba}
   and obtain the string 
  
    
      
        a
        a
        b
        a
        b
        b
      
    
    {\displaystyle aababb}
  , and are done. We can write this series of choices more briefly, using symbols: 
  
    
      
        S
        ⇒
        a
        S
        b
        ⇒
        a
        a
        S
        b
        b
        ⇒
        a
        a
        b
        a
        b
        b
      
    
    {\displaystyle S\Rightarrow aSb\Rightarrow aaSbb\Rightarrow aababb}
  . The language of the grammar is the set of all the strings that can be generated using this process: 
  
    
      
        {
        b
        a
        ,
        a
        b
        a
        b
        ,
        a
        a
        b
        a
        b
        b
        ,
        a
        a
        a
        b
        a
        b
        b
        b
        ,
        …
        }
      
    
    {\displaystyle \{ba,abab,aababb,aaababbb,\dotsc \}}
  .

See also
Formal grammar
Finite automata
Generative grammar
L-system
Rewrite rule
Backus–Naur form (A compact form for writing the productions of a context-free grammar.)
Phrase structure rule
Post canonical system (Emil Post's production systems- a model of computation.)


== References ==
PropBank is a corpus that is annotated with verbal propositions and their arguments—a "proposition bank". Although "PropBank" refers to a specific corpus produced by Martha Palmer et al., the term propbank is also coming to be used as a common noun referring to any corpus that has been annotated with propositions and their arguments.
The PropBank project has played a role in recent research in natural language processing, and has been used in semantic role labelling.

Comparison
PropBank differs from FrameNet, the resource to which it is most frequently compared, in several ways.
PropBank is a verb-oriented resource, while FrameNet is centered on the more abstract notion of frames, which generalizes descriptions across similar verbs (e.g. "describe" and "characterize") as well as nouns and other words (e.g. "description"). PropBank does not annotate events or states of affairs described using nouns. PropBank commits to annotating all verbs in a corpus, whereas the FrameNet project chooses sets of example sentences from a large corpus and only in a few cases has annotated longer continuous stretches of text.
PropBank-style annotations often remain close to the syntactic level, while FrameNet-style annotations are sometimes more semantically motivated. From the start, PropBank was developed with the idea of serving as training data for machine learning-based semantic role labeling systems in mind. It requires that all arguments to a verb be syntactic constituents and different senses of a word are only distinguished if the differences bear on the arguments. Due to such differences, semantic role labeling with respect to PropBank is often a somewhat easier task than producing FrameNet-style annotations.

See also
VerbNet
FrameNet

References
External links
PropBank website
NomBank website
SALSA website
Question answering (QA) is a computer science discipline within the fields of information retrieval and natural language processing (NLP), which is concerned with building systems that automatically answer questions posed by humans in a natural language.

Overview
A question answering implementation, usually a computer program, may construct its answers by querying a structured database of knowledge or information, usually a knowledge base. More commonly, question answering systems can pull answers from an unstructured collection of natural language documents.
Some examples of natural language document collections used for question answering systems include:

a local collection of reference texts
internal organization documents and web pages
compiled newswire reports
a set of Wikipedia pages
a subset of World Wide Web pagesQuestion answering research attempts to deal with a wide range of question types including: fact, list, definition, How, Why, hypothetical, semantically constrained, and cross-lingual questions.

Closed-domain question answering deals with questions under a specific domain (for example, medicine or automotive maintenance), and can exploit domain-specific knowledge frequently formalized in ontologies. Alternatively, closed-domain might refer to a situation where only a limited type of questions are accepted, such as questions asking for descriptive rather than procedural information. Question answering systems in the context of machine reading applications have also been constructed in the medical domain, for instance related to Alzheimers disease
Open-domain question answering deals with questions about nearly anything, and can only rely on general ontologies and world knowledge. On the other hand, these systems usually have much more data available from which to extract the answer.Multimodal question answering uses multiple modalities of user input to answer questions, such as text and images.

History
Two early question answering systems were BASEBALL and LUNAR. BASEBALL answered questions about the US baseball league over a period of one year. LUNAR, in turn, answered questions about the geological analysis of rocks returned by the Apollo moon missions. Both question answering systems were very effective in their chosen domains. In fact, LUNAR was demonstrated at a lunar science convention in 1971 and it was able to answer 90% of the questions in its domain posed by people untrained on the system. Further restricted-domain question answering systems were developed in the following years. The common feature of all these systems is that they had a core database or knowledge system that was hand-written by experts of the chosen domain. The language abilities of BASEBALL and LUNAR used techniques similar to ELIZA and DOCTOR, the first chatterbot programs.
SHRDLU was a highly successful question-answering program developed by Terry Winograd in the late 1960s and early 1970s. It simulated the operation of a robot in a toy world (the "blocks world"), and it offered the possibility of asking the robot questions about the state of the world. Again, the strength of this system was the choice of a very specific domain and a very simple world with rules of physics that were easy to encode in a computer program.
In the 1970s, knowledge bases were developed that targeted narrower domains of knowledge. The question answering systems developed to interface with these expert systems produced more repeatable and valid responses to questions within an area of knowledge. These expert systems closely resembled modern question answering systems except in their internal architecture. Expert systems rely heavily on expert-constructed and organized knowledge bases, whereas many modern question answering systems rely on statistical processing of a large, unstructured, natural language text corpus.
The 1970s and 1980s saw the development of comprehensive theories in computational linguistics, which led to the development of ambitious projects in text comprehension and question answering. One example of such a system was the Unix Consultant (UC), developed by Robert Wilensky at U.C. Berkeley in the late 1980s. The system answered questions pertaining to the Unix operating system. It had a comprehensive hand-crafted knowledge base of its domain, and it aimed at phrasing the answer to accommodate various types of users. Another project was LILOG, a text-understanding system that operated on the domain of tourism information in a German city. The systems developed in the UC and LILOG projects never went past the stage of simple demonstrations, but they helped the development of theories on computational linguistics and reasoning.
Specialized natural language question answering systems have been developed, such as EAGLi for health and life scientists, and Wolfram Alpha, an online computational knowledge engine that answers factual queries directly by computing the answer from externally sourced curated data.

Architecture
As of 2001, question answering systems typically included a question classifier module that determines the type of question and the type of answer. A multiagent question-answering architecture has been proposed, where each domain is represented by an agent which tries to answer questions taking into account its specific knowledge; a meta–agent controls the cooperation between question answering agents and chooses the most relevant answer(s).

Question answering methods
Question answering is very dependent on a good search corpus—for without documents containing the answer, there is little any question answering system can do. It thus makes sense that larger collection sizes generally lend well to better question answering performance, unless the question domain is orthogonal to the collection. The notion of data redundancy in massive collections, such as the web, means that nuggets of information are likely to be phrased in many different ways in differing contexts and documents, leading to two benefits:

By having the right information appear in many forms, the burden on the question answering system to perform complex NLP techniques to understand the text is lessened.
Correct answers can be filtered from false positives by relying on the correct answer to appear more times in the documents than instances of incorrect ones.Some question answering systems rely heavily on automated reasoning. There are a number of question answering systems designed in Prolog, a logic programming language associated with artificial intelligence.

Open domain question answering
In information retrieval, an open domain question answering system aims at returning an answer in response to the user's question. The returned answer is in the form of short texts rather than a list of relevant documents. The system uses a combination of techniques from computational linguistics, information retrieval and knowledge representation for finding answers.
The system takes a natural language question as an input rather than a set of keywords, for example, "When is the national day of China?" The sentence is then transformed into a query through its logical form. Having the input in the form of a natural language question makes the system more user-friendly, but harder to implement, as there are various question types and the system will have to identify the correct one in order to give a sensible answer. Assigning a question type to the question is a crucial task, the entire answer extraction process relies on finding the correct question type and hence the correct answer type.
Keyword extraction is the first step for identifying the input question type. In some cases, there are clear words that indicate the question type directly. i.e. "Who", "Where" or "How many", these words tell the system that the answers should be of type "Person", "Location", "Number" respectively. In the example above, the word "When" indicates that the answer should be of type "Date". POS (part-of-speech) tagging and syntactic parsing techniques can also be used to determine the answer type. In this case, the subject is "Chinese National Day", the predicate is "is" and the adverbial modifier is "when", therefore the answer type is "Date". Unfortunately, some interrogative words like "Which", "What" or "How" do not give clear answer types. Each of these words can represent more than one type. In situations like this, other words in the question need to be considered. First thing to do is to find the words that can indicate the meaning of the question. A lexical dictionary such as WordNet can then be used for understanding the context.
Once the question type has been identified, an information retrieval system is used to find a set of documents containing the correct key words. A tagger and NP/Verb Group chunker can be used to verify whether the correct entities and relations are mentioned in the found documents. For questions such as "Who" or "Where", a named-entity recogniser is used to find relevant "Person" and "Location" names from the retrieved documents. Only the relevant paragraphs are selected for ranking.
A vector space model can be used as a strategy for classifying the candidate answers. Check if the answer is of the correct type as determined in the question type analysis stage. An inference technique can also be used to validate the candidate answers. A score is then given to each of these candidates according to the number of question words it contains and how close these words are to the candidate, the more and the closer the better. The answer is then translated into a compact and meaningful representation by parsing. In the previous example, the expected output answer is "1st Oct."

Mathematical question answering
An open source math-aware question answering system based on Ask Platypus and Wikidata was published in 2018. The system takes an English or Hindi natural language question as input and returns a mathematical formula retrieved from Wikidata as succinct answer. The resulting formula is translated into a computable form, allowing the user to insert values for the variables. Names and values of variables and common constants are retrieved from Wikidata if available. It is claimed that the system outperforms a commercial computational mathematical knowledge engine on a test set.

Progress
Question answering systems have been extended in recent years to encompass additional domains of knowledge  For example, systems have been developed to automatically answer temporal and geospatial questions, questions of definition and terminology, biographical questions, multilingual questions, and questions about the content of audio, images, and video. Current question answering research topics include:

interactivity—clarification of questions or answers
answer reuse or caching
semantic parsing
answer presentation
knowledge representation and reasoning
social media analysis with question answering systems
sentiment analysis
utilization of thematic roles
semantic resolution: to bridge the gap between syntactically different questions and answer-bearing texts
utilization of linguistic resources, such as WordNet, FrameNet, and the similar
Image captioning for visual question answeringIBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.
Facebook Research has made their DrQA system available under an open source license. This system has been used for open domain question answering using Wikipedia as knowledge source.

References
Further reading
Dragomir R. Radev, John Prager, and Valerie Samn. Ranking suspected answers to natural language questions using predictive annotation. In Proceedings of the 6th Conference on Applied Natural Language Processing, Seattle, WA, May 2000.
John Prager, Eric Brown, Anni Coden, and Dragomir Radev. Question-answering by predictive annotation. In Proceedings, 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Athens, Greece, July 2000.
Hutchins, W. John; Harold L. Somers (1992). An Introduction to Machine Translation. London: Academic Press. ISBN 978-0-12-362830-5.
L. Fortnow, Steve Homer (2002/2003).   A Short History of Computational Complexity.  In D. van Dalen, J. Dawson, and A. Kanamori, editors, The History of Mathematical Logic. North-Holland, Amsterdam.

External links
Question Answering Evaluation at NTCIR
Question Answering Evaluation at TREC
Question Answering Evaluation at CLEF
Quiz Question Answers
Online Question Answering System
Dragomir R. Radev is a Yale University professor of computer science working on natural language processing and information retrieval. He previously served as a University of Michigan computer science professor and Columbia University computer science adjunct professor. Radev serves as Member of the Advisory Board of Lawyaw.He is currently working in the fields of open domain question answering,  multi-document summarization, and the application of NLP in Bioinformatics, Social Network Analysis and Political Science.
Radev received his PhD in Computer Science from Columbia University in 1999. He is the secretary of [1] (2006–present) and associate editor of JAIR.

Awards
As NACLO founder, Radev shared the Linguistic Society of America 2011 Linguistics, Language and the Public Award. He is the  Co-winner of the Gosnell Prize (2006).
In 2015 he was named a fellow of the Association for Computing Machinery "for contributions to natural language processing and computational linguistics."

IOL
Radev has served as the coach and led the US national team in the International Linguistics Olympiad (IOL) to several gold medals [2][3].

Books
Puzzles in Logic, Languages and Computation (2013) 
Mihalcea and Radev (2011) Graph-based methods for NLP and IR

Selected Papers
SIGIR 1995 Generating summaries of multiple news articles
ANLP 1997 Building a generation knowledge source using internet-accessible newswire
Computational Linguistics 1998 Generating natural language summaries from multiple on-line sources
ACL 1998 Learning correlations between linguistic indicators and semantic constraints: Reuse of context dependent descriptions of entities
ANLP 2000 Ranking suspected answers to natural language questions using predictive annotation
CIKM 2001 Mining the web for answers to natural language questions
AAAI 2002 Towards CST-enhanced summarization
ACL 2003 Evaluation challenges in large-scale multi-document summarization: the Mead project
Information Processing and Management 2004 Centroid-based summarization of multiple documents
Journal of Artificial Intelligence Research 2004 LexRank: Graph-based lexical centrality as salience in text summarization
Journal of the American Association of Information Science and Technology 2005 Probabilistic question answering on the web
Communications of the ACM 2005 NewsInEssence: summarizing online news topics
EMNLP 2007 Semi-supervised classification for extracting protein interaction sentences using dependency parsing
Bioinformatics 2008 Identifying gene-disease associations using centrality on a literature mined gene-interaction network
IEEE Intelligent Systems 2008 natural language processing and the web
NAACL 2009 Generating surveys of scientific paradigms
Nucleic Acids Research 2009 Michigan molecular interactions r2: from interacting proteins to pathways
Journal of the American Association of Information Science and Technology 2009 Visual overviews for discovering key papers and influences across research fronts
KDD 2010 Divrank: the interplay of prestige and diversity in information networks
American Journal of Political Science 2010 How to Analyze Political Attention with Minimal Assumptions and Costs
Arxiv 2011 The effect of linguistic constraints on the large scale organization of language
Journal of Biomedical Semantics 2011 Mining of vaccine-associated ifn-gamma gene interaction networks using the vaccine ontology

External links
Team USA Brings Home the Linguistics Gold
Dragomir Radev, Co-Founders Recognized as NACLO Receives Linguistics, Language and the Public Award
Dragomir Radev Coaches US Linguistics Team to Multiple Wins
Dragomir Radev Honored as ACM Distinguished Scientist
Prof. Dragomir Radev Receives Gosnell Prize


== References ==
In linguistics, realization is the process by which some kind of surface representation is derived from its underlying representation; that is, the way in which some abstract object of linguistic analysis comes to be produced in actual language. Phonemes are often said to be realized by speech sounds. The different sounds that can realize a particular phoneme are called its allophones.
Realization is also a subtask of natural language generation, which involves
creating an actual text in a human language (English, French, etc.) from a syntactic
representation.  There are a number of software packages available for realization,
most of which have been developed by academic research groups in NLG. The remainder of this article concerns realization of this kind.

Example
For example, the following Java code causes the simplenlg system [2]  to print out the text The women do not smoke.:

In this example, the computer program has specified the linguistic constituents of the sentence (verb, subject), and also linguistic features (plural subject, negated), and from this information the realiser has constructed the actual sentence.

Processing
Realisation involves three kinds of processing:
Syntactic realisation:  Using grammatical knowledge to choose inflections, add function words and also to decide the order of components.  For example, in English the subject usually precedes the verb, and the negated form of smoke is do not smoke.
Morphological realisation: Computing inflected forms, for example the plural form of woman is women (not womans).
Orthographic realisation: Dealing with casing, punctuation, and formatting.  For example, capitalising The because it is the first word of the sentence.
The above examples are very basic, most realisers are capable of considerably more complex processing.

Systems
A number of realisers have been developed over the past 20 years.  These systems differ in terms of complexity and sophistication of their processing, robustness in dealing with unusual cases, and whether they are accessed programmatically via an API or whether they take a textual representation of a syntactic structure as their input.
There are also major differences in pragmatic factors such as documentation, support, licensing terms, speed and memory usage, etc.
It is not possible to describe all realisers here, but a few of the emerging areas are:

Simplenlg [3]: a document realizing engine with an api which intended to be simple to learn and use, focused on limiting scope to only finding the surface area of a document.
KPML [4]: this is the oldest realiser, which has been under development under different guises since the 1980s.  It comes with grammars for ten different languages.
FUF/SURGE [5]: a realiser which was widely used in the 1990s, and is still used in some projects today
OpenCCG [6]: an open-source realiser which has a number of nice features, such as the ability to use statistical language models to make realisation decisions.

References
External links
[7] - ACL NLG Portal (contains links to the above and many other realisers)
A recursive transition network ("RTN") is a graph theoretical schematic used to represent the rules of a context-free grammar. RTNs have application to programming languages, natural language and lexical analysis. Any sentence that is constructed according to the rules of an RTN is said to be "well-formed". The structural elements of a well-formed sentence may also be well-formed sentences by themselves, or they may be simpler structures. This is why RTNs are described as recursive.

Notes and references
See also
Syntax diagram
Computational linguistics
Context free language
Finite state machine
Formal grammar
Parse tree
Parsing
Augmented transition network
Referring expression generation (REG) is the subtask of natural language generation (NLG) that received most scholarly attention. While NLG is concerned with the conversion of non-linguistic information into natural language, REG focuses only on the creation of referring expressions (noun phrases) that identify specific entities called targets.
This task can be split into two sections. The content selection part determines which set of properties distinguish the intended target and the linguistic realization part defines how these properties are translated into natural language.
A variety of algorithms have been developed in the NLG community to generate different types of referring expressions.

Types of referring expressions
A referring expression (RE), in linguistics, is any noun phrase, or surrogate for a noun phrase, whose function in discourse is to identify some individual object (thing, being, event...) The technical terminology for identify differs a great deal from one school of linguistics to another. The most widespread term is probably refer, and a thing identified is a referent, as for example in the work of John Lyons. In linguistics, the study of reference relations belongs to pragmatics, the study of language use, though it is also a matter of great interest to philosophers, especially those wishing to understand the nature of knowledge, perception and cognition more generally.
Various devices can be used for reference: determiners, pronouns, proper names... Reference relations can be of different kinds; referents can be in a "real" or imaginary world, in discourse itself, and they may be singular, plural, or collective.

Pronouns
The simplest type of referring expressions are pronoun such as he and it.  The linguistics and natural language processing communities have developed various models for predicting anaphor referents, such as centering theory, and ideally referring-expression generation would be based on such models.  However most NLG systems use much simpler algorithms, for example using a pronoun if the referent was mentioned in the previous sentence (or sentential clause), and no other entity of the same gender was mentioned in this sentence.

Definite noun phrases
There has been a considerable amount of research on generating definite noun phrases, such as the big red book.  Much of this builds on the model proposed by Dale and Reiter. This has been extended in various ways, for example Krahmer et al. present a graph-theoretic model of definite NP generation with many nice properties.  In recent years a shared-task event has compared different algorithms for definite NP generation, using the TUNA corpus.

Spatial and temporal reference
Recently there has been more research on generating referring expressions for time and space.  Such references tend to be imprecise (what is the exact meaning of tonight?), and also to be interpreted in different ways by different people.  Hence it may be necessary to explicitly reason about false positive vs false negative tradeoffs, and even calculate the utility of different possible referring expressions in a particular task context.

Criteria for good expressions
Ideally, a good referring expression should satisfy a number of criteria:

Referential success: It should unambiguously identify the referent to the reader.
Ease of comprehension: The reader should be able to quickly read and understand it.
Computational complexity: The generation algorithm should be fast
No false inferences: The expression should not confuse or mislead the reader by suggesting false implicatures or other pragmatic inferences.  For example, a reader may be confused if he is told Sit by the brown wooden table in a context where there is only one table.

History
Pre-2000 era
REG goes back to the early days of NLG. One of the first approaches was done by Winograd in 1972 who developed an "incremental" REG algorithm for his SHRDLU program. Afterwards researchers started to model the human abilities to create referring expressions in the 1980s. This new approach to the topic was influenced by the researchers Appelt and Kronfeld who created the programs KAMP and BERTRAND and considered referring expressions as parts of bigger speech acts.
Some of their most interesting findings were the fact that referring expressions can be used to add information beyond the identification of the referent as well as the influence of communicative context and the Gricean maxims on referring expressions. Furthermore, its skepticism concerning the naturalness of minimal descriptions made Appelt and Kronfeld's research a foundation of later work on REG.
The search for simple, well-defined problems changed the direction of research in the early 1990s. This new approach was led by Dale and Reiter who stressed the identification of the referent as the central goal.
Like Appelt they discuss the connection between the Gricean maxims and referring expressions in their culminant paper in which they also propose a formal problem definition. Furthermore, Reiter and Dale discuss the Full Brevity and Greedy Heuristics algorithms as well as their Incremental Algorithm(IA) which became one of the most important algorithms in REG.

Later developments
After 2000 the research began to lift some of the simplifying assumptions, that had been made in early REG research in order to create more simple algorithms. Different research groups concentrated on different limitations creating several expanded algorithms. Often these extend the IA in a single perspective for example in relation to:

Reference to Sets like "the t-shirt wearers" or "the green apples and the banana on the left"
Relational Descriptions like "the cup on the table" or "the woman who has three children"
Context Dependency, Vagueness and Gradeability include statements like "the older man" or "the car on the left" which are often unclear without a context
Salience and Generation of Pronouns are highly discourse dependent making for example "she" a reference to "the (most salient) female person"Many simplifying assumptions are still in place or have just begun to be worked on. Also a combination of the different extensions has yet to be done and is called a "non-trivial enterprise" by Krahmer and van Deemter.Another important change after 2000 was the increasing use of empirical studies in order to evaluate algorithms. This development took place due to the emergence of transparent corpora. Although there are still discussions about what the best evaluation metrics are, the use of experimental evaluation has already led to a better comparability of algorithms, a discussion about the goals of REG and more task-oriented research.
Furthermore, research has extended its range to related topics such as the choice of Knowledge Representation(KR) Frameworks. In this area the main question, which KR framework is most suitable for the use in REG remains open. The answer to this question depends on how well descriptions can be expressed or found. A lot of the potential of KR frameworks has been left unused so far.
Some of the different approaches are the usage of:

Graph search which treats relations between targets in the same way as properties.
Constraint Satisfaction which allows for a separation between problem specification and the implementation.
Modern Knowledge Representation which offers logical inference in for example Description Logic or Conceptual Graphs.

Problem definition
Dale and Reiter (1995) think about referring expressions as distinguishing descriptions.
They define:

The referent as the entity that should be described
The context set as set of salient entities
The contrast set or potential distractors as all elements of the context set except the referent
A property as a reference to a single attribute-value pairEach entity in the domain can be characterised as a set of attribute-value pairs for example 
  
    
      
        ⟨
      
    
    {\displaystyle \langle }
  type, dog
  
    
      
        ⟩
      
    
    {\displaystyle \rangle }
  , 
  
    
      
        ⟨
      
    
    {\displaystyle \langle }
  gender, female
  
    
      
        ⟩
      
    
    {\displaystyle \rangle }
   or 
  
    
      
        ⟨
      
    
    {\displaystyle \langle }
  age, 10 years
  
    
      
        ⟩
      
    
    {\displaystyle \rangle }
  .
The problem then is defined as follows:
Let 
  
    
      
        r
      
    
    {\displaystyle r}
   be the intended referent, and 
  
    
      
        C
      
    
    {\displaystyle C}
   be the contrast set. Then, a set 
  
    
      
        L
      
    
    {\displaystyle L}
   of attribute–value pairs will represent a distinguishing description if the following two conditions hold:

Every attribute–value pair in 
  
    
      
        L
      
    
    {\displaystyle L}
   applies to 
  
    
      
        r
      
    
    {\displaystyle r}
  : that is, every element of 
  
    
      
        L
      
    
    {\displaystyle L}
   specifies an attribute–value that 
  
    
      
        r
      
    
    {\displaystyle r}
   possesses.
For every member 
  
    
      
        c
      
    
    {\displaystyle c}
   of 
  
    
      
        C
      
    
    {\displaystyle C}
  , there is at least one element 
  
    
      
        l
      
    
    {\displaystyle l}
   of 
  
    
      
        L
      
    
    {\displaystyle L}
   that does not apply to 
  
    
      
        c
      
    
    {\displaystyle c}
  :  that is, there is an 
  
    
      
        l
      
    
    {\displaystyle l}
   in 
  
    
      
        L
      
    
    {\displaystyle L}
   that specifies an attribute–value that 
  
    
      
        c
      
    
    {\displaystyle c}
   does not  possess. 
  
    
      
        l
      
    
    {\displaystyle l}
   is said to rule out 
  
    
      
        c
      
    
    {\displaystyle c}
  .In other words, to generate a referring expression one is looking for a set of properties that apply to the referent but not to the distractors.The problem could be easily solved by conjoining all the properties of the referent which often leads to long descriptions violating the second Gricean Maxim of Quantity. Another approach would be to find the shortest distinguishing description like the Full Brevity algorithm does.
Yet in practice it is most common to instead include the condition that referring expressions produced by an algorithm should be as similar to human-produced ones as possible although this is often not explicitly mentioned.

Basic algorithms
Full Brevity
The Full Brevity algorithm always finds a minimal distinguishing description meaning there is no shorter distinguishing description in regard to properties used.
Therefore, it iterates over 
  
    
      
        n
        =
        1
        ,
        2
        ,
        3
        ,
        4
        ,
        .
        .
        .
      
    
    {\displaystyle n=1,2,3,4,...}
   and checks every description of a length of 
  
    
      
        n
      
    
    {\displaystyle n}
   properties until a distinguishing description is found.
Two problems arise from this way of creating referring expressions. Firstly the algorithm has a high complexity meaning it is NP-hard which makes it impractical to use. Secondly human speakers produce descriptions that are not minimal in many situations.

Greedy Heuristics
The Greedy Heuristics algorithm approximates the Full Brevity algorithm by iteratively adding the most distinguishing property to the description. The most distinguishing property means the property that rules out most of the remaining distractors. The Greedy Heuristics algorithm is more efficient than the Full Brevity algorithm.Dale and Reiter(1995) present the following algorithm for the Greedy Heuristic:
Let 
  
    
      
        L
      
    
    {\displaystyle L}
   be the set of properties to be realised in our description; let 
  
    
      
        P
      
    
    {\displaystyle P}
   be the set of properties known to be true of our intended referent 
  
    
      
        r
      
    
    {\displaystyle r}
   (we assume that 
  
    
      
        P
      
    
    {\displaystyle P}
   is non-empty); and let 
  
    
      
        C
      
    
    {\displaystyle C}
   be the set of distractors (the contrast set). The initial conditions are thus as follows:

  
    
      
        C
        =
        {
        ⟨
      
    
    {\displaystyle C=\{\langle }
  all distractors
  
    
      
        ⟩
        }
      
    
    {\displaystyle \rangle \}}
  ;

  
    
      
        P
        =
        {
        ⟨
      
    
    {\displaystyle P=\{\langle }
  all properties true of 
  
    
      
        r
        ⟩
        }
      
    
    {\displaystyle r\rangle \}}
  ;

  
    
      
        L
        =
        {
        }
      
    
    {\displaystyle L=\{\}}
  

In order to describe the intended referent 
  
    
      
        r
      
    
    {\displaystyle r}
   with respect to the contrast set 
  
    
      
        C
      
    
    {\displaystyle C}
  , we do the
following:

1. Check Success:
   if 
  
    
      
        
          |
        
        C
        
          |
        
        =
        0
      
    
    {\displaystyle |C|=0}
   then return 
  
    
      
        L
      
    
    {\displaystyle L}
   as a distinguishing description
   elseif 
  
    
      
        P
        =
        ∅
      
    
    {\displaystyle P=\emptyset }
   then fail
   else goto Step 2.
2. Choose Property:
   for each 
  
    
      
        
          p
          
            i
          
        
        ∈
        P
      
    
    {\displaystyle p_{i}\in P}
   do: 
  
    
      
        
          C
          
            i
          
        
        ←
        C
        ∩
        {
        x
        
          |
        
        
          p
          
            i
          
        
        (
        x
        )
        }
      
    
    {\displaystyle C_{i}\leftarrow C\cap \{x|p_{i}(x)\}}
  
   Chosen property is 
  
    
      
        
          p
          
            j
          
        
      
    
    {\displaystyle p_{j}}
   , where 
  
    
      
        
          C
          
            j
          
        
      
    
    {\displaystyle C_{j}}
   is the smallest set.
   goto Step 3.
3. Extend Description (wrt the chosen 
  
    
      
        
          p
          
            j
          
        
      
    
    {\displaystyle p_{j}}
  ):
   
  
    
      
        L
        ←
        L
        ∪
        {
        
          p
          
            j
          
        
        }
      
    
    {\displaystyle L\leftarrow L\cup \{p_{j}\}}
  
   
  
    
      
        C
        ←
        
          C
          
            j
          
        
      
    
    {\displaystyle C\leftarrow C_{j}}
  
   
  
    
      
        P
        ←
        P
        −
        {
        
          p
          
            j
          
        
        }
      
    
    {\displaystyle P\leftarrow P-\{p_{j}\}}
  
   goto Step 1.

Incremental Algorithm
The Incremental Algorithm (IA) by Dale and Reiter was the most influential algorithm before 2000. It is based on the idea of a preferential order of attributes or properties that speakers go by. So in order to run the Incremental Algorithm, first a preference order of attributes has to be given. Now the algorithm follows that order and adds those properties to the description which rule out any remaining distractors. Furthermore, Dale and Reiter stress the attribute type which is always included in their descriptions even if it does not rule out any distractors.
Also the type values are part of a subsumption hierarchy including some basic level values. For example, in the pet domain chihuahua is subsumed by dog and dog by animal. Because dog is defined as a basic level dog would be preferred by the algorithms, if chihuahua does not rule out any distractors.
The Incremental Algorithm is easy to implement and also computationally efficient running in polynomial time. The description generated by the IA can contain redundant properties that are superfluous because of later added properties. The creators do not consider this as a weakness, but rather as making the expressions less "psycholinguistically implausible".The following algorithm is a simplified version of Dale and Reiter’s Incremental Algorithm by Krahmer and van Deemter that takes as input the referent r, the D containing a collection of domain objects and a domain-specific ordered list Pref of preferred attributes. In the notation L is the description, C the context set of distractors and the function RulesOut(⟨Ai, V⟩) returns the set of objects which have a value different to V for attribute Ai.

IncrementalAlgorithm ({r}, D, Pref){
 L ← ∅
 C ← D - {r}
 for each Ai in list Pref do
    V = Value(r, Ai)
    if C ∩ RulesOut(⟨Ai, V⟩) ≠ ∅
    then L ← L ∪ {⟨Ai, V⟩}
         C ← C - RulesOut(⟨Ai, V⟩)
    endif
    if C = ∅
    then return L
    endif
 return failure }

Evaluation of REG systems
Before 2000 evaluation of REG systems has been of theoretical nature like the one done by Dale and Reiter. More recently, empirical studies have become popular which are mostly based on the assumption that the generated expressions should be similar to human-produced ones. Corpus-based evaluation began quite late in REG due to a lack of suitable data sets. Still corpus-based evaluation is the most dominant method at the moment though there is also evaluation by human judgement.

Corpus-based evaluation
First the distinction between text corpora and experimental corpora has to be made. Text corpora like the GNOME corpus can contain texts from all kind of domains. In REG they are used to evaluate the realization part of algorithms. The content selection part of REG on the other hand requires a corpus that contains the properties of all domain objects as well as the properties used in references. Typically those fully "semantically transparent" created in experiments using simple and controlled settings.
These experimental corpora once again can be separated into General-Purpose Corpora that were collected for another purpose but have been analysed for referring expressions and Dedicated Corpora that focus specifically on referring expressions. Examples of General-Purpose Corpora are the Pear Stories, the Map Task corpus or the Coconut corpus while the Bishop corpus, the Drawer corpus and the TUNA corpus count to the Dedicated Corpora. 
The TUNA corpus which contains web-collected data on the two domains furniture and people has been used in three shared REG challenges already.

Evaluation metrics
To measure the correspondence between corpora and the results of REG algorithms several Metrics have been developed.
To measure the content selection part the Dice coefficient or the MASI (Measuring Agreement on Set-valued Items) metric are used. These measure the overlap of properties in two descriptions. In an evaluation the scores are usually averaged over references made by different human participants in the corpus. Also sometimes a measure called Perfect Recall Percentage (PRP) or Accuracy is used which calculates the percentage of perfect matches between an algorithm-produced and a human-produced reference.
For the linguistic realization part of REG the overlap between strings has been measured using metrics like BLEU or NIST. A problem that occurs with string-based metrics is that for example "The small monkey" is measured closer to "The small donkey" than to "The little monkey".
A more time consuming way to evaluate REG algorithms is by letting humans judge the Adequacy (How clear is the description?) and Fluency (Is the description given in good and clear English?) of the generated expression. Also Belz and Gatt evaluated referring expressions using an experimental setup. The participants get a generated description and then have to click on the target. Here the extrinsic metrics reading time, identification time and error rate could be evaluated.

Notes


== References ==
Rhetorical structure theory (RST) was originally developed by William Mann and Sandra Thompson of the University of Southern California's Information Sciences Institute (ISI) and defined in a seminal paper in 1988. This theory was developed as part of studies of computer based text generation. Natural language researchers later began using RST in text summarization and other applications. RST addresses text organization by means of relations that hold between parts of text. It explains coherence by postulating a hierarchical, connected structure of texts.  In 2000, Daniel Marcu, also of ISI, demonstrated that practical discourse parsing and text summarization also could be achieved using RST.

Rhetorical relations
Rhetorical relations or coherence relations or discourse relations are paratactic (coordinate) or hypotactic (subordinate) relations that hold across two or more text spans. It is widely accepted that notion of coherence is through text relations like this. RST using rhetorical relations provide a systematic way for an analyst to analyse the text. An analysis is usually built by reading the text and constructing a tree using the relations. The following example is a title and
summary, appearing at the top of an article in Scientific American magazine (Ramachandran and Anstis, 1986). The original text, broken into numbered units, is:

[Title:] The Perception of Apparent Motion
[Abstract:] When the motion of an intermittently seen object is ambiguous
the visual system resolves confusion
by applying some tricks that reflect a built-in knowledge of properties of the physical worldIn the figure, numbers 1,2,3,4 show the corresponding units as explained above.
The fourth unit and the third unit form a relation "Means". The fourth unit is the essential part of this relation, so it is called the nucleus of the relation and third unit is called the satellite of the relation. Similarly second unit to third and fourth unit is forming relation "Condition". All units are also spans and spans may be composed of more than one unit.

Nuclearity in discourse
RST establishes two different types of units. Nuclei are considered as the most important parts of text whereas satellites contribute to the nuclei and are secondary. 
Nucleus contains basic information and satellite contains additional information about nucleus. The satellite is often incomprehensible without nucleus, whereas a text where a satellites have been deleted can be understood to a certain extent.

Hierarchy in the analysis
RST relations are applied recursively in a text, until all units in that text are constituents in an RST relation. The result of such analyses is that RST structure are typically represented as trees, with one top level relation that encompasses other relations at lower levels.

Why RST?
From linguistic point of view, RST proposes a different view of text organization than most linguistic theories.
RST points to a tight relation between relations and coherence in text
From a computational point of view, it provides a characterization of text relations that has been implemented in different systems and for applications as text generation and summarization.

In design rationale
Computer scientists Ana Cristina Bicharra Garcia and Clarisse Sieckenius de Souz have used RST as the basis of a design rationale system called ADD+. In ADD+, RST is used as the basis for the rhetorical organization of a knowledge base, in a way comparable to other knowledge representation systems such as issue-based information system (IBIS). Similarly, RST has been used in representation schemes for argumentation.

See also
Argument mining
Parse tree


== References ==
Naomi Sager (born 1927) is an American computational linguistics research scientist. She is a former research professor at New York University, now retired. She is a pioneer in the development of natural language processing for computers.

Early life and education
Sager was born in Chicago, Illinois in 1927. In 1946 she earned a bachelor of philosophy degree from the University of Chicago. She obtained a bachelor of science in Electrical Engineering from Columbia University in 1953.

Career
After graduating from Columbia, Sager worked for five years as an electronics engineer in the Biophysics Department of the Sloan-Kettering Institute for Cancer Research in New York City. In 1959 she moved to the University of Pennsylvania, where she worked on natural language computer processing. She was part of the team that developed the first English language parsing program, running on the UNIVAC I. Sager developed an algorithm to deal with syntactic ambiguity (where a sentence can be interpreted several ways due to ambiguity in its structure) and to convert sublanguage texts into suitable data formats for retrieval. This was "one of the first major practical applications of sublanguage analysis." This work formed the basis for a PhD thesis, and in 1968 she was awarded a PhD in linguistics from the University of Pennsylvania.Her work in linguistics led her to New York University, where she collaborated with James Morris and Morris Salkoff to develop a parsing program based on natural language processing. In 1965 NYU launched the Linguistic String Project under Sager's leadership. It was aimed at developing computer methods to access information in the scientific and technical literature, based on linguistic principles. In particular, the team drew on Zellig Harris's discourse analysis methodology to develop a system for computer analysis of natural language. Sager managed the project for 30 years until her retirement in 1995.At NYU she taught classes in natural language processing and advised doctoral students, many of whom (such as Jerry Hobbs and Carol Friedman) are now leaders in the field of natural language processing.

Selected publications
Sager, Naomi. Natural Language Information Processing: A Computer Grammar of English and Its Applications Addison-Wesley Publishing Company, Inc. (1981).
Sager, Naomi. Syntactic analysis of natural language. Advances in computers 8.153–188 (1967): 35.
Sager, Naomi, et al. Natural Language Processing and the Representation of Clinical Data. Journal of the American Medical Informatics Association 1:142–160 (March–April 1994).


== References ==
Sayre’s Paradox is a dilemma encountered in the design of automated handwriting recognition systems.  A standard statement of the paradox is that a cursively written word cannot be recognized without being segmented and cannot be segmented without being recognized. The paradox was first articulated in a 1973 publication by Kenneth M. Sayre, after whom it was named.

Nature of the problem
It is relatively easy to design automated systems capable of recognizing words inscribed in a printed format.  Such words are segmented into letters by the very act of writing them on the page.  Given templates matching typical letter shapes in a given language, individual letters can be identified with a high degree of probability.  In cases of ambiguity, probable letter sequences can be compared with a selection of properly spelled words in that language (called a lexicon). If necessary, syntactic features of the language can be applied to render a generally accurate identification of the words in question. Printed-character recognition systems of this sort are commonly used in processing standardized government forms, in sorting mail by zip code, and so forth.
In cursive writing, however, letters comprising a given word typically flow sequentially without gaps between them.  Unlike a sequence of printed letters, cursively connected letters are not segmented in advance.  Here is where Sayre’s Paradox comes into play.  Unless the word is already segmented into letters, template-matching techniques like those described above cannot be applied.  Prior segmentation, that is to say, is necessary for word recognition.  On the other hand, there are no reliable techniques for segmenting a word into letters unless the word itself has been previously identified.  Word recognition requires letter segmentation, and letter segmentation requires word recognition.  There is no way a cursive writing recognition system employing standard template-matching techniques can do both simultaneously.
Advantages to be gained by use of automated cursive writing recognition systems include routing mail with handwritten addresses, reading handwritten bank checks, and automated digitalization of hand-written documents. These are practical incentives for finding ways of circumventing Sayre’s Paradox.

Avoiding the paradox
One way of ameliorating the adverse effects of the paradox is to normalize the word inscriptions to be recognized.  Normalization amounts to eliminating idiosyncrasies in the penmanship of the writer, such as unusual slope of the letters and unusual slant of the cursive line. This procedure can increase the probability of a correct match with a letter template, resulting in an incremental improvement in the success rate of the system.  Since improvement of this sort still depends on accurate segmentation, however, it remains subject to the limitations of Sayre’s Paradox. Researchers have come to realize that the only way to circumvent the paradox is by use of procedures that do not rely on accurate segmentation.

Directions of current research
Segmentation is accurate to the extent that it matches distinctions among letters in the actual inscriptions presented to the system for recognition (the input data).  This is sometimes referred to as “explicit segmentation”. “Implicit segmentation,” by contrast, is division of the cursive line into more parts than the number of actual letters in the cursive line itself.  Processing these “implicit parts” to achieve eventual word identification requires specific statistical procedures involving Hidden Markov Models (HMM).
A Markov model is a statistical representation of a random process, which is to say a process in which future states are independent of states occurring before the present. In such a process, a given state is dependent only on the conditional probability of its following the state immediately before it.  An example is a series of outcomes from successive casts of a die.  An HMM is a Markov model, individual states of which are not fully known.  Conditional probabilities between states are still determinate, but the identities of individual states are not fully disclosed.
Recognition proceeds by matching HMMs of words to be recognized with previously prepared HMMs of words in the lexicon.  The best match in a given case is taken to indicate the identity of the handwritten word in question.  As with systems based on explicit segmentation, automated recognition systems based on implicit segmentation are judged more or less successful according to the percentage of correct identifications they accomplish.
Instead of explicit segmentation techniques, most automated handwriting recognition systems today employ implicit segmentation in conjunction with HMM-based matching procedures. The constraints epitomized by Sayre’s Paradox are largely responsible for this shift in approach.

References
External links
Kenneth M. Sayre and the Philosophic Institute.
Semantic analysis (computational) is a composite of semantic analysis and computational components. Semantic analysis refers to a formal analysis of meaning, and computational refers to approaches that in principle support effective implementation in digital computers.

See also
Computational semantics
Natural language processing
Semantic analytics
Semantic analysis (machine learning)
Semantic Web
SemEval

References
Further reading
Chris Fox (2010), "Computational Semantics", In Alexander Clark, Chris Fox, and Shalom Lappin, editors. The Handbook of Computational Linguistics and Natural Language Processing. Malden, MA: Wiley-Blackwell, 394-428.
Agirre, Eneko, Lluis Marquez & Richard Wincentowski (2009), "Computational semantic analysis of language: SemEval-2007 and beyond", Language Resources and Evaluation 43(2):97-104
Semantic analytics, also termed semantic relatedness, is the use of ontologies to analyze content in web resources. This field of research combines text analytics and Semantic Web technologies like RDF. Semantic analytics measures the relatedness of different ontological concepts.
Some academic research groups that have active project in this area include Kno.e.sis Center at Wright State University among others.

History
An important milestone in the beginning of semantic analytics occurred in 1996, although the historical progression of these algorithms is largely subjective. In his seminal study publication, Philip Resnik established that computers have the capacity to emulate human judgement. Spanning the publications of multiple journals, improvements to the accuracy of general semantic analytic computations all claimed to revolutionize the field. However, the lack of a standard terminology throughout the late 1990s was the cause of much miscommunication. This prompted Budanitsky & Hirst to standardize the subject in 2006 with a summary that also set a framework for modern spelling and grammar analysis.In the early days of semantic analytics, obtaining a large enough reliable knowledge bases was difficult. In 2006, Strube & Ponzetto demonstrated that Wikipedia could be used in semantic analytic calculations. The usage of a large knowledge base like Wikipedia allows for an increase in both the accuracy and applicability of semantic analytics.

Methods
Given the subjective nature of the field, different methods used in semantic analytics depend on the domain of application. No singular methods is considered correct, however one of the most generally effective and applicable method is explicit semantic analysis (ESA). ESA was developed by Evgeniy Gabrilovich and Shaul Markovitch in the late 2000s. It uses machine learning techniques to create a semantic interpreter, which extracts text fragments from articles into a sorted list. The fragments are sorted by how related they are to the surrounding text.
Latent semantic analysis (LSA) is another common method that does not use ontologies, only considering the text in the input space.

Applications
Entity linking
Ontology building / knowledge base population
Search and query tasks
Natural language processing
Spoken dialog systems (e.g., Amazon Alexa, Google Assistant, Microsoft's Cortana)
Artificial intelligence
Knowledge managementThe application of semantic analysis methods generally streamlines organizational processes of any knowledge management system. Academic libraries often use a domain-specific application to create a more efficient organizational system. By classifying scientific publications using semantics and Wikipedia, researchers are helping people find resources faster. Search engines like Semantic Scholar provide organized access to millions of articles.

See also
Relationship extraction
Semantic similarity
Text analytics

References
External Links
Semantic Scholar
In natural language processing, semantic compression is a process of compacting a lexicon used to build 
a textual document (or a set of documents) by reducing language heterogeneity, while maintaining text semantics. 
As a result, the same ideas can be represented using a smaller set of words.
In most applications, semantic compression is a lossy compression, that is, increased prolixity does not compensate for the lexical compression, and an original document cannot be reconstructed in a reverse process.

By generalization
Semantic compression is basically achieved in two steps, using frequency dictionaries and semantic network:

determining cumulated term frequencies to identify target lexicon,
replacing less frequent terms with their hypernyms (generalization) from target lexicon.Step 1 requires assembling word frequencies and 
information on semantic relationships, specifically hyponymy. Moving upwards in word hierarchy, 
a cumulative concept frequency is calculating by adding a sum of hyponyms' frequencies to frequency of their hypernym:

  
    
      
        c
        u
        m
        f
        (
        
          k
          
            i
          
        
        )
        =
        f
        (
        
          k
          
            i
          
        
        )
        +
        
          ∑
          
            j
          
        
        c
        u
        m
        f
        (
        
          k
          
            j
          
        
        )
      
    
    {\displaystyle cumf(k_{i})=f(k_{i})+\sum _{j}cumf(k_{j})}
   where 
  
    
      
        
          k
          
            i
          
        
      
    
    {\displaystyle k_{i}}
   is a hypernym of 
  
    
      
        
          k
          
            j
          
        
      
    
    {\displaystyle k_{j}}
  .
Then, a desired number of words with top cumulated frequencies are chosen to build a targed lexicon.
In the second step, compression mapping rules are defined for the remaining words, in order to handle every occurrence 
of a less frequent hyponym as its hypernym in output text.

ExampleThe below fragment of text has been processed by the semantic compression. Words in bold have been replaced by their hypernyms.

They are both nest building social insects, but paper wasps and honey bees organize their colonies 
in very different ways. In a new study, researchers report that despite their differences, these insects 
rely on the same network of genes to guide their social behavior.The study appears in the Proceedings of the 
Royal Society B: Biological Sciences. Honey bees and paper wasps are separated by more than 100 million years of 

evolution, and there are striking differences in how they divvy up the work of maintaining a colony.
The procedure outputs the following text:

They are both facility building insect, but insects and honey insects arrange their biological groups 
in very different structure. In a new study, researchers report that despite their difference of opinions, these insects 
act the same network of genes to steer their party demeanor. The study appears in the proceeding of the 
institution bacteria Biological Sciences. Honey insects and insect are separated by more than hundred million years of 

organic processes, and there are impinging differences of opinions in how they divvy up the work of affirming a biological group.

Implicit semantic compression
A natural tendency to keep natural language expressions concise can be perceived as a form of implicit semantic compression, by omitting unmeaningful words or redundant meaningful words (especially to avoid pleonasms).

Applications and advantages
In the vector space model, compacting a lexicon leads to a reduction of dimensionality, which results in less 
computational complexity and a positive influence on efficiency. 
Semantic compression is advantageous in information retrieval tasks, improving their effectiveness (in terms of both precision and recall). This is due to more precise descriptors (reduced effect of language diversity – limited language redundancy, a step towards a controlled dictionary).
As in the example above, it is possible to display the output as natural text (re-applying inflexion, adding stop words).

See also
Controlled natural language
Information theory
Lexical substitution
Quantities of information
Text simplification

References
External links
Semantic compression on Project SENECA (Semantic Networks and Categorization) website
A semantic decomposition is an algorithm that breaks down the meanings of phrases or concepts into less complex concepts. The result of a semantic decomposition is a representation of meaning. This representation can be used for tasks, such as those related to artificial intelligence or machine learning. Semantic decomposition is common in natural language processing applications.
The basic idea of a semantic decomposition is taken from the learning skills of adult humans, where words are explained using other words. It is based on Meaning-text theory. Meaning-text theory is used as a theoretical linguistic framework to describe the meaning of concepts with other concepts.

Background
Given that an AI does not inherently have language, it is unable to think about the meanings behind the words of a language. An artificial notion of meaning needs to be created for a strong AI to emerge. AI today is able to capture the syntax of language for many specific problems, but never establishes meaning for the words of these languages, nor is it able to abstract these words to higher-order concepts Creating an artificial representation of meaning requires the analysis of what meaning is. Many terms are associated with meaning, including semantics, pragmatics, knowledge and understanding or word sense. Each term describes a particular aspect of meaning, and contributes to a multitude of theories explaining what meaning is. These theories need to be analyzed further to develop an artificial notion of meaning best fit for our current state of knowledge.

Graph representations
Representing meaning as a graph is one of the two ways that both an AI cognition and a linguistic researcher think about meaning (connectionist view). Logicians utilize a formal representation of meaning to build upon the idea of symbolic representation, whereas description logics describe languages and the meaning of symbols. This contention between 'neat' and 'scruffy' techniques has been discussed since the 1970s.Research has so far identified semantic measures and with that Word-sense disambiguation (WSD) - the differentiation of meaning of words - as the main problem of language understanding. As an AI-complete environment, WSD is a core problem of natural language understanding. AI approaches that use knowledge-given reasoning creates a notion of meaning combining the state of the art knowledge of natural meaning with the symbolic and connectionist formalization of meaning for AI. The abstract approach is shown in Figure. First, a connectionist knowledge representation is created as a semantic network consisting of concepts and their relations to serve as the basis for the representation of meaning.This graph is built out of different knowledge sources like WordNet, Wiktionary, and BabelNET. The graph is created by lexical decomposition that recursively breaks each concept semantically down into a set of semantic primes. The primes are taken from the theory of Natural Semantic Metalanguage, which has been analyzed for usefulness in formal languages. Upon this graph marker passing is used to create the dynamic part of meaning representing thoughts. The marker passing algorithm, where symbolic information is passed along relations form one concept to another, uses node and edge interpretation to guide its markers. The node and edge interpretation model is the symbolic influence of certain concepts.
Future work uses the created representation of meaning to build heuristics and evaluate them through capability matching and agent planning, chatbots or other applications of natural language understanding.

See also
Latent Semantic Analysis
Lexical semantics
Principle of compositionality


== References ==
Semantic folding theory describes a procedure for encoding the semantics of natural language text in a semantically grounded binary representation. This approach provides a framework for modelling how language data is processed by the neocortex.

Theory
Semantic folding theory draws inspiration from Douglas R. Hofstadter's Analogy as the Core of Cognition which suggests that the brain makes sense of the world by identifying and applying analogies. The theory hypothesises that semantic data must therefore be introduced to the neocortex in such a form as to allow the application of a similarity measure and offers, as a solution, the sparse binary vector employing a two-dimensional topographic semantic space as a distributional reference frame. The theory builds on the computational theory of the human cortex known as hierarchical temporal memory (HTM), and positions itself as a complementary theory for the representation of language semantics.
A particular strength claimed by this approach is that the resulting binary representation enables complex semantic operations to be performed simply and efficiently at the most basic computational level.

Two-dimensional semantic space
Analogous to the structure of the neocortex, Semantic Folding theory posits the implementation of a semantic space as a two-dimensional grid. This grid is populated by context-vectors in such a way as to place similar context-vectors closer to each other, for instance, by using competitive learning principles. This vector space model is presented in the theory as an equivalence to the well known word space model described in the Information Retrieval literature.
Given a semantic space (implemented as described above) a word-vector can be obtained for any given word Y by employing the following algorithm:
For each position X in the semantic map (where X represents cartesian coordinates)

    if the word Y is contained in the context-vector at position X
         then add 1 to the corresponding position in the word-vector for Y
    else 
         add 0 to the corresponding position in the word-vector for Y

The result of this process will be a word-vector containing all the contexts in which the word Y appears and will therefore be representative of the semantics of that word in the semantic space. It can be seen that the resulting word-vector is also in a sparse distributed representation (SDR) format [Schütze, 1993] & [Sahlgreen, 2006]. Some properties of word-SDRs that are of particular interest with respect to computational semantics are:
high noise resistance: As a result of similar contexts being placed closer together in the underlying map, word-SDRs are highly tolerant of false or shifted "bits".
boolean logic: It is possible to manipulate word-SDRs in a meaningful way using boolean (OR, AND, exclusive-OR) and/or arithmetical (SUBtract) functions .
sub-sampling: Word-SDRs can be sub-sampled to a high degree without any appreciable loss of semantic information.
topological two-dimensional representation: The SDR representation maintains the topological distribution of the underlying map therefore words with similar meanings will have similar word-vectors. This suggests that a variety of measures can be applied to the calculation of semantic similarity, from a simple overlap of vector elements, to a range of distance measures such as: Euclidean distance, Hamming distance, Jaccard distance, cosine similarity, Levenshtein distance, Sørensen-Dice index, etc.

Semantic spaces
Semantic spaces in the natural language domain aim to create representations of natural language that are capable of capturing meaning. The original motivation for semantic spaces stems from two core challenges of natural language: Vocabulary mismatch (the fact that the same meaning can be expressed in many ways) and ambiguity of natural language (the fact that the same term can have several meanings).
The application of semantic spaces in natural language processing (NLP) aims at overcoming limitations of rule-based or model-based approaches operating on the keyword level. The main drawback with these approaches is their brittleness, and the large manual effort required to create either rule-based NLP systems or training corpora for model learning. Rule-based and machine learning-based models are fixed on the keyword level and break down if the vocabulary differs from that defined in the rules or from the training material used for the statistical models.
Research in semantic spaces dates back more than 20 years. In 1996, two papers were published that raised a lot of attention around the general idea of creating semantic spaces: latent semantic analysis from Microsoft and Hyperspace Analogue to Language from the University of California. However, their adoption was limited by the large computational effort required to construct and use those semantic spaces. A breakthrough with regard to the accuracy of modelling associative relations between words (e.g. "spider-web", "lighter-cigarette", as opposed to synonymous relations such as "whale-dolphin", "astronaut-driver") was achieved by explicit semantic analysis (ESA) in 2007. ESA was a novel (non-machine learning) based approach that represented words in the form of vectors with 100,000 dimensions (where each dimension represents an Article in Wikipedia). However practical applications of the approach are limited due to the large number of required dimensions in the vectors.
More recently, advances in neural networking techniques in combination with other new approaches (tensors) led to a host of new recent developments: Word2vec from Google and GloVe from Stanford University.
Semantic folding represents a novel, biologically inspired approach to semantic spaces where each word is represented as a sparse binary vector with 16,000 dimensions (a semantic fingerprint) in a 2D semantic map (the semantic universe). Sparse binary representation are advantageous in terms of computational efficiency, and allow for the storage of very large numbers of possible patterns.

Visualization
The topological distribution over a two-dimensional grid (outlined above) lends itself to a bitmap type visualization of the semantics of any word or text, where each active semantic feature can be displayed as e.g. a pixel. As can be seen in the images shown here, this representation allows for a direct visual comparison of the semantics of two (or more) linguistic items.
Image 1 clearly demonstrates that the two disparate terms "dog" and "car" have, as expected, very obviously different semantics.
Image 2 shows that only one of the meaning contexts of  "jaguar", that of "Jaguar" the car, overlaps with the meaning of Porsche (indicating partial similarity). Other meaning contexts of "jaguar" e.g. "jaguar" the animal clearly have different non-overlapping contexts.
The visualization of semantic similarity using Semantic Folding bears a strong resemblance to the fMRI images produced in a research study conducted by A.G. Huth et al., where it is claimed that words are grouped in the brain by meaning.

Notes


== References ==
Semantic interpretation is an important component in dialog systems. It is related to natural language understanding, but mostly it refers to the last stage of understanding. The goal of interpretation is binding the user utterance to concept, or something the system can understand.
Typically it is creating a database query based on user utterance.
Semantic neural network (SNN) is based on John von Neumann's neural network [von Neumann, 1966] and Nikolai Amosov M-Network. There are limitations to a link topology for the von Neumann’s network but SNN accept a case without these limitations. Only logical values can be processed, but SNN accept that fuzzy values can be processed too. All neurons into the von Neumann network are synchronized by tacts. For further use of self-synchronizing circuit technique SNN accepts neurons can be self-running or synchronized.
In contrast to the von Neumann network there are no limitations for topology of neurons for semantic networks. It leads to the impossibility of relative addressing of neurons as it was done by von Neumann. In this case an absolute readdressing should be used. Every neuron should have a unique identifier that would provide a direct access to another neuron. Of course, neurons interacting by axons-dendrites should have each other's identifiers. An absolute readdressing can be modulated by using neuron specificity as it was realized for biological neural networks.
There’s no description for self-reflectiveness and self-modification abilities into the initial description of semantic networks [Dudar Z.V., Shuklin D.E., 2000]. But in [Shuklin D.E. 2004] a conclusion had been drawn about the necessity of introspection and self-modification abilities in the system. For maintenance of these abilities a concept of pointer to neuron is provided. Pointers represent virtual connections between neurons. In this model, bodies and signals transferring through the neurons connections represent a physical body, and virtual connections between neurons are representing an astral body. It is proposed to create models of artificial neuron networks on the basis of virtual machine supporting the opportunity for paranormal effects.
SNN is generally used for natural language processing.

Related models
Computational creativity
Semantic hashing 
Semantic Pointer Architecture
Sparse distributed memory

References

Neumann, J., 1966. Theory of self-reproducing automata, edited and completed by Arthur W. Burks. - University of Illinois press, Urbana and London
Dudar Z.V., Shuklin D.E., 2000. Implementation of neurons for semantic neural nets that’s understanding texts in natural language. In Radio-electronika i informatika KhTURE, 2000. No 4. Р. 89-96.
Shuklin D.E., 2004. The further development of semantic neural network models. In Artificial Intelligence, Donetsk, "Nauka i obrazovanie" Institute of Artificial Intelligence, Ukraine, 2004, No 3. P. 598-606Shuklin D.E. The Structure of a Semantic Neural Network Extracting the Meaning from a Text, In Cybernetics and Systems Analysis, Volume 37, Number 2, 4 March 2001, pp. 182–186(5) [1]
Shuklin D.E. The Structure of a Semantic Neural Network Realizing Morphological and Syntactic Analysis of a Text, In Cybernetics and Systems Analysis, Volume 37, Number 5, September 2001, pp. 770–776(7)
Shuklin D.E. Realization of a Binary Clocked Linear Tree and Its Use for Processing Texts in Natural Languages, In Cybernetics and Systems Analysis, Volume 38, Number 4, July 2002, pp. 503–508(6)
Semantic spaces in the natural language domain aim to create representations of natural language that are capable of capturing meaning. The original motivation for semantic spaces stems from two core challenges of natural language: Vocabulary mismatch (the fact that the same meaning can be expressed in many ways) and ambiguity of natural language (the fact that the same term can have several meanings).
The application of semantic spaces in natural language processing (NLP) aims at overcoming limitations of rule-based or model-based approaches operating on the keyword level. The main drawback with these approaches is their brittleness, and the large manual effort required to create either rule-based NLP systems or training corpora for model learning. Rule-based and machine learning based models are fixed on the keyword level and break down if the vocabulary differs from that defined in the rules or from the training material used for the statistical models.
Research in semantic spaces dates back more than 20 years. In 1996, two papers were published that raised a lot of attention around the general idea of creating semantic spaces: latent semantic analysis and Hyperspace Analogue to Language. However, their adoption was limited by the large computational effort required to construct and use those semantic spaces. A breakthrough with regard to the accuracy of modelling associative relations between words (e.g. "spider-web", "lighter-cigarette", as opposed to synonymous relations such as "whale-dolphin", "astronaut-driver") was achieved by explicit semantic analysis (ESA) in 2007. ESA was a novel (non-machine learning) based approach that represented words in the form of vectors with 100,000 dimensions (where each dimension represents an Article in Wikipedia). However practical applications of the approach are limited due to the large number of required dimensions in the vectors.
More recently, advances in neural networking techniques in combination with other new approaches (tensors) led to a host of new recent developments: Word2vec from Google, GloVe from Stanford University, and fastText from Facebook AI Research (FAIR) labs.

See also
Word embedding
Semantic folding


== References ==
SemEval (Semantic Evaluation) is an ongoing series of evaluations of computational semantic analysis systems; it evolved from the Senseval word sense evaluation series. The evaluations are intended to explore the nature of meaning in language. While meaning is intuitive to humans, transferring those intuitions to computational analysis has proved elusive.
This series of evaluations is providing a mechanism to characterize in more precise terms exactly what is necessary to compute in meaning. As such, the evaluations provide an emergent mechanism to identify the problems and solutions for computations with meaning. These exercises have evolved to articulate more of the dimensions that are involved in our use of language. They began with apparently simple attempts to identify word senses computationally. They have evolved to investigate the interrelationships among the elements in a sentence (e.g., semantic role labeling), relations between sentences (e.g., coreference), and the nature of what we are saying (semantic relations and sentiment analysis).
The purpose of the SemEval and Senseval exercises is to evaluate semantic analysis systems. "Semantic Analysis" refers to a formal analysis of meaning, and "computational" refer to approaches that in principle support effective implementation.The first three evaluations, Senseval-1 through Senseval-3, were focused on word sense disambiguation, each time growing in the number of languages offered in the tasks and in the number of participating teams. Beginning with the fourth workshop, SemEval-2007 (SemEval-1), the nature of the tasks evolved to include semantic analysis tasks outside of word sense disambiguation.Triggered by the conception of the *SEM conference, the SemEval community had decided to hold the evaluation workshops yearly in association with the *SEM conference. It was also the decision that not every evaluation task will be run every year, e.g. none of the WSD tasks were included in the SemEval-2012 workshop.

History
Early evaluation of algorithms for word sense disambiguation
From the earliest days, assessing the quality of word sense disambiguation (WSD) algorithms had been primarily a matter of intrinsic evaluation, and “almost no attempts had been made to evaluate embedded WSD components”. Only very recently (2006) had extrinsic evaluations begun to provide some evidence for the value of WSD in end-user applications. Until 1990 or so, discussions of the sense disambiguation task focused mainly on illustrative examples rather than comprehensive evaluation. The early 1990s saw the beginnings of more systematic and rigorous intrinsic evaluations, including more formal experimentation on small sets of ambiguous words.

Senseval to SemEval
In April 1997, Martha Palmer and Marc Light organized a workshop entitled Tagging with Lexical Semantics: Why, What, and How? in conjunction with the Conference on Applied Natural Language Processing. At the time, there was a clear recognition that manually annotated corpora had revolutionized other areas of NLP, such as part-of-speech tagging and parsing, and that corpus-driven approaches had the potential to revolutionize automatic semantic analysis as well. Kilgarriff recalled that there was “a high degree of consensus that the field needed evaluation,” and several practical proposals by Resnik and Yarowsky kicked off a discussion that led to the creation of the Senseval evaluation exercises.

SemEval's 3, 2 or 1 year(s) cycle
After SemEval-2010, many participants feel that the 3-year cycle is a long wait. Many other shared tasks such as Conference on Natural Language Learning (CoNLL) and Recognizing Textual Entailments (RTE) run annually. For this reason, the SemEval coordinators gave the opportunity for task organizers to choose between a 2-year or a 3-year cycle. The SemEval community favored the 3-year cycle. 
Although the votes within the SemEval community favored a 3-year cycle, organizers and coordinators had settled to split the SemEval task into 2 evaluation workshops. This was triggered by the introduction of the new *SEM conference. The SemEval organizers thought it would be appropriate to associate our event with the *SEM conference and collocate the SemEval workshop with the *SEM conference. The organizers got very positive responses (from the task coordinators/organizers and participants) about the association with the yearly *SEM, and 8 tasks were willing to switch to 2012.  Thus was born SemEval-2012 and SemEval-2013.  The current plan is to switch to a yearly SemEval schedule to associate it with the *SEM conference but not every task needs to run every year.

List of Senseval and SemEval Workshops
Senseval-1 took place in the summer of 1998 for English, French, and Italian, culminating in a workshop held at Herstmonceux Castle, Sussex, England on September 2–4.
Senseval-2 took place in the summer of 2001, and was followed by a workshop held in July 2001 in Toulouse, in conjunction with ACL 2001. Senseval-2 included tasks for Basque, Chinese, Czech, Danish, Dutch, English, Estonian, Italian, Japanese, Korean, Spanish and Swedish.
Senseval-3 took place in March–April 2004, followed by a workshop held in July 2004 in Barcelona, in conjunction with ACL 2004. Senseval-3 included 14 different tasks for core word sense disambiguation, as well as identification of semantic roles, multilingual annotations, logic forms, subcategorization acquisition.
SemEval-2007 (Senseval-4) took place in 2007, followed by a workshop held in conjunction with ACL in Prague. SemEval-2007 included 18 different tasks targeting the evaluation of systems for the semantic analysis of text. A special issue of Language Resources and Evaluation is devoted to the result.
SemEval-2010 took place in 2010, followed by a workshop held in conjunction with ACL in Uppsala. SemEval-2010 included 18 different tasks targeting the evaluation of semantic analysis systems.
SemEval-2012 took place in 2012; it was associated with the new *SEM, First Joint Conference on Lexical and Computational Semantics, and co-located with NAACL, Montreal, Canada. SemEval-2012 included 8 different tasks targeting at evaluating computational semantic systems. However, there was no WSD task involved in SemEval-2012, the WSD related tasks were scheduled in the upcoming SemEval-2013.
SemEval-2013 was associated with NAACL 2013, North American Association of Computational Linguistics, Georgia, USA and took place in 2013. It included 13 different tasks targeting at evaluating computational semantic systems.
SemEval-2014 took place in 2014. It was co-located with COLING 2014, 25th International Conference on Computational Linguistics and *SEM 2014, Second Joint Conference on Lexical and Computational Semantics, Dublin, Ireland. There were 10 different tasks in SemEval-2014 evaluating various computational semantic systems.
SemEval-2015 took place in 2015. It was co-located with NAACL-HLT 2015, 2015 Conference of the North American Chapter of the Association for Computational Linguistics – Human Language Technologies and *SEM 2015, Third Joint Conference on Lexical and Computational Semantics, Denver, USA. There were 17 different tasks in SemEval-2015 evaluating various computational semantic systems.

SemEval Workshop framework
The framework of the SemEval/Senseval evaluation workshops emulates the Message Understanding Conferences (MUCs) and other evaluation workshops ran by ARPA (Advanced Research Projects Agency, renamed the Defense Advanced Research Projects Agency (DARPA)). 

Stages of SemEval/Senseval evaluation workshops
Firstly, all likely participants were invited to express their interest and participate in the exercise design.
A timetable towards a final workshop was worked out.
A plan for selecting evaluation materials was agreed.
'Gold standards' for the individual tasks were acquired, often human annotators were considered as a gold standard to measure precision and recall scores of computer systems. These 'gold standards' are what the computational systems strive towards. In WSD tasks, human annotators were set on the task of generating a set of correct WSD answers (i.e. the correct sense for a given word in a given context)
The gold standard materials, without answers, were released to participants, who then had a short time to run their programs over them and return their sets of answers to the organizers.
The organizers then scored the answers and the scores were announced and discussed at a workshop.

Semantic evaluation tasks
Senseval-1 & Senseval-2 focused on evaluation WSD systems on major languages that were available corpus and computerized dictionary. Senseval-3 looked beyond the lexemes and started to evaluate systems that looked into wider areas of semantics, such as Semantic Roles (technically known as Theta roles in formal semantics), Logic Form Transformation (commonly semantics of phrases, clauses or sentences were represented in first-order logic forms) and Senseval-3 explored performances of semantics analysis on Machine translation.
As the types of different computational semantic systems grew beyond the coverage of WSD, Senseval evolved into SemEval, where more aspects of computational semantic systems were evaluated.

Overview of Issues in Semantic Analysis
The SemEval exercises provide a mechanism for examining issues in semantic analysis of texts. The topics of interest fall short of the logical rigor that is found in formal computational semantics, attempting to identify and characterize the kinds of issues relevant to human understanding of language. The primary goal is to replicate human processing by means of computer systems. The tasks (shown below) are developed by individuals and groups to deal with identifiable issues, as they take on some concrete form.
The first major area in semantic analysis is the identification of the intended meaning at the word level (taken to include idiomatic expressions). This is word-sense disambiguation (a concept that is evolving away from the notion that words have discrete senses, but rather are characterized by the ways in which they are used, i.e., their contexts). The tasks in this area include lexical sample and all-word disambiguation, multi- and cross-lingual disambiguation, and lexical substitution. Given the difficulties of identifying word senses, other tasks relevant to this topic include word-sense induction, subcategorization acquisition, and evaluation of lexical resources.
The second major area in semantic analysis is the understanding of how different sentence and textual elements fit together. Tasks in this area include semantic role labeling, semantic relation analysis, and coreference resolution. Other tasks in this area look at more specialized issues of semantic analysis, such as temporal information processing, metonymy resolution, and sentiment analysis. The tasks in this area have many potential applications, such as information extraction, question answering, document summarization, machine translation, construction of thesauri and semantic networks, language modeling, paraphrasing,
and recognizing textual entailment. In each of these potential applications, the contribution of the types of semantic analysis constitutes the most outstanding research issue.
For example, in the word sense induction and disambiguation task, there are three separate phases:

In the training phase, evaluation task participants were asked to use a training dataset to induce the sense inventories for a set of polysemous words. The training dataset consisting of a set of polysemous nouns/verbs and the sentence instances that they occurred in. No other resources were allowed other than morphological and syntactic Natural Language Processing components, such as morphological analyzers, Part-Of-Speech taggers and syntactic parsers.
In the testing phase, participants were provided with a test set for the disambiguating subtask using the induced sense inventory from the training phase.
In the evaluation phase, answers of to the testing phase were evaluated in a supervised an unsupervised framework.The unsupervised evaluation for WSI considered two types of evaluation V Measure (Rosenberg and Hirschberg, 2007), and paired F-Score (Artiles et al., 2009). This evaluation follows the supervised evaluation of SemEval-2007 WSI task (Agirre and Soroa, 2007)

Senseval and SemEval tasks overview
The tables below reflects the workshop growth from Senseval to SemEval and gives an overview of which area of computational semantics was evaluated throughout the Senseval/SemEval workshops.

The Multilingual WSD task was introduced for the SemEval-2013 workshop. The task is aimed at evaluating Word Sense Disambiguation systems in a multilingual scenario using BabelNet as its sense inventory. Unlike similar task like crosslingual WSD or the multilingual lexical substitution task, where no fixed sense inventory is specified, Multilingual WSD uses the BabelNet as its sense inventory. Prior to the development of BabelNet, a bilingual lexical sample WSD evaluation task was carried out in SemEval-2007 on Chinese-English bitexts.The Cross-lingual WSD task was introduced in the SemEval-2007 evaluation workshop and re-proposed in the SemEval-2013 workshop 
.  To facilitate the ease of integrating WSD systems into other Natural Language Processing (NLP) applications, such as Machine Translation and multilingual Information Retrieval, the cross-lingual WSD evaluation task was introduced a language-independent and knowledge-lean approach to WSD. The task is an unsupervised Word Sense Disambiguation task for English nouns by means of parallel corpora. It follows the lexical-sample variant of the Classic WSD task, restricted to only 20 polysemous nouns.
It is worth noting that the SemEval-2014 have only two tasks that were multilingual/crosslingual, i.e. (i) the L2 Writing Assistant task, which is a crosslingual WSD task that includes English, Spanish, German, French and Dutch and (ii) the Multilingual Semantic Textual Similarity task that evaluates systems on English and Spanish texts.

Areas of evaluation
The major tasks in semantic evaluation include the following areas of natural language processing. This list is expected to grow as the field progresses.The following table shows the areas of studies that were involved in Senseval-1 through SemEval-2014 (S refers to Senseval and SE refers to SemEval, e.g. S1 refers to Senseval-1 and SE07 refers to SemEval2007):

Type of Semantic Annotations
SemEval tasks have created many types of semantic annotations, each type with various schema. In SemEval-2015, the organizers have decided to group tasks together into several tracks. These tracks are by the type of semantic annotations that the task hope to achieve. Here lists the type of semantic annotations involved in the SemEval workshops:

Learning Semantic Relations
Question and Answering
Semantic Parsing
Semantic Taxonomy
Sentiment Analysis
Text Similarity
Time and Space
Word Sense Disambiguation and InductionA task and its track allocation is flexible; a task might develop into its own track, e.g. the taxonomy evaluation task in SemEval-2015 was under the Learning Semantic Relations track and in SemEval-2016, there is a dedicated track for Semantic Taxonomy with a new Semantic Taxonomy Enrichment task.

See also
List of computer science awards
Computational semantics
Natural language processing
Word sense
Word sense disambiguation
Different variants of WSD evaluations
Semantic analysis (computational)

References
External links
Special Interest Group on the Lexicon (SIGLEX)  of the Association for Computational Linguistics (ACL)
Semeval-2010 - Semantic Evaluation Workshop (endorsed by SIGLEX)
Senseval - international organization devoted to the evaluation of Word Sense Disambiguation Systems (endorsed by SIGLEX)
SemEval Portal on the Wiki of the Association for Computational Linguistics
Senseval / SemEval tasks:
Senseval-1 - the first evaluation exercise on word sense disambiguation systems; the lexical-sample task was evaluated on English, French and Italian
Senseval-2 - evaluated word sense disambiguation systems on three types of tasks (the all-words, lexical-sample and the translation task)
Senseval-3  - included tasks for word sense disambiguation, as well as identification of semantic roles, multilingual annotations, logic forms, subcategorization acquisition.
SemEval-2007 - included tasks which were more elaborate than Senseval as it crosses the different areas of studies in Natural Language Processing
SemEval-2010 - added tasks that were from new areas of studies in computational semantics, viz., Coreference, Elipsis, Keyphrase Extraction, Noun Compounds and Textual Entailment.
SemEval-2012  - co-located with the first *SEM conference and the Semantic Similarity Task was being promoted as the  *Sem Shared Task
SemEval-2013  - SemEval moved from 2–3 years cycle to an annual workshop
SemEval-2014  - the first time SemEval is located in a non-ACL event in COLING
SemEval-2015  - the first SemEval with tasks categorized into various tracks
SemEval-2016  - the second SemEval without a WSD task (the first was in SemEval-2012)
*SEM- conference for SemEval-related papers other than task systems.
Message Understanding Conferences (MUCs)
BabelNet
Open Multilingual WordNet - Compilation of WordNets with Open licenses
Sentence embedding is the collective name for a set of techniques in natural language processing (NLP) where sentences are mapped to vectors of real numbers

.

Application
Sentence embedding is used by the machine learning software libraries PyTorch and TensorFlow

Evaluation
A way of testing sentence encodings is to apply them on Sentences Involving Compositional Knowledge (SICK) corpus
for both entailment (SICK-E) and relatedness (SICK-R).
In  the best results are obtained using a BiLSTM network trained on the Stanford Natural Language Inference (SNLI) Corpus. The Pearson correlation coefficient for SICK-R is 0.885 and the result for SICK-E is 86.3. A slight improvement over previous scores is presented in : SICK-R: 0.888 and SICK-E: 87.8 using a concatenation of bidirectional Gated recurrent unit.

See also
Distributional semantics
Word embedding

External links
InferSent sentence embeddings and training code
Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning


== References ==
Sentence extraction is a technique used for automatic summarization of a text.
In this shallow approach, statistical heuristics are used to identify the most salient sentences of a text. Sentence extraction is a low-cost approach compared to more knowledge-intensive deeper approaches which require additional knowledge bases such as ontologies or linguistic knowledge. In short "sentence extraction" works as a filter which allows only important sentences to pass.
The major downside of applying sentence-extraction techniques to the task of summarization is the loss of coherence in the resulting summary. 
Nevertheless, sentence extraction summaries can give valuable clues to the main points of a document and are frequently sufficiently intelligible to human readers.

Procedure
Usually, a combination of heuristics is used to determine the most important sentences within the document. Each heuristic assigns a (positive or negative) score to the sentence. After all heuristics have been applied, the highest-scoring sentences are included in the summary.
The individual heuristics are weighted according to their importance.

Early approaches and some sample heuristics
Seminal papers which laid the foundations for many techniques used today have been published by Hans Peter Luhn in 1958 and H. P Edmundson in 1969.Luhn proposed to assign more weight to sentences at the beginning of the document or a paragraph.
Edmundson stressed the importance of title-words for summarization and was the first to employ stop-lists in order to filter uninformative words of low semantic content (e.g. most grammatical words such as "of", "the", "a"). He also distinguished between bonus words and stigma words, i.e. words that probably occur together with important (e.g. the word form "significant") or unimportant information.
His idea of using key-words, i.e. words which occur significantly frequently in the document, is still one of the core heuristics of today's summarizers. With large linguistic corpora available today, the tf–idf value which originated in information retrieval, can be successfully applied to identify the key words of a text: If for example the word "cat" occurs significantly more often in the text to be summarized (TF = "term frequency") than in the corpus (IDF means "inverse document frequency"; here the corpus is meant by "document"), then "cat" is likely to be an important word of the text; the text may in fact be a text about cats.

See also
Text segmentation
Sentence boundary disambiguation


== References ==
Sentiment analysis (also known as opinion mining or emotion AI) refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine.

Examples
The objective and challenges of sentiment analysis can be shown through some simple examples.

Simple cases
Coronet has the best lines of all day cruisers.
Bertram has a deep V hull and runs easily through seas.
Pastel-colored 1980s day cruisers from Florida are ugly.
I dislike old cabin cruisers.

More challenging examples
I do not dislike cabin cruisers. (Negation handling)
Disliking watercraft is not really my thing. (Negation, inverted word order)
Sometimes I really hate RIBs. (Adverbial modifies the sentiment)
I'd really truly love going out in this weather! (Possibly sarcastic)
Chris Craft is better looking than Limestone. (Two brand names, identifying the target of attitude is difficult).
Chris Craft is better looking than Limestone, but Limestone projects seaworthiness and reliability. (Two attitudes, two brand names).
The movie is surprising with plenty of unsettling plot twists. (Negative term used in a positive sense in certain domains).
You should see their decadent dessert menu. (Attitudinal term has shifted polarity recently in certain domains)
I love my mobile but would not recommend it to any of my colleagues. (Qualified positive sentiment, difficult to categorise)
Next week's gig will be right koide9! ("Quoi de neuf?" Fr.: "what's new?". Newly minted terms can be highly attitudinal but volatile in polarity and often out of known vocabulary.)

Types
A basic task in sentiment analysis is classifying the polarity of a given text at the document, sentence, or feature/aspect level—whether the expressed opinion in a document, a sentence or an entity feature/aspect is positive, negative, or neutral. Advanced, "beyond polarity" sentiment classification looks, for instance, at emotional states such as "angry", "sad", and "happy".Precursors to sentimental analysis include the General Inquirer, which provided hints toward quantifying patterns in text and, separately, psychological research that examined a person's psychological state based on analysis of their verbal behavior.Subsequently, the method described in a patent by Volcani and Fogel, looked specifically at sentiment and identified individual words and phrases in text with respect to different emotional scales. A current system based on their work, called EffectCheck, presents synonyms that can be used to increase or decrease the level of evoked emotion in each scale.
Many other subsequent efforts were less sophisticated, using a mere polar view of sentiment, from positive to negative, such as work by Turney, and Pang who applied different methods for detecting the polarity of product reviews and movie reviews respectively. This work is at the document level. One can also classify a document's polarity on a multi-way scale, which was attempted by Pang and Snyder among others: Pang and Lee expanded the basic task of classifying a movie review as either positive or negative to predict star ratings on either a 3- or a 4-star scale, while Snyder performed an in-depth analysis of restaurant reviews, predicting ratings for various aspects of the given restaurant, such as the food and atmosphere (on a five-star scale).
First steps to bringing together various approaches—learning, lexical, knowledge-based, etc.—were taken in the 2004 AAAI Spring Symposium where linguists, computer scientists, and other interested researchers first aligned interests and proposed shared tasks and benchmark data sets for the systematic computational research on affect, appeal, subjectivity, and sentiment in text.Even though in most statistical classification methods, the neutral class is ignored under the assumption that neutral texts lie near the boundary of the binary classifier, several researchers suggest that, as in every polarity problem, three categories must be identified. Moreover, it can be proven that specific classifiers such as the Max Entropy and SVMs can benefit from the introduction of a neutral class and improve the overall accuracy of the classification. There are in principle two ways for operating with a neutral class. Either, the algorithm proceeds by first identifying the neutral language, filtering it out and then assessing the rest in terms of positive and negative sentiments, or it builds a three-way classification in one step. This second approach often involves estimating a probability distribution over all categories (e.g. naive Bayes classifiers as implemented by the NLTK). Whether and how to use a neutral class depends on the nature of the data: if the data is clearly clustered into neutral, negative and positive language, it makes sense to filter the neutral language out and focus on the polarity between positive and negative sentiments. If, in contrast, the data are mostly neutral with small deviations towards positive and negative affect, this strategy would make it harder to clearly distinguish between the two poles.
A different method for determining sentiment is the use of a scaling system whereby words commonly associated with having a negative, neutral, or positive sentiment with them are given an associated number on a −10 to +10 scale (most negative up to most positive) or simply from 0 to a positive upper limit such as +4. This makes it possible to adjust the sentiment of a given term relative to its environment (usually on the level of the sentence). When a piece of unstructured text is analyzed using natural language processing, each concept in the specified environment is given a score based on the way sentiment words relate to the concept and its associated score. This allows movement to a more sophisticated understanding of sentiment, because it is now possible to adjust the sentiment value of a concept relative to modifications that may surround it. Words, for example, that intensify, relax or negate the sentiment expressed by the concept can affect its score. Alternatively, texts can be given a positive and negative sentiment strength score if the goal is to determine the sentiment in a text rather than the overall polarity and strength of the text.There are various other types of sentiment analysis like- Aspect Based sentiment analysis, Grading sentiment analysis (positive,negative,neutral), Multilingual sentiment analysis and detection of emotions.

Subjectivity/objectivity identification
This task is commonly defined as classifying a given text (usually a sentence) into one of two classes: objective or subjective. This problem can sometimes be more difficult than polarity classification. The subjectivity of words and phrases may depend on their context and an objective document may contain subjective sentences (e.g., a news article quoting people's opinions). Moreover, as mentioned by Su, results are largely dependent on the definition of subjectivity used when annotating texts. However, Pang showed that removing objective sentences from a document before classifying its polarity helped improve performance.

Feature/aspect-based
It refers to determining the opinions or sentiments expressed on different features or aspects of entities, e.g., of a cell phone, a digital camera, or a bank. A feature or aspect is an attribute or component of an entity, e.g., the screen of a cell phone, the service for a restaurant, or the picture quality of a camera. The advantage of feature-based sentiment analysis is the possibility to capture nuances about objects of interest. Different features can generate different sentiment responses, for example a hotel can have a convenient location, but mediocre food. This problem involves several sub-problems, e.g., identifying relevant entities, extracting their features/aspects, and determining whether an opinion expressed on each feature/aspect is positive, negative or neutral. The automatic identification of features can be performed with syntactic methods, with topic modeling, or with deep learning. More detailed discussions about this level of sentiment analysis can be found in Liu's work.

Methods and features
Existing approaches to sentiment analysis can be grouped into three main categories: knowledge-based techniques, statistical methods, and hybrid approaches. Knowledge-based techniques classify text by affect categories based on the presence of unambiguous affect words such as happy, sad, afraid, and bored. Some knowledge bases not only list obvious affect words, but also assign arbitrary words a probable "affinity" to particular emotions. Statistical methods leverage elements from machine learning such as latent semantic analysis, support vector machines, "bag of words", "Pointwise Mutual Information" for Semantic Orientation, and deep learning. More sophisticated methods try to detect the holder of a sentiment (i.e., the person who maintains that affective state) and the target (i.e., the entity about which the affect is felt). To mine the opinion in context and get the feature about which the speaker has opined, the grammatical relationships of words are used. Grammatical dependency relations are obtained by deep parsing of the text. Hybrid approaches leverage both machine learning and elements from knowledge representation such as ontologies and semantic networks in order to detect semantics that are expressed in a subtle manner, e.g., through the analysis of concepts that do not explicitly convey relevant information, but which are implicitly linked to other concepts that do so.Open source software tools as well as range of free and paid sentiment analysis tools deploy machine learning, statistics, and natural language processing techniques to automate sentiment analysis on large collections of texts, including web pages, online news, internet discussion groups, online reviews, web blogs, and social media. Knowledge-based systems, on the other hand, make use of publicly available resources, to extract the semantic and affective information associated with natural language concepts. Sentiment analysis can also be performed on visual content, i.e., images and videos (see Multimodal sentiment analysis). One of the first approaches in this direction is SentiBank utilizing an adjective noun pair representation of visual content. In addition, the vast majority of sentiment classification approaches rely on the bag-of-words model, which disregards context, grammar and even word order. Approaches that analyses the sentiment based on how words compose the meaning of longer phrases have shown better result, but they incur an additional annotation overhead.
A human analysis component is required in sentiment analysis, as automated systems are not able to analyze historical tendencies of the individual commenter, or the platform and are often classified incorrectly in their expressed sentiment. Automation impacts approximately 23% of comments that are correctly classified by humans. However, humans often disagree, and it is argued that the inter-human agreement provides an upper bound that automated sentiment classifiers can eventually reach.Sometimes, the structure of sentiments and topics is fairly complex. Also, the problem of sentiment analysis is non-monotonic in respect to sentence extension and stop-word substitution (compare THEY would not let my dog stay in this hotel vs I would not let my dog stay in this hotel). To address this issue a number of rule-based and reasoning-based approaches have been applied to sentiment analysis, including defeasible logic programming. Also, there is a number of tree traversal rules applied to syntactic parse tree to extract the topicality of sentiment in open domain setting.

Evaluation
The accuracy of a sentiment analysis system is, in principle, how well it agrees with human judgments. This is usually measured by variant measures based on precision and recall over the two target categories of negative and positive texts. However, according to research human raters typically only agree about 80% of the time (see Inter-rater reliability). Thus, a program that achieves 70% accuracy in classifying sentiment is doing nearly as well as humans, even though such accuracy may not sound impressive. If a program were "right" 100% of the time, humans would still disagree with it about 20% of the time, since they disagree that much about any answer.On the other hand, computer systems will make very different errors than human assessors, and thus the figures are not entirely comparable. For instance, a computer system will have trouble with negations, exaggerations, jokes, or sarcasm, which typically are easy to handle for a human reader: some errors a computer system makes will seem overly naive to a human. In general, the utility for practical commercial tasks of sentiment analysis as it is defined in academic research has been called into question, mostly since the simple one-dimensional model of sentiment from negative to positive yields rather little actionable information for a client worrying about the effect of public discourse on e.g. brand or corporate reputation.To better fit market needs, evaluation of sentiment analysis has moved to more task-based measures, formulated together with representatives from PR agencies and market research professionals. The focus in e.g. the RepLab evaluation data set is less on the content of the text under consideration and more on the effect of the text in question on brand reputation.Because evaluation of sentiment analysis is becoming more and more task based, each implementation needs a separate training model to get a more accurate representation of sentiment for a given data set.

Web 2.0
The rise of social media such as blogs and social networks has fueled interest in sentiment analysis.  With the proliferation of reviews, ratings, recommendations and other forms of online expression, online opinion has turned into a kind of virtual currency for businesses looking to market their products, identify new opportunities and manage their reputations.  As businesses look to automate the process of filtering out the noise, understanding the conversations, identifying the relevant content and actioning it appropriately, many are now looking to the field of sentiment analysis. Further complicating the matter, is the rise of anonymous social media platforms such as 4chan and Reddit. If web 2.0 was all about democratizing publishing, then the next stage of the web may well be based on democratizing data mining of all the content that is getting published.One step towards this aim is accomplished in research. Several research teams in universities around the world currently focus on understanding the dynamics of sentiment in e-communities through sentiment analysis. The CyberEmotions project, for instance, recently identified the role of negative emotions in driving social networks discussions.The problem is that most sentiment analysis algorithms use simple terms to express sentiment about a product or service.  However, cultural factors, linguistic nuances, and differing contexts make it extremely difficult to turn a string of written text into a simple pro or con sentiment.  The fact that humans often disagree on the sentiment of text illustrates how big a task it is for computers to get this right.  The shorter the string of text, the harder it becomes.
Even though short text strings might be a problem, sentiment analysis within microblogging has shown that Twitter can be seen as a valid online indicator of political sentiment. Tweets' political sentiment demonstrates close correspondence to parties' and politicians' political positions, indicating that the content of Twitter messages plausibly reflects the offline political landscape. Furthermore, sentiment analysis on Twitter has also been shown to capture the public mood behind human reproduction cycles on a planetary scale, as well as other problems of public-health relevance such as adverse drug reactions.

Application in recommender systems
For a recommender system, sentiment analysis has been proven to be a valuable technique. A recommender system aims to predict the preference for an item of a target user. Mainstream recommender systems work on explicit data set. For example, collaborative filtering works on the rating matrix, and content-based filtering works on the meta-data of the items.
In many social networking services or e-commerce websites, users can provide text review, comment or feedback to the items. These user-generated text provide a rich source of user's sentiment opinions about numerous products and items. Potentially, for an item, such text can reveal both the related feature/aspects of the item and the users' sentiments on each feature. The item's feature/aspects described in the text play the same role with the meta-data in content-based filtering, but the former are more valuable for the recommender system. Since these features are broadly mentioned by users in their reviews, they can be seen as the most crucial features that can significantly influence the user's experience on the item, while the meta-data of the item (usually provided by the producers instead of consumers) may ignore features that are concerned by the users. For different items with common features, a user may give different sentiments. Also, a feature of the same item may receive different sentiments from different users. Users' sentiments on the features can be regarded as a multi-dimensional rating score, reflecting their preference on the items.
Based on the feature/aspects and the sentiments extracted from the user-generated text, a hybrid recommender system can be constructed. There are two types of motivation to recommend a candidate item to a user. The first motivation is the candidate item have numerous common features with the user's preferred items, while the second motivation is that the candidate item receives a high sentiment on its features. For a preferred item, it is reasonable to believe that items with the same features will have a similar function or utility. So, these items will also likely to be preferred by the user. On the other hand, for a shared feature of two candidate items, other users may give positive sentiment to one of them while giving negative sentiment to another. Clearly, the high evaluated item should be recommended to the user. Based on these two motivations, a combination ranking score of similarity and sentiment rating can be constructed for each candidate item.Except for the difficulty of the sentiment analysis itself, applying sentiment analysis on reviews or feedback also faces the challenge of spam and biased reviews. One direction of work is focused on evaluating the helpfulness of each review. Review or feedback poorly written is hardly helpful for recommender system. Besides, a review can be designed to hinder sales of a target product, thus be harmful to the recommender system even it is well written.
Researchers also found that long and short forms of user-generated text should be treated differently. An interesting result shows that short-form reviews are sometimes more helpful than long-form, because it is easier to filter out the noise in a short-form text. For the long-form text, the growing length of the text does not always bring a proportionate increase in the number of features or sentiments in the text.
Lamba & Madhusudhan  introduce a nascent way to cater the information needs of today’s library users by repackaging the results from sentiment analysis of social media platforms like Twitter and provide it as a consolidated time-based service in different formats. Further, they propose a new way of conducting marketing in libraries using social media mining and sentiment analysis.

See also
Emotion recognition
Market sentiment
Behavioral analysis of markets


== References ==
Speech segmentation is the process of identifying the boundaries between words, syllables, or phonemes in spoken natural languages. The term applies both to the mental processes used by humans, and to artificial processes of natural language processing.
Speech segmentation is a subfield of general speech perception and an important subproblem of the technologically focused field of speech recognition, and cannot be adequately solved in isolation. As in most natural language processing problems, one must take into account context, grammar, and semantics, and even so the result is often a probabilistic division (statistically based on likelihood) rather than a categorical one. Though it seems that coarticulation—a phenomenon which may happen between adjacent words just as easily as within a single word—presents the main challenge in speech segmentation across languages, some other problems and strategies employed in solving those problems can be seen in the following sections.
This problem overlaps to some extent with the problem of text segmentation that occurs in some languages which are traditionally written without inter-word spaces, like Chinese and Japanese, compared to writing systems which indicate speech segmentation between words by a word divider, such as the space. However, even for those languages, text segmentation is often much easier than speech segmentation, because the written language usually has little interference between adjacent words, and often contains additional clues not present in speech (such as the use of Chinese characters for word stems in Japanese). Word Boundary Identification can be overcome by NLU approaches such as Patom theory integrated with Role and Reference Grammar (RRG) for languages without spaces between words such as Japanese and Chinese.

Lexical recognition
In natural languages, the meaning of a complex spoken sentence can be understood by decomposing it into smaller lexical segments (roughly, the words of the language), associating a meaning to each segment, and combining those meanings according to the grammar rules of the language.
Though lexical recognition is not thought to be used by infants in their first year, due to their highly limited vocabularies, it is one of the major processes involved in speech segmentation for adults. Three main models of lexical recognition exist in current research: first, whole-word access, which argues that words have a whole-word representation in the lexicon; second, decomposition, which argues that morphologically complex words are broken down into their morphemes (roots, stems, inflections, etc.) and then interpreted and; third, the view that whole-word and decomposition models are both used, but that the whole-word model provides some computational advantages and is therefore dominant in lexical recognition.To give an example, in a whole-word model, the word "cats" might be stored and searched for by letter, first "c", then "ca", "cat", and finally "cats". The same word, in a decompositional model, would likely be stored under the root word "cat" and could be searched for after removing the "s" suffix. "Falling", similarly, would be stored as "fall" and suffixed with the "ing" inflection.Though proponents of the decompositional model recognize that a morpheme-by-morpheme analysis may require significantly more computation, they argue that the unpacking of morphological information is necessary for other processes (such as syntactic structure) which may occur parallel to lexical searches.
As a whole, research into systems of human lexical recognition is limited due to little experimental evidence that fully discriminates between the three main models.In any case, lexical recognition likely contributes significantly to speech segmentation through the contextual clues it provides, given that it is a heavily probabilistic system—based on the statistical likelihood of certain words or constituents occurring together. For example, one can imagine a situation where a person might say "I bought my dog at a ____ shop" and the missing word's vowel is pronounced as in "net", "sweat", or "pet". While the probability of "netshop" is extremely low, since "netshop" isn't currently a compound or phrase in English, and "sweatshop" also seems contextually improbable, "pet shop" is a good fit because it is a common phrase and is also related to the word "dog".Moreover, an utterance can have different meanings depending on how it is split into words. A popular example, often quoted in the field, is the phrase "How to wreck a nice beach", which sounds very similar to "How to recognize speech". As this example shows, proper lexical segmentation depends on context and semantics which draws on the whole of human knowledge and experience, and would thus require advanced pattern recognition and artificial intelligence technologies to be implemented on a computer.
Lexical recognition is of particular value in the field of computer speech recognition, since the ability to build and search a network of semantically connected ideas would greatly increase the effectiveness of speech-recognition software. Statistical models can be used to segment and align recorded speech to words or phones. Applications include automatic lip-synch timing for cartoon animation, follow-the-bouncing-ball video sub-titling, and linguistic research. Automatic segmentation and alignment software is commercially available.

Phonotactic cues
For most spoken languages, the boundaries between lexical units are difficult to identify; phonotactics are one answer to this issue. One might expect that the inter-word spaces used by many written languages like English or Spanish would correspond to pauses in their spoken version, but that is true only in very slow speech, when the speaker deliberately inserts those pauses. In normal speech, one typically finds many consecutive words being said with no pauses between them, and often the final sounds of one word blend smoothly or fuse with the initial sounds of the next word.
The notion that speech is produced like writing, as a sequence of distinct vowels and consonants, may be a relic of alphabetic heritage for some language communities. In fact, the way vowels are produced depends on the surrounding consonants just as consonants are affected by surrounding vowels; this is called coarticulation. For example, in the word "kit", the [k] is farther forward than when we say 'caught'. But also, the vowel in "kick" is phonetically different from the vowel in "kit", though we normally do not hear this. In addition, there are language-specific changes which occur in casual speech which makes it quite different from spelling. For example, in English, the phrase "hit you" could often be more appropriately spelled "hitcha".
From a decompositional perspective, in many cases, phonotactics play a part in letting speakers know where to draw word boundaries. In English, the word "strawberry" is perceived by speakers as consisting (phonetically) of two parts: "straw" and "berry". Other interpretations such as "stra" and "wberry" are inhibited by English phonotactics, which does not allow the cluster "wb" word-initially. Other such examples are "day/dream" and "mile/stone" which are unlikely to be interpreted as "da/ydream" or "mil/estone" due to the phonotactic probability or improbability of certain clusters. The sentence "Five women left", which could be phonetically transcribed as [faɪvwɪmɘnlɛft], is marked since neither /vw/ in /faɪvwɪmɘn/ or /nl/ in /wɪmɘnlɛft/ are allowed as syllable onsets or codas in English phonotactics. These phonotactic cues often allow speakers to easily distinguish the boundaries in words.
Vowel harmony in languages like Finnish can also serve to provide phonotactic cues. While the system does not allow front vowels and back vowels to exist together within one morpheme, compounds allow two morphemes to maintain their own vowel harmony while coexisting in a word. Therefore, in compounds such as "selkä/ongelma" ('back problem') where vowel harmony is distinct between two constituents in a compound, the boundary will be wherever the switch in harmony takes place—between the "ä" and the "ö" in this case. Still, there are instances where phonotactics may not aid in segmentation. Words with unclear clusters or uncontrasted vowel harmony as in "opinto/uudistus" ('student reform') do not offer phonotactic clues as to how they are segmented.From the perspective of the whole-word model, however, these words are thought be stored as full words, so the constituent parts wouldn't necessarily be relevant to lexical recognition.

Speech segmentation in infants and non-natives
Infants are one major focus of research in speech segmentation. Since infants have not yet acquired a lexicon capable of providing extensive contextual clues or probability-based word searches within their first year, as mentioned above, they must often rely primarily upon phonotactic and rhythmic cues (with prosody being the dominant cue), all of which are language-specific. Between 6 and 9 months, infants begin to lose the ability to discriminate between sounds not present in their native language and grow sensitive to the sound structure of their native language, with the word segmentation abilities appearing around 7.5 months.
Though much more research needs to be done on the exact processes that infants use to begin speech segmentation, current and past studies suggest that English-native infants approach stressed syllables as the beginning of words. At 7.5 months, infants appear to be able to segment bisyllabic words with strong-weak stress patterns, though weak-strong stress patterns are often misinterpreted, e.g. interpreting "guiTAR is" as "GUI TARis". It seems that infants also show some complexity in tracking frequency and probability of words, for instance, recognizing that although the syllables "the" and "dog" occur together frequently, "the" also commonly occurs with other syllables, which may lead to the analysis that "dog" is an individual word or concept instead of the interpretation "thedog".Language learners are another set of individuals being researched within speech segmentation. In some ways, learning to segment speech may be more difficult for a second-language learner than for an infant, not only in the lack of familiarity with sound probabilities and restrictions but particularly in the overapplication of the native language's patterns. While some patterns may occur between languages, as in the syllabic segmentation of French and English, they may not work well with languages such as Japanese, which has a mora-based segmentation system. Further, phonotactic restrictions like the boundary-marking cluster /ld/ in German or Dutch are permitted (without necessarily marking boundaries) in English. Even the relationship between stress and vowel length, which may seem intuitive to speakers of English, may not exist in other languages, so second-language learners face an especially great challenge when learning a language and its segmentation cues.

See also
Ambiguity
Speech recognition
Speech processing
Hyphenation
Mondegreen
Speech perception
Sentence boundary disambiguation

References
External links
"Phonolyze" speech segmentation software
SPPAS - the automatic annotation and analysis of speech
In software, a spell checker (or spell check) is a software feature that checks for misspellings in a text. Spell-checking features are often embedded in software or services, such as a word processor, email client, electronic dictionary, or search engine.

Design
A basic spell checker carries out the following processes:

It scans the text and extracts the words contained in it.
It then compares each word with a known list of correctly spelled words (i.e. a dictionary). This might contain just a list of words, or it might also contain additional information, such as hyphenation points or lexical and grammatical attributes.
An additional step is a language-dependent algorithm for handling morphology. Even for a lightly inflected language like English, the spell-checker will need to consider different forms of the same word, such as plurals, verbal forms, contractions, and possessives. For many other languages, such as those featuring agglutination and more complex declension and conjugation, this part of the process is more complicated.It is unclear whether morphological analysis—allowing for many different forms of a word depending on its grammatical role—provides a significant benefit for English, though its benefits for highly synthetic languages such as German, Hungarian or Turkish are clear.
As an adjunct to these components, the program's user interface will allow users to approve or reject replacements and modify the program's operation.
An alternative type of spell checker uses solely statistical information, such as n-grams, to recognize errors instead of correctly-spelled words. This approach usually requires a lot of effort to obtain sufficient statistical information. Key advantages include needing less runtime storage and the ability to correct errors in words that are not included in a dictionary.In some cases spell checkers use a fixed list of misspellings and suggestions for those misspellings; this less flexible approach is often used in paper-based correction methods, such as the see also entries of encyclopedias.
Clustering algorithms have also been used for spell checking combined with phonetic information.

History
In 1961, Les Earnest, who headed the research on this budding technology, saw it necessary to include the first spell checker that accessed a list of 10,000 acceptable words. Ralph Gorin, a graduate student under Earnest at the time, created the first true spelling checker program written as an applications program (rather than research) for general English text: SPELL for the DEC PDP-10 at Stanford University's Artificial Intelligence Laboratory, in February 1971.  Gorin wrote SPELL in assembly language, for faster action; he made the first spelling corrector by searching the word list for plausible correct spellings that differ by a single letter or adjacent letter transpositions and presenting them to the user.  Gorin made SPELL publicly accessible, as was done with most SAIL (Stanford Artificial Intelligence Laboratory) programs, and it soon spread around the world via the new ARPAnet, about ten years before personal computers came into general use.   SPELL, its algorithms and data structures inspired the Unix ispell program.
The first spell checkers were widely available on mainframe computers in the late 1970s.  A group of six linguists from Georgetown University developed the first spell-check system for the IBM corporation. Henry Kučera invented one for the VAX machines of Digital Equipment Corp in 1981.The first spell checkers for personal computers appeared in 1980, such as "WordCheck" for Commodore systems which was released in late 1980 in time for advertisements to go to print in January 1981. Developers such as Maria Mariani and Random House rushed OEM packages or end-user products into the rapidly expanding software market, primarily for the PC but also for Apple Macintosh, VAX, and Unix. On the PCs, these spell checkers were standalone programs, many of which could be run in TSR mode from within word-processing packages on PCs with sufficient memory.
However, the market for standalone packages was short-lived, as by the mid-1980s developers of popular word-processing packages like WordStar and WordPerfect had incorporated spell checkers in their packages, mostly licensed from the above companies, who quickly expanded support from just English to many European and eventually even Asian languages. However, this required increasing sophistication in the morphology routines of the software, particularly with regard to heavily-agglutinative languages like Hungarian and Finnish. Although the size of the word-processing market in a country like Iceland might not have justified the investment of implementing a spell checker, companies like WordPerfect nonetheless strove to localize their software for as many national markets as possible as part of their global marketing strategy.
Firefox 2.0, a web browser, has spell check support for user-written content, such as when editing Wikitext, writing on many webmail sites, blogs, and social networking websites. The web browsers Google Chrome, Konqueror, and Opera, the email client Kmail and the instant messaging client Pidgin also offer spell checking support, transparently using previously GNU Aspell and currently Hunspell as their engine. Mac OS X now has spell check system-wide, extending the service to virtually all bundled and third party applications.
Some spell checkers have separate support for medical dictionaries to help prevent medical errors.

Functionality
The first spell checkers were "verifiers" instead of "correctors." They offered no suggestions for incorrectly spelled words. This was helpful for typos but it was not so helpful for logical or phonetic errors. The challenge the developers faced was the difficulty in offering useful suggestions for misspelled words. This requires reducing words to a skeletal form and applying pattern-matching algorithms.
It might seem logical that where spell-checking dictionaries are concerned, "the bigger, the better," so that correct words are not marked as incorrect. In practice, however, an optimal size for English appears to be around 90,000 entries. If there are more than this, incorrectly spelled words may be skipped because they are mistaken for others. For example, a linguist might determine on the basis of corpus linguistics that the word baht is more frequently a misspelling of bath or bat than a reference to the Thai currency. Hence, it would typically be more useful if a few people who write about Thai currency were slightly inconvenienced than if the spelling errors of the many more people who discuss baths were overlooked.

The first MS-DOS spell checkers were mostly used in proofing mode from within word processing packages. After preparing a document, a user scanned the text looking for misspellings. Later, however, batch processing was offered in such packages as Oracle's short-lived CoAuthor and allowed a user to view the results after a document was processed and correct only the words that were known to be wrong. When memory and processing power became abundant, spell checking was performed in the background in an interactive way, such as has been the case with the Sector Software produced Spellbound program released in 1987 and Microsoft Word since Word 95.
In recent years, spell checkers have become increasingly sophisticated; some are now capable of recognizing simple grammatical errors. However, even at their best, they rarely catch all the errors in a text (such as homophone errors) and will flag neologisms and foreign words as misspellings. Nonetheless, spell checkers can be considered as a type of foreign language writing aid that non-native language learners can rely on to detect and correct their misspellings in the target language.

Spell-checking non-English languages
English is unusual in that most words used in formal writing have a single spelling that can be found in a typical dictionary, with the exception of some jargon and modified words. In many languages, words are often concatenated into new combinations of words. In German, compound nouns are frequently coined from other existing nouns. Some scripts do not clearly separate one word from another, requiring word-splitting algorithms. Each of these presents unique challenges to non-English language spell checkers.

Context-sensitive spell checkers
There has been research on developing algorithms that are capable of recognizing a misspelled word, even if the word itself is in the vocabulary, based on the context of the surrounding words. Not only does this allow words such as those in the poem above to be caught, but it mitigates the detrimental effect of enlarging dictionaries, allowing more words to be recognized. For example, baht in the same paragraph as Thai or Thailand would not be recognized as a misspelling of bath. The most common example of errors caught by such a system are homophone errors, such as the bold words in the following sentence:

Their coming too sea if its reel.The most successful algorithm to date is Andrew Golding and Dan Roth's "Winnow-based  spelling correction algorithm", published in 1999, which is able to recognize about 96% of context-sensitive spelling errors, in addition to ordinary non-word spelling errors. A context-sensitive spell checker appears in Microsoft Office 2007, and also appeared in the now-defunct Google Wave.Grammar checkers attempt to fix problems with grammar beyond spelling errors, including incorrect choice of words.

See also
Approximate string matching
Cupertino effect
Grammar checker
Record linkage problem
Spelling suggestion
Words (Unix)

References
External links
List of spell checkers at Curlie
Norvig.com, "How to Write a Spelling Corrector", by Peter Norvig
BBK.ac.uk, "Spellchecking by computer", by Roger Mitton
CBSNews.com, Spell-Check Crutch Curtails Correctness, by Lloyd de Vries
NIU.edu, Candidate for a Pullet Surprise - Complete corrected poem
Corrector.co, Why grammar checker tools are important? - A detailed explanation by Marcus Camille
SPL (Sentence Plan Language) is an abstract notation representing the semantics of a sentence in natural language. A generator can be used to transform input in SPL notation into a sentence in a natural language. There is no distinct definition of SPL available on the Internet, although some papers are available on the subject.

See also
Natural language generation

External links
Robert T. Kasper. "A Flexible Interface for Linking Applications to Penman's Sentence Generator" (PDF). (418 KB)
In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. Algorithms for stemming have been studied in computer science since the 1960s. Many search engines treat words with the same stem as synonyms as a kind of query expansion, a process called conflation.
A computer program or subroutine that stems word may be called a stemming program, stemming algorithm, or stemmer.

Examples
A stemmer for English operating on the stem cat should identify such strings as cats, catlike, and catty. A stemming algorithm might also reduce the words fishing, fished, and fisher to the stem fish. The stem need not be a word, for example the Porter algorithm reduces, argue, argued, argues, arguing, and argus to the stem argu.

History
The first published stemmer was written by Julie Beth Lovins in 1968. This paper was remarkable for its early date and had great influence on later work in this area. Her paper refers to three earlier major attempts at stemming algorithms, by Professor John W. Tukey of Princeton University, the algorithm developed at Harvard University by Michael Lesk, under the direction of Professor Gerard Salton, and a third algorithm developed by James L. Dolby of R and D Consultants, Los Altos, California.
A later stemmer was written by Martin Porter and was published in the July 1980 issue of the journal Program. This stemmer was very widely used and became the de facto standard algorithm used for English stemming. Dr. Porter received the Tony Kent Strix award in 2000 for his work on stemming and information retrieval.
Many implementations of the Porter stemming algorithm were written and freely distributed; however, many of these implementations contained subtle flaws. As a result, these stemmers did not match their potential. To eliminate this source of error, Martin Porter released an official free software (mostly BSD-licensed) implementation of the algorithm around the year 2000. He extended this work over the next few years by building Snowball, a framework for writing stemming algorithms, and implemented an improved English stemmer together with stemmers for several other languages.
The Paice-Husk Stemmer was developed by Chris D Paice at Lancaster University in the late 1980s, it is an iterative stemmer and features an externally stored set of stemming rules. The standard set of rules provides a 'strong' stemmer and may specify the removal or replacement of an ending. The replacement technique avoids the need for a separate stage in the process to recode or provide partial matching. Paice also developed a direct measurement for comparing stemmers based on counting the over-stemming and under-stemming errors.

Algorithms
There are several types of stemming algorithms which differ in respect to performance and accuracy and how certain stemming obstacles are overcome.
A simple stemmer looks up the inflected form in a lookup table. The advantages of this approach are that it is simple, fast, and easily handles exceptions. The disadvantages are that all inflected forms must be explicitly listed in the table: new or unfamiliar words are not handled, even if they are perfectly regular (e.g. cats ~ cat), and the table may be large. For languages with simple morphology, like English, table sizes are modest, but highly inflected languages like Turkish may have hundreds of potential inflected forms for each root.
A lookup approach may use preliminary part-of-speech tagging to avoid overstemming.

The production technique
The lookup table used by a stemmer is generally produced semi-automatically. For example, if the word is "run", then the inverted algorithm might automatically generate the forms "running", "runs", "runned", and "runly". The last two forms are valid constructions, but they are unlikely..

Suffix-stripping algorithms
Suffix stripping algorithms do not rely on a lookup table that consists of inflected forms and root form relations. Instead, a typically smaller list of "rules" is stored which provides a path for the algorithm, given an input word form, to find its root form. Some examples of the rules include:

if the word ends in 'ed', remove the 'ed'
if the word ends in 'ing', remove the 'ing'
if the word ends in 'ly', remove the 'ly'Suffix stripping approaches enjoy the benefit of being much simpler to maintain than brute force algorithms, assuming the maintainer is sufficiently knowledgeable in the challenges of linguistics and morphology and encoding suffix stripping rules. Suffix stripping algorithms are sometimes regarded as crude given the poor performance when dealing with exceptional relations (like 'ran' and 'run'). The solutions produced by suffix stripping algorithms are limited to those lexical categories which have well known suffixes with few exceptions. This, however, is a problem, as not all parts of speech have such a well formulated set of rules. Lemmatisation attempts to improve upon this challenge.
Prefix stripping may also be implemented. Of course, not all languages use prefixing or suffixing.

Additional algorithm criteria
Suffix stripping algorithms may differ in results for a variety of reasons. One such reason is whether the algorithm constrains whether the output word must be a real word in the given language. Some approaches do not require the word to actually exist in the language lexicon (the set of all words in the language). Alternatively, some suffix stripping approaches maintain a database (a large list) of all known morphological word roots that exist as real words. These approaches check the list for the existence of the term prior to making a decision. Typically, if the term does not exist, alternate action is taken. This alternate action may involve several other criteria. The non-existence of an output term may serve to cause the algorithm to try alternate suffix stripping rules.
It can be the case that two or more suffix stripping rules apply to the same input term, which creates an ambiguity as to which rule to apply. The algorithm may assign (by human hand or stochastically) a priority to one rule or another. Or the algorithm may reject one rule application because it results in a non-existent term whereas the other overlapping rule does not. For example, given the English term friendlies, the algorithm may identify the ies suffix and apply the appropriate rule and achieve the result of friendl. friendl is likely not found in the lexicon, and therefore the rule is rejected.
One improvement upon basic suffix stripping is the use of suffix substitution. Similar to a stripping rule, a substitution rule replaces a suffix with an alternate suffix. For example, there could exist a rule that replaces ies with y. How this affects the algorithm varies on the algorithm's design. To illustrate, the algorithm may identify that both the ies suffix stripping rule as well as the suffix substitution rule apply. Since the stripping rule results in a non-existent term in the lexicon, but the substitution rule does not, the substitution rule is applied instead. In this example, friendlies becomes friendly instead of friendl.
Diving further into the details, a common technique is to apply rules in a cyclical fashion (recursively, as computer scientists would say). After applying the suffix substitution rule in this example scenario, a second pass is made to identify matching rules on the term friendly, where the ly stripping rule is likely identified and accepted. In summary, friendlies becomes (via substitution) friendly which becomes (via stripping) friend.
This example also helps illustrate the difference between a rule-based approach and a brute force approach. In a brute force approach, the algorithm would search for friendlies in the set of hundreds of thousands of inflected word forms and ideally find the corresponding root form friend. In the rule-based approach, the three rules mentioned above would be applied in succession to converge on the same solution. Chances are that the brute force approach would be slower, as lookup algorithms have a direct access to the solution, while rule-based should try several options, and combinations of them, and then choose which result seems to be the best.

Lemmatisation algorithms
A more complex approach to the problem of determining a stem of a word is lemmatisation. This process involves first determining the part of speech of a word, and applying different normalization rules for each part of speech. The part of speech is first detected prior to attempting to find the root since for some languages, the stemming rules change depending on a word's part of speech.
This approach is highly conditional upon obtaining the correct lexical category (part of speech). While there is overlap between the normalization rules for certain categories, identifying the wrong category or being unable to produce the right category limits the added benefit of this approach over suffix stripping algorithms. The basic idea is that, if the stemmer is able to grasp more information about the word being stemmed, then it can apply more accurate normalization rules (which unlike suffix stripping rules can also modify the stem).

Stochastic algorithms
Stochastic algorithms involve using probability to identify the root form of a word. Stochastic algorithms are trained (they "learn") on a table of root form to inflected form relations to develop a probabilistic model. This model is typically expressed in the form of complex linguistic rules, similar in nature to those in suffix stripping or lemmatisation. Stemming is performed by inputting an inflected form to the trained model and having the model produce the root form according to its internal ruleset, which again is similar to suffix stripping and lemmatisation, except that the decisions involved in applying the most appropriate rule, or whether or not to stem the word and just return the same word, or whether to apply two different rules sequentially, are applied on the grounds that the output word will have the highest probability of being correct (which is to say, the smallest probability of being incorrect, which is how it is typically measured).
Some lemmatisation algorithms are stochastic in that, given a word which may belong to multiple parts of speech, a probability is assigned to each possible part. This may take into account the surrounding words, called the context, or not. Context-free grammars do not take into account any additional information. In either case, after assigning the probabilities to each possible part of speech, the most likely part of speech is chosen, and from there the appropriate normalization rules are applied to the input word to produce the normalized (root) form.

n-gram analysis
Some stemming techniques use the n-gram context of a word to choose the correct stem for a word.

Hybrid approaches
Hybrid approaches use two or more of the approaches described above in unison. A simple example is a suffix tree algorithm which first consults a lookup table using brute force. However, instead of trying to store the entire set of relations between words in a given language, the lookup table is kept small and is only used to store a minute amount of "frequent exceptions" like "ran => run". If the word is not in the exception list, apply suffix stripping or lemmatisation and output the result.

Affix stemmers
In linguistics, the term affix refers to either a prefix or a suffix. In addition to dealing with suffixes, several approaches also attempt to remove common prefixes. For example, given the word indefinitely, identify that the leading "in" is a prefix that can be removed. Many of the same approaches mentioned earlier apply, but go by the name affix stripping. A study of affix stemming for several European languages can be found here.

Matching algorithms
Such algorithms use a stem database (for example a set of documents that contain stem words). These stems, as mentioned above, are not necessarily valid words themselves (but rather common sub-strings, as the "brows" in "browse" and in "browsing"). In order to stem a word the algorithm tries to match it with stems from the database, applying various constraints, such as on the relative length of the candidate stem within the word (so that, for example, the short prefix "be", which is the stem of such words as "be", "been" and "being", would not be considered as the stem of the word "beside")..

Language challenges
While much of the early academic work in this area was focused on the English language (with significant use of the Porter Stemmer algorithm), many other languages have been investigated.Hebrew and Arabic are still considered difficult research languages for stemming. English stemmers are fairly trivial (with only occasional problems, such as "dries" being the third-person singular present form of the verb "dry", "axes" being the plural of "axe" as well as "axis"); but stemmers become harder to design as the morphology, orthography, and character encoding of the target language becomes more complex. For example, an Italian stemmer is more complex than an English one (because of a greater number of verb inflections), a Russian one is more complex (more noun declensions), a Hebrew one is even more complex (due to nonconcatenative morphology, a writing system without vowels, and the requirement of prefix stripping: Hebrew stems can be two, three or four characters, but not more), and so on.

Multilingual stemming
Multilingual stemming applies morphological rules of two or more languages simultaneously instead of rules for only a single language when interpreting a search query. Commercial systems using multilingual stemming exist.

Error metrics
There are two error measurements in stemming algorithms, overstemming and understemming. Overstemming is an error where two separate inflected words are stemmed to the same root, but should not have been—a false positive. Understemming is an error where two separate inflected words should be stemmed to the same root, but are not—a false negative. Stemming algorithms attempt to minimize each type of error, although reducing one type can lead to increasing the other.
For example, the widely used Porter stemmer stems "universal", "university", and "universe" to "univers". This is a case of overstemming: though these three words are etymologically related, their modern meanings are in widely different domains, so treating them as synonyms in a search engine will likely reduce the relevance of the search results.
An example of understemming in the Porter stemmer is "alumnus" → "alumnu", "alumni" → "alumni", "alumna"/"alumnae" → "alumna".  This English word keeps Latin morphology, and so these near-synonyms are not conflated.

Applications
Stemming is used as an approximate method for grouping words with a similar basic meaning together. For example, a text mentioning "daffodils" is probably closely related to a text mentioning "daffodil" (without the s). But in some cases, words with the same morphological stem have idiomatic meanings which are not closely related: a user searching for "marketing" will not be satisfied by most documents mentioning "markets" but not "marketing".

Information retrieval
Stemmers are common elements in query systems such as Web search engines. The effectiveness of stemming for English query systems were soon found to be rather limited, however, and this has led early information retrieval researchers to deem stemming irrelevant in general. An alternative approach, based on searching for n-grams rather than stems, may be used instead. Also, stemmers may provide greater benefits in other languages than English.

Domain analysis
Stemming is used to determine domain vocabularies in domain analysis.

Use in commercial products
Many commercial companies have been using stemming since at least the 1980s and have produced algorithmic and lexical stemmers in many languages.The Snowball stemmers have been compared with commercial lexical stemmers with varying results.Google search adopted word stemming in 2003. Previously a search for "fish" would not have returned "fishing". Other software search algorithms vary in their use of word stemming. Programs that simply search for substrings obviously will find "fish" in "fishing" but when searching for "fishes" will not find occurrences of the word "fish".

See also
Root (linguistics) - linguistic definition of the term "root"
Stem (linguistics) - linguistic definition of the term "stem"
Morphology (linguistics)
Lemma (morphology) - linguistic definition
Lemmatization
Lexeme
Inflection
Derivation - stemming is a form of reverse derivation
Natural language processing - stemming is generally regarded as a form of NLP
Text mining - stemming algorithms play a major role in commercial NLP software
Computational linguistics
Snowball (programming language) - designed for creating stemming algorithms
NLTK - implements several stemming algorithms in Python

References
Further reading
External links
Apache OpenNLP includes Porter and Snowball stemmersSMILE Stemmer - free online service, includes Porter and Paice/Husk' Lancaster stemmers (Java API)
Themis - open source IR framework, includes Porter stemmer implementation (PostgreSQL, Java API)
Snowball - free stemming algorithms for many languages, includes source code, including stemmers for five romance languages
Snowball on C# - port of Snowball stemmers for C# (14 languages)
Python bindings to Snowball API
Ruby-Stemmer - Ruby extension to Snowball API
PECL - PHP extension to the Snowball API
Oleander Porter's algorithm - stemming library in C++ released under BSD
Unofficial home page of the Lovins stemming algorithm - with source code in a couple of languages
Official home page of the Porter stemming algorithm - including source code in several languages
Official home page of the Lancaster stemming algorithm - Lancaster University, UK
Official home page of the UEA-Lite Stemmer  - University of East Anglia, UK
Overview of stemming algorithms
PTStemmer - A Java/Python/.Net stemming toolkit for the Portuguese language
jsSnowball - open source JavaScript implementation of Snowball stemming algorithms for many languages
Snowball Stemmer - implementation for Java
hindi_stemmer - open source stemmer for Hindi
czech_stemmer - open source stemmer for Czech
Comparative Evaluation of Arabic Language Morphological Analysers and Stemmers
Tamil StemmerThis article is based on material taken from  the Free On-line Dictionary of Computing  prior to 1 November 2008 and incorporated under the "relicensing" terms of the GFDL, version 1.3 or later.
In machine learning and data mining, a string kernel is a kernel function that operates on strings, i.e. finite sequences of symbols that need not be of the same length. String kernels can be intuitively understood as functions measuring the similarity of pairs of strings: the more similar two strings a and b are, the higher the value of a string kernel K(a, b) will be.
Using string kernels with kernelized learning algorithms such as support vector machines allow such algorithms to work with strings, without having to translate these to fixed-length, real-valued feature vectors. String kernels are used in domains where sequence data are to be clustered or classified, e.g. in text mining and gene analysis.

Informal introduction
Suppose one wants to compare some text passages automatically and indicate their relative similarity.
For many applications, it might be sufficient to find some keywords which match exactly.
One example where exact matching is not always enough is found in spam detection.
Another would be in computational gene analysis, where homologous genes have mutated, resulting in common subsequences along with deleted, inserted or replaced symbols.

Motivation
Since several well-proven data clustering, classification and information retrieval
methods (for example support vector machines) are designed to work on vectors
(i.e. data are elements of a vector space), using a string kernel allows the extension of these methods to handle sequence data.
The string kernel method is to be contrasted with earlier approaches for text classification where feature vectors only indicated
the presence or absence of a word.
Not only does it improve on these approaches, but it is an example for a whole class of kernels adapted to data structures, which
began to appear at the turn of the 21st century. A survey of such methods has been compiled by Gärtner.In bioinformatics string kernels are used especially to transform biological sequences such as proteins or DNA into vectors for further use in machine learning models. An example of a string kernel used for that purpose is the profile kernel.

Definition
A kernel on a domain 
  
    
      
        D
      
    
    {\displaystyle D}
   is a function 
  
    
      
        K
        :
        D
        ×
        D
        →
        
          R
        
      
    
    {\displaystyle K:D\times D\rightarrow \mathbb {R} }
  
satisfying some conditions (being symmetric in the arguments, continuous and positive semidefinite in a certain sense).
Mercer's theorem asserts that 
  
    
      
        K
      
    
    {\displaystyle K}
   can then be expressed as 
  
    
      
        K
        (
        x
        ,
        y
        )
        =
        φ
        (
        x
        )
        ⋅
        φ
        (
        y
        )
      
    
    {\displaystyle K(x,y)=\varphi (x)\cdot \varphi (y)}
   with 
  
    
      
        φ
      
    
    {\displaystyle \varphi }
   mapping the arguments into an inner product space.
We can now reproduce the definition of a string subsequence kernel
on strings over an alphabet 
  
    
      
        Σ
      
    
    {\displaystyle \Sigma }
  . Coordinate-wise, the mapping is defined as follows:

  
    
      
        
          φ
          
            u
          
        
        :
        
          {
          
            
              
                
                  
                    Σ
                    
                      n
                    
                  
                  →
                  
                    
                      R
                    
                    
                      
                        Σ
                        
                          n
                        
                      
                    
                  
                
              
              
                
                  s
                  ↦
                  
                    ∑
                    
                      
                        i
                      
                      :
                      u
                      =
                      
                        s
                        
                          
                            i
                          
                        
                      
                    
                  
                  
                    λ
                    
                      l
                      (
                      
                        i
                      
                      )
                    
                  
                
              
            
          
          
        
      
    
    {\displaystyle \varphi _{u}:\left\{{\begin{array}{l}\Sigma ^{n}\rightarrow \mathbb {R} ^{\Sigma ^{n}}\\s\mapsto \sum _{\mathbf {i} :u=s_{\mathbf {i} }}\lambda ^{l(\mathbf {i} )}\end{array}}\right.}
  The 
  
    
      
        
          i
        
      
    
    {\displaystyle \mathbf {i} }
   are multiindices and 
  
    
      
        u
      
    
    {\displaystyle u}
   is a string of length 
  
    
      
        n
      
    
    {\displaystyle n}
  :
subsequences can occur in a non-contiguous manner, but gaps are penalized.
The multiindex 
  
    
      
        
          i
        
      
    
    {\displaystyle \mathbf {i} }
   gives the positions of the characters matching 
  
    
      
        u
      
    
    {\displaystyle u}
   in 
  
    
      
        s
      
    
    {\displaystyle s}
  . 
  
    
      
        l
        (
        
          i
        
        )
      
    
    {\displaystyle l(\mathbf {i} )}
   is the difference between the first and last entry in 
  
    
      
        
          i
        
      
    
    {\displaystyle \mathbf {i} }
  , that is: how far apart in 
  
    
      
        s
      
    
    {\displaystyle s}
   the subsequence matching 
  
    
      
        u
      
    
    {\displaystyle u}
   is. 
The parameter 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
   may be set to any value between 
  
    
      
        0
      
    
    {\displaystyle 0}
   (gaps are not allowed, as only 
  
    
      
        
          0
          
            0
          
        
      
    
    {\displaystyle 0^{0}}
   is not 
  
    
      
        0
      
    
    {\displaystyle 0}
   but 
  
    
      
        1
      
    
    {\displaystyle 1}
  ) and 
  
    
      
        1
      
    
    {\displaystyle 1}
  
(even widely-spread "occurrences" are weighted the same as appearances as a contiguous substring, as 
  
    
      
        
          1
          
            l
            (
            
              i
            
            )
          
        
        =
        1
      
    
    {\displaystyle 1^{l(\mathbf {i} )}=1}
  ).

For several relevant algorithms, data enters into the algorithm only in expressions involving an inner product of feature vectors,
hence the name kernel methods. A desirable consequence of this is that one does not need to explicitly calculate the transformation 
  
    
      
        ϕ
        (
        x
        )
      
    
    {\displaystyle \phi (x)}
  , only the inner product via the kernel, which may be a lot quicker, especially when approximated.


== References ==
Studies in Natural Language Processing is the book series of the
Association for Computational Linguistics, published by
Cambridge University Press.
Steven Bird is the series editor.
The editorial board has the following members:
Chu-Ren Huang, Chair Professor of Applied Chinese Language Studies in the Department of Chinese and Bilingual Studies and the Dean of the Faculty of Humanities (The Hong Kong Polytechnic University),
Chris Manning, Associate Professor of Linguistics and Computer Science in the Department of Linguistics and Computer Science (Stanford University), 
Yuji Matsumoto, Professor of Computational Linguistics in Graduate School of Information Science (Nara Institute of Science and Technology), 
Maarten de Rijke, Professor of Information Processing and Internet in the Informatics Institute (the University of Amsterdam) and
Harold Somers, Professor of Language Engineering(Emeritus)in School of Computer Science (University of Manchester).


== Books Currently in Print ==
Sukhotin's algorithm (introduced by Boris V. Sukhotin) is a statistical classification algorithm for classifying characters in a text as vowels or consonants. It may also be of use in some of substitution ciphers and has been considered in deciphering the Voynich manuscript, though one problem is to agree on the set of symbols the manuscript is written in.

See also
Computational linguistics
Cryptanalysis

External links
An excerpt from CRYPTOLOGY July 1991 Volume XV Number 3
"OPTIMIZATION ALGORITHMS OF DECIPHERING AS THE ELEMENTS OF A LINGUISTIC THEORY - B.V. Sukhotin
Syntax guessing, also known as guess-the-verb, guess-the-noun and the syntax quest, is a problem sometimes encountered in text-based video games, such as interactive fiction games and MUDs.  For various reasons - including a limited vocabulary, or a simple VERB-NOUN parser - the command syntax necessary to carry out an action may be obscure, such as with a button where the player must type PRESS BUTTON, while PUSH BUTTON does not work (a "guess-the-verb" problem), an item described as a pillow that can only be obtained by typing GET CUSHION rather than GET PILLOW (a "guess-the-noun" problem), or a situation where the command TURN THE DIAL works but TURN DIAL does not (a generalized syntax guessing problem).  If syntax guessing is necessary at a critical step, the game may appear unwinnable, with the player stuck until the right phrasing is guessed or is supplied by a walkthrough.  A quest that requires syntax guessing to complete is a "syntax quest", especially if it consists of little to no content other than syntax guessing.People whose native language is not English are particularly affected by syntax guessing.  TADS games supply a fair list of verbs commonly used in their documentation; if the game designer uses new actions not covered in this list, and which cannot easily be inferred by the purpose and context of the object, the player will probably encounter this problem.A similar problem can occur when attempting to accomplish a game goal using a certain combination of actions, locations and objects, which may appear rational and legitimate to the player, and are accepted by the game, only to be incorrectly handled and resulting in a gameplay error or game crash.

See also
Natural language understanding


== References ==
T9 is a predictive text technology for mobile phones (specifically those that contain a 3×4 numeric keypad), originally developed by Tegic Communications, now part of Nuance Communications. T9 stands for Text on 9 keys.T9 is used on phones from Verizon Wireless, NEC, Nokia, Samsung Electronics, Siemens, Sony Ericsson, Sanyo, Sagem and others, as well as PDA's such as Avigo during the late 1990s. The main competing technologies include iTap created by Motorola, SureType created by RIM, Eatoni's LetterWise and WordWise, and Intelab's Tauto. 
During the smartphone revolution, T9 became obsolete, since newer phones had full touchscreen keyboards. T9 is still used on certain inexpensive phones without a touchscreen. However, modern Android phones have T9 dialing which can be used to dial contacts by spelling the name of the contact one is trying to call.
The technology is protected by multiple US patents.

Design
T9's objective is to make it easier to enter text messages. It allows words to be formed by a single keypress for each letter, which is an enormous improvement over the multi-tap approach used in conventional mobile phone text entry at the time, in which several letters are associated with each key, and selecting one letter often requires multiple keypresses.
T9 combines the groups of letters on each phone key with a fast-access dictionary of words. It will then look up in the dictionary all words corresponding to the sequence of keypresses and order them by frequency of use. As T9 "gains familiarity" with the words and phrases the user commonly uses, it speeds up the process by offering the most frequently used words first and then letting the user access other choices with one or more presses of a predefined "Next" key.
The dictionary is expandable. After introducing a new word, the next time the user tries to produce that word, T9 adds it to the predictive dictionary. The user database (UDB) can be expanded via multi-tap. The implementation of the user database is dependent on the version of T9 and how T9 is actually integrated on the device. Some phone manufacturers supply a permanent user database, while others do so for the duration of the session.

Features
Some T9 implementations feature smart punctuation. This feature allows the user to insert sentence and word punctuation using the '1'-key. Depending on the context, smart punctuation inserts sentence punctuation (period or 'full stop') or embedded punctuation (period or hyphen) or word punctuation (apostrophe in can't, won't, isn't, and the possessive  's). Depending on the language, T9 also supports word breaking after punctuation to support clitics such as l'  and n' in French and  's in English.
The UDB is an optional feature that allows words that were explicitly entered by the user to be stored for future reference. The number of words stored depend on the implementation and the language.
In later versions of T9, the order of the words presented is adapted to the usage pattern. For instance, in English, 4663 matches "good", "home", "gone", "hood", etc. Such combinations are known as textonyms; e.g., "home" is referred to as a textonym of "good". T9 is encoded to prefer the word that its programmers determined to be the most common "textonym", such as "good" over "home" or "gone", "hand" over "game", or "bad" over "cad" or "ace".
When the user tends to use "home" more often than "good", eventually the two words switch position, and "home" is presented as the default option instead. Information about common word combinations can also be learned and stored for future predictions (so, "I'm feeling" followed by 4663 will be offered as "good" instead of "home", whereas "I went back" and 4663 would be "home", not "good").
For words entered by the user, word completion can be enabled. When the user enters matching keypresses, in addition to words and stems, the system also provides completions.
In later versions of T9, the user can select a primary and secondary language and access matches from both languages. This enables users to write messages in their native language, as well as a foreign one.
Some implementations learn commonly used word pairs and provide word prediction (e.g. if one often writes "eat food", after entering "eat" the phone will suggest "food", which can be confirmed by pressing Next).
T9 can automatically recognize and correct typing/texting errors, by looking at neighboring keys on the keypad to determine an incorrect keypress. For example, the word "testing" is entered with the key combination "8378464". Entering the same number but with two incorrect keypresses of neighboring keys, e.g., "8278494" results in T9 suggesting the words "tasting" (8278464), "testing" (8378464), and "tapping" (8277464).

Algorithm
In order to achieve compression ratios of close to 1 byte per word, T9 uses an optimized algorithm that maintains word order and partial words (also known as stems); however, because of this compression, it over-generates words that are sometimes visible as "junk words". This is a side effect of the requirements for small database sizes on the lower end embedded devices.

Examples
On a phone with a numeric keypad, each time a key (1-9) is pressed (when in a text field), the algorithm returns a guess for what letters are most likely for the keys pressed to that point. For example, to enter the word 'the', the user would press 8 then 4 then 3, and the display would display 't' then 'th' then 'the'. If the less-common word 'fore' is intended (3673) the predictive algorithm may select 'Ford'. Pressing the 'next' key (typically the '*' key) might bring up 'dose', and finally 'fore'. If 'fore' is selected, then the next time the user presses the sequence 3673, fore will be more likely to be the first word displayed. If the word "Felix" is intended, however, when entering 33549, the display shows 'E', then 'De', 'Del', 'Deli', and 'Felix.' This is an example of a letter changing while entering words.

Successors
Many smart keyboards now exist, such as Swype or Swiftkey, that have taken the idea of T9 and married it with the advanced touchscreen technology found in Android phones and iPhones. These advances have made T9 obsolete in newer cellphones for many users, since it is predicated on the use of a keypad with nothing besides numbers, the asterisk and the pound key. Many features, such as predictive text, have been adopted by and improved by future generations of keyboard software. However, T9 remains viable. For example, those with larger fingertips still use the T9 based keyboard on smartphones for text entry, because key press accuracy increases with the larger screen area per key on a numeric-style 4×3 keyboard. Such T9 formats for text entry therefore remain viable in all latest iterations of LG keyboards (called "phone layout") as well as certain Samsung keyboards and third party T9 keyboards that also provide keyboard and dictionary customization, such as Go keyboard for Androids and Type Nine for iPhones -- for this feature that most smartphone keyboards still can't address, even with their otherwise full-featured chiclet qwerty keyboards.

See also
XT9
LetterWise
Predictive text
Telephone keypad
Phoneword

References
External links
Nuance T9 Customer facing site
Tatoeba is a free collaborative online database of example sentences geared towards foreign language learners.  Its name comes from the Japanese term "tatoeba" (例えば), meaning "for example".  Unlike other online dictionaries, which focus on words, Tatoeba focuses on translation of complete sentences.  In addition, the structure of the database and interface emphasize one-to-many relationships. Not only can a sentence have multiple translations within a single language, but its translations into all languages are readily visible, as are indirect translations that involve a chain of stepwise links from one language to another.

The aim of the project
The aim of the Tatoeba Project is to create a database of sentences and translations that can be used by anyone developing a language learning application. The idea is that the project creates the data, so programmers can just focus on coding the application.
The data collected by the project is freely available under a Creative Commons Attribution (CC-BY) license.

Content
As of June 2019, the Tatoeba Corpus has over 7,500,000 sentences in 337 languages. The top 10 languages make up 73% of the corpus. Ninety-eight of these languages have over 1,000 sentences. The top 14 languages have over 100,000 sentences each.
Tatoebais also the current home of the Tanaka Corpus, a public-domain series of about 150,000 English–Japanese sentence pairs compiled by Hyogo University professor Yasuhito Tanaka first released in 2001, and where it is undergoing its latest revisions.The statistics for all languages are found at [1].

History
Tatoeba was founded by Trang Ho in 2006. She originally hosted the project on Sourceforge under the project name "multilangdict".

Interface
Users, even those who are not registered, can search for words in any language to retrieve sentences that use them.  Each sentence in the Tatoeba database is displayed next to its likely translations in other languages; direct and indirect translations are differentiated.  Sentences are tagged for content such as subject matter, dialect, or vulgarity; they also each have individual comment threads to facilitate feedback and corrections from other users and cultural notes. As of early 2016, more than 200,000 sentences in 19 languages had audio readings of different quality. Sentences can also be browsed by language, tag, or audio.
Registered users can add new sentences or translate or proofread existing ones, even if their target language is not their native tongue. However, it is preferred that users translate into their native or "strongest" language and add sentences from their native language rather than translating into or adding from their target language.This means that the text corpus is by far not free of errors, every user can translate sentences even if they have no idea about this specific language – due to the number of sentences it is not possible to check any sentence if it is correct or not. Furthermore, as of late 2019 even the terms of use of the website are not translated. 
Translations are linked to the original sentence automatically.  Users can freely edit their sentences, "adopt" and correct sentences without an owner, and comment on others' sentences.  Advanced contributors, a rank above ordinary contributors, can tag, link, and unlink sentences. Corpus maintainers, a rank above advanced contributors, can untag and delete sentences. They can also modify owned sentences, though they typically do so only if the owner fails to respond to a request to make the change.

Database structure
Tatoeba's basic data structure is a series of nodes and links.  Each sentence is a node; each link bridges two sentences with the same meaning.

License
The entire Tatoeba database is published under a Creative Commons Attribution 2.0 license, freeing it for academic and other use.

Grants
Tatoeba received a grant from Mozilla Drumbeat in December 2010.Some work on the Tatoeba infrastructure was sponsored by Google Summer of Code, 2014 edition.In May 2018 they received a $25,000 Mozilla Open Source Support (MOSS) program grant.In Aug 2019 they received a $15,000 Mozilla Open Source Support (MOSS) program grant.

Usage
Parallel text corpora such as Tatoeba are used for a variety of natural language processing tasks such as machine translation.  The Tatoeba data has been used as data for treebanking Japanese and statistical machine translation, as well as the WWWJDIC Japanese–English dictionary and the Bilingual Sentence Pairs and Japanese Reading and Translation Practice on www.ManyThings.org.

Offline edition
Selected content from Tatoeba – 83,932 phrases in Esperanto along with all their translations into other languages – has appeared in the third edition of the multilingual DVD Esperanto Elektronike ("Electronic Esperanto") published in 6,000 copies by E@I in July 2011.
Tab-delimited data ready for import into Anki and similar software can be downloaded directly at the Tatoeba Website.

See also
Phrase book
List of linguistic example sentences

References
External links

Official website 
(Youtube) Video presenting the key ideas behind the Tatoeba Project
The Tehran Monolingual Corpus (TMC) is a large-scale Persian monolingual corpus. TMC is suited for Language Modeling and relevant research areas in Natural Language Processing.
The corpus is extracted from Hamshahri Corpus and ISNA news agency website. The quality of Hamshahri corpus is improved for language modeling purpose by a series of tokenization and spell-checking steps.
TMC comprises more than 250 million words. The total number of unique words (with frequency of two or more) of the corpus is about 300 thousand, which is relatively good for a highly-inflectional language like Persian.
TMC is created by Natural Language Processing Lab of University of Tehran. The corpus is free for research use, after obtaining permission from the corpus aggregator.

See also
TEP: Tehran English-Persian parallel corpus
Hamshahri Corpus

External links
TMC description page
Temporal annotation is the study of how to automatically add semantic information regarding time to natural language documents. It plays a role in natural language processing and computational linguistics.

About
Temporal annotation involves the application of a semantic annotation to a document. Significant temporal annotation standards include TimeML, ISO-TimeML and TIDES. These standards typically include annotations for some or all of temporal expressions (or timexes), events, temporal relations, temporal signals,  and temporal relation types.
In natural language texts, events may be associated with times; e.g., they may start or end at a given at a time. Events are also associated with other events, like occurring before or after them. We call these relations temporal relations. Temporal relation typing classifies the relation between two arguments, and is an important and difficult sub-task of figuring out all the temporal information in a document. Allen's interval algebra is one scheme for types of temporal relations. Rule-engineering and machine learning approaches to temporal annotation have both been successful, though achieving high performance in temporal relation typing remains a difficult task.

Applications
Successful temporal annotation enables systems to find out when facts asserted in texts are true, to build timelines,  to extract plans, and to discover mentions of change. This has had applications in many domains, such as information extraction, digital history, processing social media, and clinical text mining.

Evaluation
The TempEval task series sets a shared temporal annotation task, and has run at SemEval three times, attracting system entries from around the world. The task originally centred on determining the types of temporal relations only. In TempEval-2 and -3, this expanded to include event and timex annotation. In addition, the i2b2 clinical evaluation shared task was a temporal annotation exercise in 2012, which attracted a great deal of interest.

See also
Computational semantics
Natural language processing
SemEval
TimeML

Further reading
Boguraev, B. and Ando, R.K. (2005), TimeML-Compliant Text Analysis for Temporal Reasoning. Proceedings of IJCAI.
Derczynski, L. (2013), Determining the Types of Temporal Relations in Discourse, PhD thesis, University of Sheffield.
Pustejovsky et al. (2003), The TimeBank Corpus, Proceedings of the Corpus Linguistics Conference.
Pustejovsky et al. (2005), The specification language TimeML, in 'The Language of Time'. ISBN 9780199268542.
UzZaman, N. and Allen, J. (2010), Event and Temporal Expression extraction from raw text: first step towards a temporally aware system, International Journal of Semantic Computing 4(4).

References
External links
TimeML.org
THYME project
Pheme project
Teragram Corporation is a fully owned subsidiary of SAS Institute, a major producer of statistical analysis software, headquartered in Cary, North Carolina, USA.  Teragram is based in Cambridge, Massachusetts and specializes in the application of computational linguistics to multilingual natural language processing.
Teragram's technology is licensed to public search engines such as Ask.com and Yahoo!, to media companies such as the New York Times and the Tribune Company, and to original equipment manufacturer (OEM) customers such as Fast Search & Transfer and Verity.[4]
Teragram was founded by Emmanuel Roche and Yves Schabes in 1997 and acquired by SAS Institute in 2008.Knowledge management and digital content trade magazines have named it one of the top 100 companies in each of those fields.  Its major competitor is Inxight.

Notes
External links
Teragram web site
In natural language processing (NLP), a text graph is a graph representation of a text item (document, passage or sentence). It is typically created as a preprocessing step to support NLP tasks such as text condensationterm disambiguation
(topic-based) text summarization, relation extraction  and textual entailment.

Representation
The semantics of what a text graph's nodes and edges represent can vary widely. Nodes for example can simply connect to tokenized words, or to domain-specific terms, or to entities mentioned in the text. The edges, on the other hand, can be between these text-based tokens or they can also link to a knowledge base.

TextGraphs Workshop series
The TextGraphs Workshop series is a series of regular academic workshops intended to encourage the synergy between the fields of natural language processing (NLP) and graph theory. The mix between the two started small, with graph theoretical framework providing efficient and elegant solutions for NLP applications that focused on single documents for part-of-speech tagging, word sense disambiguation and semantic role labelling, got progressively larger with ontology learning and information extraction from large text collections.
The 11th edition of the workshop (TextGraphs-11) will be collocated with the Annual Meeting of Association for Computational Linguistics (ACL 2017) in Vancouver, BC, Canada.

Areas of interest
Graph-based methods for providing reasoning and interpretation of deep learning methods
Graph-based methods for reasoning and interpreting deep processing by neural networks,
Explorations of the capabilities and limits of graph-based methods applied to neural networks in general
Investigation of which aspects of neural networks are not susceptible to graph-based methods.
Graph-based methods for Information Retrieval, Information Extraction, and Text Mining
Graph-based methods for word sense disambiguation,
Graph-based representations for ontology learning,
Graph-based strategies for semantic relations identification,
Encoding semantic distances in graphs,
Graph-based techniques for text summarization, simplification, and paraphrasing
Graph-based techniques for document navigation and visualization
Reranking with graphs
Applications of label propagation algorithms, etc.
New graph-based methods for NLP applications
Random walk methods in graphs
Spectral graph clustering
Semi-supervised graph-based methods
Methods and analyses for statistical networks
Small world graphs
Dynamic graph representations
Topological and pretopological analysis of graphs
Graph kernels, etc.
Graph-based methods for applications on social networks
Rumor proliferation
E-reputation
Multiple identity detection
Language dynamics studies
Surveillance systems, etc.
Graph-based methods for NLP and Semantic Web
Representation learning methods for knowledge graphs (i.e., knowledge graph embedding)
Using graphs-based methods to populate ontologies using textual data,
Inducing knowledge of ontologies into NLP applications using graphs,
Merging ontologies with graph-based methods using NLP techniques.

See also
Bag-of-words model
Document classification
Document-term matrix
Hyperlinking
Graph database
Wiki

References
External links
Gabor Melli's page on text graphs Description of text graphs from a semantic processing perspective.
According to Hotho et al. (2005) we can differ three different perspectives of text mining, namely text mining as information extraction, text mining as text data mining, and text mining as KDD (Knowledge Discovery in Databases) process. Text mining is "the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources." Written resources can be websites, books, emails, reviews, articles.
Text mining, also referred to as text data mining, roughly equivalent to text analytics, is the process of deriving high-quality information from text. High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning. Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interest. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities).
Text analysis involves information retrieval, lexical analysis to study word frequency distributions, pattern recognition, tagging/annotation, information extraction, data mining techniques including link and association analysis, visualization, and predictive analytics. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP), different types of algorithms and analytical methods. An important phase of this process is the interpretation of the gathered information.
A typical application is to scan a set of documents written in a natural language and either model the document set for predictive classification purposes or populate a database or search index with the information extracted.
The document is the basic element while starting with text mining. Here, we define a document as a unit of textual data, which normally exists in many types of collections.

Text analytics
The term text analytics describes a set of linguistic, statistical, and machine learning techniques that model and structure the information content of textual sources for business intelligence, exploratory data analysis, research, or investigation. The term is roughly synonymous with text mining; indeed, Ronen Feldman modified a 2000 description of "text mining" in 2004 to describe "text analytics". The latter term is now used more frequently in business settings while "text mining" is used in some of the earliest application areas, dating to the 1980s, notably life-sciences research and government intelligence.
The term text analytics also describes that application of text analytics to respond to business problems, whether independently or in conjunction with query and analysis of fielded, numerical data. It is a truism that 80 percent of business-relevant information originates in unstructured form, primarily text. These techniques and processes discover and present knowledge – facts, business rules, and relationships – that is otherwise locked in textual form, impenetrable to automated processing.

Text analysis processes
Subtasks—components of a larger text-analytics effort—typically include:

Dimensionality reduction is important technique for pre-processing data. Technique is used to identify the root word for actual words and reduce the size of the text data.
Information retrieval or identification of a corpus is a preparatory step: collecting or identifying a set of textual materials, on the Web or held in a file system, database, or content corpus manager, for analysis.
Although some text analytics systems apply exclusively advanced statistical methods, many others apply more extensive natural language processing, such as part of speech tagging, syntactic parsing, and other types of linguistic analysis.
Named entity recognition is the use of gazetteers or statistical techniques to identify named text features: people, organizations, place names, stock ticker symbols, certain abbreviations, and so on.
Disambiguation—the use of contextual clues—may be required to decide where, for instance, "Ford" can refer to a former U.S. president, a vehicle manufacturer, a movie star, a river crossing, or some other entity.
Recognition of Pattern Identified Entities: Features such as telephone numbers, e-mail addresses, quantities (with units) can be discerned via regular expression or other pattern matches.
Document clustering: identification of sets of similar text documents.
Coreference: identification of noun phrases and other terms that refer to the same object.
Relationship, fact, and event Extraction: identification of associations among entities and other information in text
Sentiment analysis involves discerning subjective (as opposed to factual) material and extracting various forms of attitudinal information: sentiment, opinion, mood, and emotion. Text analytics techniques are helpful in analyzing sentiment at the entity, concept, or topic level and in distinguishing opinion holder and opinion object.
Quantitative text analysis is a set of techniques stemming from the social sciences where either a human judge or a computer extracts semantic or grammatical relationships between words in order to find out the meaning or stylistic patterns of, usually, a casual personal text for the purpose of psychological profiling etc.

Applications
Text mining technology is now broadly applied to a wide variety of government, research, and business needs. All these groups may use text mining for records management and searching documents relevant to their daily activities. Legal professionals may use text mining for e-discovery, for example. Governments and military groups use text mining for national security and intelligence purposes. Scientific researchers incorporate text mining approaches into efforts to organize large sets of text data (i.e., addressing the problem of unstructured data), to determine ideas communicated through text (e.g., sentiment analysis in social media) and to support scientific discovery in fields such as the life sciences and bioinformatics. In business, applications are used to support competitive intelligence and automated ad placement, among numerous other activities.

Security applications
Many text mining software packages are marketed for security applications, especially monitoring and analysis of online plain text sources such as Internet news, blogs, etc. for national security purposes. It is also involved in the study of text encryption/decryption.

Biomedical applications
A range of text mining applications in the biomedical literature has been described, including computational approaches to assist with studies in protein docking, protein interactions, and protein-disease associations. In addition, with large patient textual datasets in the clinical field, datasets of demographic information in population studies and adverse event reports, text mining can facilitate clinical studies and precision medicine. Text mining algorithms can facilitate the stratification and indexing of specific clinical events in large patient textual datasets of symptoms, side effects, and comorbidities from electronic health records, event reports, and reports from specific diagnostic tests. One online text mining application in the biomedical literature is PubGene, a publicly accessible search engine that combines biomedical text mining with network visualization. GoPubMed is a knowledge-based search engine for biomedical texts. Text mining techniques also enable us to extract unknown knowledge from unstructured documents in the clinical domain

Software applications
Text mining methods and software is also being researched and developed by major firms, including IBM and Microsoft, to further automate the mining and analysis processes, and by different firms working in the area of search and indexing in general as a way to improve their results. Within public sector much effort has been concentrated on creating software for tracking and monitoring terrorist activities. For study purposes, Weka software is one of the most popular options in the scientific world, acting as an excellent entry point for beginners. For Python programmers, there is an excellent toolkit called NLTK for more general purposes. For more advanced programmers, there's also the Gensim library, which focuses on word embedding-based text representations.

Online media applications
Text mining is being used by large media companies, such as the Tribune Company, to clarify information and to provide readers with greater search experiences, which in turn increases site "stickiness" and revenue. Additionally, on the back end, editors are benefiting by being able to share, associate and package news across properties, significantly increasing opportunities to monetize content.

Business and marketing applications
Text mining is starting to be used in marketing as well, more specifically in analytical customer relationship management. Coussement and Van den Poel (2008) apply it to improve predictive analytics models for customer churn (customer attrition). Text mining is also being applied in stock returns prediction.

Sentiment analysis
Sentiment analysis may involve analysis of movie reviews for estimating how favorable a review is for a movie.
Such an analysis may need a labeled data set or labeling of the affectivity of words.
Resources for affectivity of words and concepts have been made for WordNet and ConceptNet, respectively.
Text has been used to detect emotions in the related area of affective computing. Text based approaches to affective computing have been used on multiple corpora such as students evaluations, children stories and news stories.

Scientific literature mining and academic applications
The issue of text mining is of importance to publishers who hold large databases of information needing indexing for retrieval. This is especially true in scientific disciplines, in which highly specific information is often contained within written text. Therefore, initiatives have been taken such as Nature's proposal for an Open Text Mining Interface (OTMI) and the National Institutes of Health's common Journal Publishing Document Type Definition (DTD) that would provide semantic cues to machines to answer specific queries contained within text without removing publisher barriers to public access.
Academic institutions have also become involved in the text mining initiative:

The National Centre for Text Mining (NaCTeM), is the first publicly funded text mining centre in the world. NaCTeM is operated by the University of Manchester in close collaboration with the Tsujii Lab, University of Tokyo. NaCTeM provides customised tools, research facilities and offers advice to the academic community. They are funded by the Joint Information Systems Committee (JISC) and two of the UK Research Councils (EPSRC & BBSRC). With an initial focus on text mining in the biological and biomedical sciences, research has since expanded into the areas of social sciences.
In the United States, the School of Information at University of California, Berkeley is developing a program called BioText to assist biology researchers in text mining and analysis.
The Text Analysis Portal for Research (TAPoR), currently housed at the University of Alberta, is a scholarly project to catalogue text analysis applications and create a gateway for researchers new to the practice.

Methods for scientific literature mining
Computational methods have been developed to assist with information retrieval from scientific literature. Published approaches include methods for searching, determining novelty, and clarifying homonyms among technical reports.

Digital humanities and computational sociology
The automatic analysis of vast textual corpora has created the possibility for scholars to analyze
millions of documents in multiple languages with very limited manual intervention. Key enabling technologies have been parsing, machine translation, topic categorization, and machine learning.

The automatic parsing of textual corpora has enabled the extraction of actors and their relational networks on a vast scale, turning textual data into network data.  The resulting networks, which can contain thousands of nodes, are then analyzed by using tools from network theory to identify the key actors, the key communities or parties, and general properties such as robustness or structural stability of the overall network, or centrality of certain nodes. This automates the approach introduced by quantitative narrative analysis, whereby subject-verb-object triplets are identified with pairs of actors linked by an action, or pairs formed by actor-object.Content analysis has been a traditional part of social sciences and media studies for a long time. The automation of content analysis has allowed a "big data" revolution to take place in that field, with studies in social media and newspaper content that include millions of news items. Gender bias, readability, content similarity, reader preferences, and even mood have been analyzed based on text mining methods over millions of documents. The analysis of readability, gender bias and topic bias was demonstrated in Flaounas et al. showing how different topics have different gender biases and levels of readability; the possibility to detect mood patterns in a vast population by analyzing Twitter content was demonstrated as well.

Software
Text mining computer programs are available from many commercial and open source companies and sources. See List of text mining software.

Intellectual property law
Situation in Europe
Under European copyright and database laws, the mining of in-copyright works (such as by web mining) without the permission of the copyright owner is illegal. In the UK in 2014, on the recommendation of the Hargreaves review, the government amended copyright law to allow text mining as a limitation and exception. It was  the second country in the world to do so, following Japan, which introduced a mining-specific exception in 2009. However, owing to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law does not allow this provision to be overridden by contractual terms and conditions.
The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licenses for Europe. The fact that the focus on the solution to this legal issue was licenses, and not limitations and exceptions to copyright law, led representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.

Situation in the United States
US copyright law, and in particular its fair use provisions, means that text mining in America, as well as other fair use countries such as Israel, Taiwan and South Korea, is viewed as being legal. As text mining is transformative, meaning that it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one such use being text and data mining.

Implications
Until recently, websites most often used text-based searches, which only found documents containing specific user-defined words or phrases. Now, through use of a semantic web, text mining can find content based on meaning and context (rather than just by a specific word). Additionally, text mining software can be used to build large dossiers of information about specific people and events.  For example, large datasets based on data extracted from news reports can be built to facilitate social networks analysis or counter-intelligence.  In effect, the text mining software may act in a capacity similar to an intelligence analyst or research librarian, albeit with a more limited scope of analysis. Text mining is also used in some email spam filters as a way of determining the characteristics of messages that are likely to be advertisements or other unwanted material. Text mining plays an important role in determining financial market sentiment.

Future
Increasing interest is being paid to multilingual data mining: the ability to gain information across languages and cluster similar items from different linguistic sources according to their meaning.
The challenge of exploiting the large proportion of enterprise information that originates in "unstructured" form has been recognized for decades. It is recognized in the earliest definition of business intelligence (BI), in an October 1958 IBM Journal article by H.P. Luhn, A Business Intelligence System, which describes a system that will:

"...utilize data-processing machines for auto-abstracting and auto-encoding of documents and for creating interest profiles for each of the 'action points' in an organization. Both incoming and internally generated documents are automatically abstracted, characterized by a word pattern, and sent automatically to appropriate action points."

Yet as management information systems developed starting in the 1960s, and as BI emerged in the '80s and '90s as a software category and field of practice, the emphasis was on numerical data stored in relational databases. This is not surprising: text in "unstructured" documents is hard to process. The emergence of text analytics in its current form stems from a refocusing of research in the late 1990s from algorithm development to application, as described by Prof. Marti A. Hearst in the paper Untangling Text Data Mining:
For almost a decade the computational linguistics community has viewed large text collections as a resource to be tapped in order to produce better text analysis algorithms. In this paper, I have attempted to suggest a new emphasis: the use of large online text collections to discover new facts and trends about the world itself. I suggest that to make progress we do not need fully artificial intelligent text analysis; rather, a mixture of computationally-driven and user-guided analysis may open the door to exciting new results.

Hearst's 1999 statement of need fairly well describes the state of text analytics technology and practice a decade later.

See also
References
Citations
Sources
Ananiadou, S. and McNaught, J. (Editors) (2006). Text Mining for Biology and Biomedicine. Artech House Books. ISBN 978-1-58053-984-5
Bilisoly, R. (2008). Practical Text Mining with Perl. New York: John Wiley & Sons. ISBN 978-0-470-17643-6
Feldman, R., and Sanger, J. (2006). The Text Mining Handbook. New York: Cambridge University Press. ISBN 978-0-521-83657-9
Hotho, A., Nürnberger, A. and Paaß, G. (2005). "A brief survey of text mining". In Ldv Forum, Vol. 20(1), p. 19-62
Indurkhya, N., and Damerau, F. (2010). Handbook Of Natural Language Processing, 2nd Edition. Boca Raton, FL: CRC Press. ISBN 978-1-4200-8592-1
Kao, A., and Poteet, S. (Editors). Natural Language Processing and Text Mining. Springer. ISBN 1-84628-175-X
Konchady, M. Text Mining Application Programming (Programming Series). Charles River Media. ISBN 1-58450-460-9
Manning, C., and Schutze, H. (1999). Foundations of Statistical Natural Language Processing. Cambridge, MA: MIT Press. ISBN 978-0-262-13360-9
Miner, G., Elder, J., Hill. T, Nisbet, R., Delen, D. and Fast, A. (2012). Practical Text Mining and Statistical Analysis for Non-structured Text Data Applications. Elsevier Academic Press. ISBN 978-0-12-386979-1
McKnight, W. (2005). "Building business intelligence: Text data mining in business intelligence". DM Review, 21-22.
Srivastava, A., and Sahami. M. (2009). Text Mining: Classification, Clustering, and Applications. Boca Raton, FL: CRC Press. ISBN 978-1-4200-5940-3
Zanasi, A. (Editor) (2007). Text Mining and its Applications to Intelligence, CRM and Knowledge Management. WIT Press. ISBN 978-1-84564-131-3

External links
Marti Hearst: What Is Text Mining? (October, 2003)
Automatic Content Extraction, Linguistic Data Consortium
Automatic Content Extraction, NIST
Text Nailing (TN) is an information extraction method of semi-automatically extracting structured information from unstructured documents. The method allows a human to interactively review small blobs of text out of a large collection of documents, to identify potentially informative expressions. The identified expressions can be used then to enhance computational methods that rely on text (e.g., Regular expression) as well as advanced natural language processing (NLP) techniques. TN combines two concepts: 1) human-interaction with narrative text to identify highly prevalent non-negated expressions, and 2) conversion of all expressions and notes into non-negated alphabetical-only representations to create homogeneous representations. In traditional machine learning approaches for text classification, a human expert is required to label phrases or entire notes, and then a supervised learning algorithm attempts to generalize the associations and apply them to new data. In contrast, using non-negated distinct expressions eliminates the need for an additional computational method to achieve generalizability.

History
TN was developed at Massachusetts General Hospital and was tested in multiple scenarios including the extraction of smoking status, family history of coronary artery disease, identifying patients with sleep disorders, improve the accuracy of the Framingham risk score for patients with non-alcoholic fatty liver disease, and classify non-adherence to type-2 diabetes. A comprehensive review regarding extracting information from textual documents in the electronic health record is available.The importance of using non-negated expressions to achieve an increased accuracy of text-based classifiers was emphasized in a letter published in Communications of the ACM in October 2018.

Source code
A sample code for extracting smoking status from narrative notes using "nailed expressions" is available in GitHub.

TN as progressive cyber-human intelligence
In July 2018 researchers from Virginia Tech and University of Illinois at Urbana–Champaign referred TN as an example for progressive cyber-human intelligence (PCHI).

Criticism of machine learning in health care
Chen & Asch 2017 wrote "With machine learning situated at the peak of inflated expectations, we can soften a subsequent crash into a “trough of disillusionment” by fostering a stronger appreciation of the technology’s capabilities and limitations."A letter published in Communications of the ACM, "Beyond brute force", emphasized that a brute force approach may perform better than traditional machine learning algorithms when applied to text. The letter stated "... machine learning algorithms, when applied to text, rely on the assumption that any language includes an infinite number of possible expressions. In contrast, across a variety of medical conditions, we observed that clinicians tend to use the same expressions to describe patients' conditions."In his viewpoint published in June 2018 concerning slow adoption of data-driven findings in medicine, Uri Kartoun, co-creator of Text Nailing states that " ...Text Nailing raised skepticism in reviewers of medical informatics journals who claimed that it relies on simple tricks to simplify the text, and leans heavily on human annotation. TN indeed may seem just like a trick of the light at ﬁrst glance, but it is actually a fairly sophisticated method that ﬁnally caught the attention of more adventurous reviewers and editors who ultimately accepted it for publication."

Criticism
The human in-the-loop process is a way to generate features using domain experts. Using domain experts to come up with features is not a novel concept. However, the specific interfaces and method which helps the domain experts create the features are most likely novel.
In this case the features the experts create are equivalent to regular expressions. Removing non-alphabetical characters and matching on "smokesppd" is equal to the regular expression /smokes[^a-zA-Z]*ppd/. Using regular expressions as features for text classification is not novel.
Given these features the classifier is a manually set threshold by the authors, decided by the performance on a set of documents. This is a classifier, it's just that the parameters of the classifier, in this case a threshold, is set manually. Given the same features and documents almost any machine learning algorithm should be able to find the same threshold or (more likely) a better one.
The authors note that using support vector machines (SVM) and hundreds of documents give inferior performance, but does not specify which features or documents the SVM was trained/tested on. A fair comparison would use the same features and document sets as those used by the manual threshold classifier.


== References ==
Text normalization is the process of transforming text into a single canonical form that it might not have had before. Normalizing text before storing or processing it allows for separation of concerns, since input is guaranteed to be consistent before operations are performed on it. Text normalization requires being aware of what type of text is to be normalized and how it is to be processed afterwards; there is no all-purpose normalization procedure.

Applications
Text normalization is frequently used when converting text to speech. Numbers, dates, acronyms, and abbreviations are non-standard "words" that need to be pronounced differently depending on context. For example:

"$200" would be pronounced as "two hundred dollars" in English, but as "lua selau tālā" in Samoan.
"vi" could be pronounced as "vie," "vee," or "the sixth" depending on the surrounding words.Text can also be normalized for storing and searching in a database. For instance, if a search for "resume" is to match the word "résumé," then the text would be normalized by removing diacritical marks; and if "john" is to match "John", the text would be converted to a single case. To prepare text for searching, it might also be stemmed (e.g. converting "flew" and "flying" both into "fly"), canonicalized (e.g. consistently using American or British English spelling), or have stop words removed.

Techniques
For simple, context-independent normalization, such as removing non-alphanumeric characters or diacritical marks, regular expressions would suffice. For example, the sed script sed ‑e "s/\s+/ /g"  inputfile would normalize runs of whitespace characters into a single space. More complex normalization requires correspondingly complicated algorithms, including domain knowledge of the language and vocabulary being normalized. Among other approaches, text normalization has been modeled as a problem of tokenizing and tagging streams of text and as a special case of machine translation.

See also
Automated paraphrasing
Canonicalization
Text simplification
Unicode equivalence


== References ==
The Text REtrieval Conference (TREC) is an ongoing series of workshops focusing on a list of different information retrieval (IR) research areas, or tracks. It is co-sponsored by the National Institute of Standards and Technology (NIST) and the Intelligence Advanced Research Projects Activity (part of the office of the Director of National Intelligence), and began in 1992 as part of the TIPSTER Text program. Its purpose is to support and encourage research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies and to increase the speed of lab-to-product transfer of technology.
Each track has a challenge wherein NIST provides participating groups with data sets and test problems. Depending on track, test problems might be questions, topics, or target extractable features. Uniform scoring is performed so the systems can be fairly evaluated. After evaluation of the results, a workshop provides a place for participants to collect together thoughts and ideas and present current and future research work.Text Retrieval Conference started in 1992, funded by DARPA (US Defense Advanced Research Project) and Run by NIST. Its purpose was to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies.

Goals
Encourage retrieval search based on large text collectionsIncrease communication among industry ,academia, and government by creating an open forum for the exchange of research ideasSpeed the transfer of technology from research labs into commercial products by demonstrating substantial improvements retrieval methodologies on real world problemsTo increase the availability of appropriate evaluation techniques for use by industry and academia including development of new evaluation techniques more applicable to current systemsTREC is overseen by a program committee consisting of representatives from government, industry, and academia. For each TREC, NIST provide a set of documents and questions. Participants run their own retrieval system on the data and return to NIST a list of retrieved top-ranked documents .NIST pools the individual result judges the retrieved documents for correctness and evaluates the results. The TREC cycle ends with a workshop that is a forum for participants to share their experiences.

Relevance judgments in TREC
TREC uses binary relevance criterion that is either the document is relevant or not relevant. Since size of TREC collection is large, it is impossible to calculate the absolute recall for each query. In order to assess the relevance of documents in relation to a query, TREC uses a specific method call pooling for calculating relative recall. All the relevant documents that occurred in the top 100 documents for each system and for each query are combined together to produce a pool of relevant documents. Recall being the proportion of the pool of relevant documents that a single system retrieved for a query topic.

Various TRECs
In 1992 TREC-1 was held at NIST. The first conference attracted 28 groups of researchers from academia and industry. It demonstrated a wide range of different approaches to the retrieval of text from large document collections .Finally TREC1 revealed the facts that automatic construction of queries from natural language query statements seems to work. Techniques based on natural language processing were no better no worse than those based on vector or probabilistic approach.
TREC2 Took place in august 1993. 31 group of researchers where participated in this. Two types of retrieval were examined. Retrieval using an ‘ad hoc’query  and retrieval using  a ‘routing query.
In TREC-3 a small group experiments worked with Spanish language collection and others dealt with interactive query formulation in multiple databases. 
TREC-4 they made even shorter to investigate the problems with very short user statements
TREC-5 includes both short and long versions of the topics with the goal of carrying out deeper investigation into which types of techniques work well on various lengths of topics.
In TREC-6 Three new tracks speech, cross language, high precision information retrieval were introduced. The goal of cross language information retrieval is to facilitate research on system that are able to retrieve relevant document regardless of language of the source document.
TREC-7 contained seven tracks out of which two were new Query track and very large corpus track. The goal of the query track was to create a large query collection.
TREC-8 contain seven tracks out of which two –question answering and web tracks were new. The objective of QA query is to explore the possibilities of providing answers to specific natural language queries
TREC-9 Includes seven tracks
In TREC-10 Video tracks introduced Video tracks design to promote research in content based retrieval from digital video.
In TREC-11Novelity tracks introduced. The goal of novelty track is to investigate systems abilities to locate relevant and new information within the ranked set of documents returned by a traditional document retrieval system.
TREC-12 held in 2003 added three new tracks Genome track, robust retrieval track, HARD (Highly Accurate Retrieval from Documents

Tracks
Current tracks
New tracks are added as new research needs are identified, this list is current for TREC 2018.
CENTRE Track - Goal: run in parallel CLEF 2018, NTCIR-14, TREC 2018 to develop and tune an IR reproducibility evaluation protocol (new track for 2018).
Common Core Track - Goal: an ad hoc search task over news documents.
Complex Answer Retrieval (CAR) - Goal: to develop systems capable of answering complex information needs by collating information from an entire corpus.
Incident Streams Track - Goal: to research technologies to automatically process social media streams during emergency situations (new track for TREC 2018).
The News Track - Goal: partnership with The Washington Post to develop test collections in news environment (new for 2018).
Precision Medicine Track - Goal: a specialization of the Clinical Decision Support track to focus on linking oncology patient data to clinical trials.
Real-Time Summarization Track (RTS) - Goal: to explore techniques for real-time update summaries from social media streams.

Past tracks
Chemical Track - Goal: to develop and evaluate technology for large scale search in chemistry-related documents, including academic papers and patents, to better meet the needs of professional searchers, and specifically patent searchers and chemists.
Clinical Decision Support Track - Goal: to investigate techniques for linking medical cases to information relevant for patient care
Contextual Suggestion Track - Goal: to investigate search techniques for complex information needs that are highly dependent on context and user interests.
Crowdsourcing Track - Goal: to provide a collaborative venue for exploring crowdsourcing methods both for evaluating search and for performing search tasks.
Genomics Track - Goal: to study the retrieval of genomic data, not just gene sequences but also supporting documentation such as research papers, lab reports, etc. Last ran on TREC 2007.
Dynamic Domain Track - Goal:  to investigate  domain-specific search algorithms that adapt to the dynamic information needs of professional users as they explore in complex domains.
Enterprise Track - Goal: to study search over the data of an organization to complete some task. Last ran on TREC 2008.
Entity Track - Goal: to perform entity-related search on Web data. These search tasks (such as finding entities and properties of entities) address common information needs that are not that well modeled as ad hoc document search.
Cross-Language Track - Goal: to investigate the ability of retrieval systems to find documents topically regardless of source language. After 1999, this track spun off into CLEF.
FedWeb Track - Goal: to select best resources to forward a query to, and merge the results so that most relevant are on the top.
Federated Web Search Track - Goal: to investigate techniques for the selection and combination of search results from a large number of real on-line web search services.
Filtering Track - Goal: to binarily decide retrieval of new incoming documents given a stable information need.
HARD Track - Goal: to achieve High Accuracy Retrieval from Documents by leveraging additional information about the searcher and/or the search context.
Interactive Track - Goal: to study user interaction with text retrieval systems.
Knowledge Base Acceleration Track - Goal: to develop techniques to dramatically improve the efficiency of (human) knowledge base curators by having the system suggest modifications/extensions to the KB based on its monitoring of the data streams.
Legal Track - Goal: to develop search technology that meets the needs of lawyers to engage in effective discovery in digital document collections.
LiveQA Track -  Goal:  to  generate answers to real questions originating from real users via a live question stream, in real time.
Medical Records Track - Goal: to explore methods for searching unstructured information found in patient medical records.
Microblog Track - Goal: to examine the nature of real-time information needs and their satisfaction in the context of microblogging environments such as Twitter.
Natural language processing Track - Goal: to examine how specific tools developed by computational linguists might improve retrieval.
Novelty Track - Goal: to investigate systems' abilities to locate new (i.e., non-redundant) information.
OpenSearch Track - Goal: to  explore an evaluation paradigm for IR that involves real users of operational search engines. For this first year of the track the task will be ad hoc Academic Search.
Question Answering Track - Goal: to achieve more information retrieval than just document retrieval by answering factoid, list and definition-style questions.
Real-Time Summarization Track -  Goal: to explore techniques for constructing real-time update summaries from social media streams in response to users' information needs.
Robust Retrieval Track - Goal: to focus on individual topic effectiveness.
Relevance Feedback Track - Goal: to further deep evaluation of relevance feedback processes.
Session Track - Goal: to develop methods for measuring multiple-query sessions where information needs drift or get more or less specific over the session.
Spam Track - Goal: to provide a standard evaluation of current and proposed spam filtering approaches.
Tasks Track - Goal:  to test whether systems can induce the possible tasks users might be trying to accomplish given a query.
Temporal Summarization Track - Goal: to develop systems that allow users to efficiently monitor the information associated with an event over time.
Terabyte Track - Goal: to investigate whether/how the IR community can scale traditional IR test-collection-based evaluation to significantly large collections.
Total Recall Track -  Goal:: to evaluate methods to achieve very high recall, including methods that include a human assessor in the loop.
Video Track - Goal: to research in automatic segmentation, indexing, and content-based retrieval of digital video.In 2003, this track became its own independent evaluation named TRECVID.Web Track - Goal: to explore information seeking behaviors common in general web search.

Related events
In 1997, a Japanese counterpart of TREC was launched (first workshop in 1999), called NTCIR (NII Test Collection for IR Systems), and in 2000, CLEF, a European counterpart, specifically vectored towards the study of cross-language information retrieval was launched. Forum for Information Retrieval Evaluation (FIRE) started in 2008 with the aim of building a South Asian counterpart for TREC, CLEF, and NTCIR,

Conference contributions to search effectiveness
NIST claims that within the first six years of the workshops, the effectiveness of retrieval systems approximately doubled. The conference was also the first to hold large-scale evaluations of non-English documents, speech, video and retrieval across languages. Additionally, the challenges have inspired a large body of publications. Technology first developed in TREC is now included in many of the world's commercial search engines.  An independent report by RTII found that "about one-third of the improvement in web search engines from 1999 to 2009 is attributable to TREC. Those enhancements likely saved up to 3 billion hours of time using web search engines. ... Additionally, the report showed that for every $1 that NIST and its partners invested in TREC, at least $3.35 to $5.07 in benefits were accrued to U.S. information retrieval researchers in both the private sector and academia."
While one study suggests that the state of the art for ad hoc search has not advanced substantially in the past decade, it is referring just to search for topically relevant documents in small news and web collections of a few gigabytes.  There have been advances in other types of ad hoc search in the past decade.  For example, test collections were created for known-item web search which found improvements from the use of anchor text, title weighting and url length, which were not useful techniques on the older ad hoc test collections.  In 2009, a new billion-page web collection was introduced, and spam filtering was found to be a useful technique for ad hoc web search, unlike in past test collections.
The test collections developed at TREC are useful not just for (potentially) helping researchers advance the state of the art, but also for allowing developers of new (commercial) retrieval products to evaluate their effectiveness on standard tests.  In the past decade, TREC has created new tests for enterprise e-mail search, genomics search, spam filtering, e-Discovery, and several other retrieval domains.
TREC systems often provide a baseline for further research.  Examples include:

Hal Varian, Chief Economist at Google, says Better data makes for better science. The history of information retrieval illustrates this principle well," and describes TREC's contribution.
TREC's Legal track has influenced the e-Discovery community both in research and in evaluation of commercial vendors.
The IBM researcher team building IBM Watson (aka DeepQA), which beat the world's best Jeopardy! players, used data and systems from TREC's QA Track as baseline performance measurements.

Participation
The conference is made up of a varied, international group of researchers and developers. In 2003, there were 93 groups from both academia and industry from 22 countries participating.

See also
List of computer science awards

References
External links
TREC website at NIST
TIPSTER
The TREC book (at Amazon)
Text simplification is an operation used in natural language processing to modify, enhance,  classify or otherwise process an existing corpus of human-readable text in such a way that the grammar and structure of the prose is greatly simplified, while the underlying meaning and information remains the same. Text simplification is an important area of research, because natural human languages ordinarily contain large vocabularies and complex compound constructions that are not easily processed through automation. In terms of reducing language diversity, semantic compression can be employed to limit and simplify a set of words used in given texts.

Example
Text Simplification is illustrated with an example from Siddharthan (2006). The first sentence contains two relative clauses and one conjoined verb phrase. A text simplification system aims to simplify the first sentence to the second sentence.

Also contributing to the firmness in copper, the analyst noted, was a report by Chicago purchasing agents, which precedes the full purchasing agents report that is due out today and gives an indication of what the full report might hold.
Also contributing to the firmness in copper, the analyst noted, was a report by Chicago purchasing agents. The Chicago report precedes the full purchasing agents report. The Chicago report gives an indication of what the full report might hold. The full report is due out today.One approach to text simplification is lexical simplification via lexical substitution, a two-step process consisting of identifying complex words and replacing them with simpler synonyms. A key challenge here is identifying complex words, which is performed by a machine learning classifier trained on labelled data. An improvement over classical methods of applying binary labels to words as simple or complex is to ask labellers to sort words in order of complexity; this results in higher consistency of resultant labels.

See also
Automated paraphrasing
Controlled natural language
Lexical simplification
Lexical substitution
Semantic compression
Text normalization
Simplified English
Basic English

References
Wei Xu, Chris Callison-Burch and Courtney Napoles. "Problems in Current Text Simplification Research". In Transactions of the Association for Computational Linguistics (TACL), Volume 3, 2015, Pages 283–297.
Advaith Siddharthan. "Syntactic Simplification and Text Cohesion". In Research on Language and Computation, Volume 4, Issue 1, Jun 2006, Pages 77–109, Springer Science, the Netherlands.
Siddhartha Jonnalagadda, Luis Tari, Joerg Hakenberg, Chitta Baral and Graciela Gonzalez. Towards Effective Sentence Simplification for Automatic Processing of Biomedical Text. In Proc. of the NAACL-HLT 2009, Boulder, USA, June. [1]

External links
Automatic Induction of Rules for Text Simplification (pdf)
Text Simplification for Information-Seeking Applications
TipTop Technologies offers a real-time web, social search engine with a unique platform for semantic analysis of natural language. TipTop Search provides results capturing individual and group sentiment, opinions, and experiences from content of various sorts including real-time messages from Twitter or consumer product reviews on Amazon.com.  TipTop Technologies and ITC Infotech have worked together to develop a semantic engine and search interface for both enterprise and consumer applications. TipTop's products are part of the "emerging Web 3.0 applications which use semantic technologies to augment the underlying Web system's functionalities."Jonathan Albright, professor at Elon University, found videos generated by TipTop Technologies software on YouTube in his research into artificial intelligence, and described it as AI-generated "fake news."Through semantic analysis of large data sets, TipTop gleaned behavioral insights from Tweets around events like Halloween, Thanksgiving, Holiday Gifting, the Super Bowl and the Academy Awards: 2010 Oscar Nominees coverage. Sentiment analysis, concept trend tracking  and real-time market research are other applications included in the TipTop Search product. TipTop's insight engine solves the problem of real-time data noise, and its ability "to sort the “good tweets” from the “bad tweets” when it comes to a product, service or a region..."In addition, products like TipTop Shopping with customizable search widgets, bring together consumer reviews, social search, and sentiment analysis enabling product comparisons across attributes like overall value and aiding purchasing decisions through user-driven product tips and pits. TipTop Finance adds another complexity to real-time search results by incorporating corporate sentiment, company stock tickers and social media into TipTop's existing social search platform. Additional success applying semantic technologies has been with polling, "if you compare these Gallup  results with TipTop, a sentiment engine based on Twitter, the results are not way off. It does surprise you but it tells me that sentiment analysis in case of public opinion about a burning social issue or a famous personality is relatively easier.".   With the increasing amount of unstructured, opinion-oriented, and user-generated content available on the Web, TipTop's technology aims to make sense of all this data, and deliver it in a useful way for consumer and enterprise users alike.TipTop Technologies is a privately held company headquartered in Silicon Valley, California with team members located globally.


== References ==
Transderivational search (often abbreviated to TDS) is a psychological and cybernetics term, meaning when a search is being conducted for a fuzzy match across a broad field. In computing the equivalent function can be performed using content-addressable memory.
Unlike usual searches, which look for literal (i.e. exact, logical, or regular expression) matches, a transderivational search is a search for a possible meaning or possible  match as part of communication, and without which an incoming communication cannot be made any sense of whatsoever. It is thus an integral part of processing language, and of attaching meaning to communication.
A psychological example of TDS is in Ericksonian hypnotherapy, where vague suggestions are used that the patient must process intensely in order to find their own meanings, thus ensuring that the practitioner does not intrude his own beliefs into the subject's inner world.

TDS in human communication and processing
Because TDS is a compelling, automatic and unconscious state of internal focus and processing (i.e. a type of everyday trance state), and often a state of internal lack of certainty, or openness to finding an answer (since something is being checked out at that moment), it can be utilized or interrupted, in order to create, or deepen, trance.
TDS is a fundamental part of human language and cognitive processing. Arguably, every word or utterance a person hears, for example, and everything they see or feel and take note of, results in a very brief trance while TDS is carried out to establish a contextual meaning for it.

Examples
Leading statements:

"And those thoughts you had yesterday..." the human mind cannot process hearing this phrase, without at some level searching internally for some thoughts or other that it had yesterday, to make the subject of the sentence.
"The many colors that fruit can be" likewise starts the human mind considering even if briefly, different fruit sorted by color.
"You did it again, didn't you!" This everyday manipulative use of TDS usually sends the recipient looking internally for some "it" they may have done for which blame is being fairly given. Regardless of whether such a matter can be identified, guilt or anger may result.
"There has been pain, hasn't there" the mind of a patient suffering an illness will find it very hard or impossible to hear or answer this sentence without conducting internal searches to verify whether this is true or not, or to find an example if so.
"You'd forgotten something [or: some part of your body], hadn't you?" the mind usually checks through the various things, or parts of the body, on hearing this, seeing if each in turn has been forgotten.Textual ambiguity:

"Do you remember line dancing on the steps?" Without sufficient context, some statements may trigger TDS in order to resolve inherent ambiguity in the interpretation of a posed question. Do I remember a bygone fad called "line dancing on the steps"? Do I remember personally engaging in dancing in the past? Do I remember my routine practice dancing by focusing on the steps of the dance? Do I tend to forget about dancing when I am standing on steps?
"Penny-wise and pound the table dance to the beat of a different drummer". The mixing of cliché and stock phrases may trigger TDS in order to reconcile the discrepancies between expected and actual utterances in sequence.Although TDS is often associated with spoken language, it can be induced in any perceptual system. Thus Milton Erickson's "hypnotic handshake" is a technique that leaves the other person performing TDS in search of meaning to a deliberately ambiguous use of touch.

References
See also
Ambiguity
Milton Erickson
Hypnosis
Linguistics
Presupposition
Artificial intelligence
Natural language processing
Information extraction
Content-addressable memory
Garden path sentence
Trigrams are a special case of the n-gram, where n is 3. They are often used in natural language processing for performing statistical analysis of texts and in cryptography for control and use of ciphers and codes.

Frequency
Context is very important, varying analysis rankings and percentages are easily derived by drawing from different sample sizes, different authors; or different document types: poetry, science-fiction, technology documentation; and writing levels: stories for children versus adults, military orders, and recipes.
Typical cryptanalytic frequency analysis finds that the 16 most common character-level trigrams in English are:
Because encrypted messages sent by telegraph often omit punctuation and spaces,
cryptographic frequency analysis of such messages includes trigrams that straddle word boundaries. This causes trigrams such as "edt" to occur frequently, even though it may never occur in any one word of those messages.

Examples
The sentence "the quick red fox jumps over the lazy brown dog" has the following word-level trigrams:

the quick red
quick red fox
red fox jumps
fox jumps over
jumps over the
over the lazy
the lazy brown
lazy brown dog

And the word-level trigram "the quick red" has the following character-level trigrams (where an underscore "_" marks a space):

the
he_
e_q
_qu
qui
uic
ick
ck_
k_r
_re
red


== References ==
In linguistics, a triphone is a sequence of three consecutive  phonemes. Triphones are useful in models of natural language processing where they are used to establish the various contexts in which a phoneme can occur in a particular natural language.

See also
Diphone


== References ==
Vocabulary mismatch is a common phenomenon in the usage of natural languages, occurring when different people name the same thing or concept differently.
Furnas et al. (1987) were perhaps the first to quantitatively study the vocabulary mismatch problem.  Their results show that on average 80% of the times different people (experts in the same field) will name the same thing differently.  There are usually tens of possible names that can be attributed to the same thing.  This research motivated the work on latent semantic indexing.
The vocabulary mismatch between user created queries and relevant documents in a corpus causes the term mismatch problem in information retrieval.  Zhao and Callan (2010) were perhaps the first to quantitatively study the vocabulary mismatch problem in a retrieval setting.  Their results show that an average query term fails to appear in 30-40% of the documents that are relevant to the user query.  They also showed that this probability of mismatch is a central probability in one of the fundamental probabilistic retrieval models, the Binary Independence Model.  They developed novel term weight prediction methods that can lead to potentially 50-80% accuracy gains in retrieval over strong keyword retrieval models.  Further research along the line shows that expert users can use Boolean Conjunctive Normal Form expansion to improve retrieval performance by 50-300% over unexpanded keyword queries.

Techniques that solve mismatch
Stemming
Full-text indexing instead of only indexing keywords or abstracts
Indexing text on inbound links from other documents (or other social tagging
Query expansion.  A 2012 study by Zhao and Callan using expert created manual Conjunctive normal form queries has shown that searchonym expansion in the Boolean conjunctive normal form is much more effective than the traditional bag of word expansion e.g. Rocchio expansion.
Translation-based models


== References ==
Voice computing is the discipline that develops hardware or software to process voice inputs.It spans many other fields including human-computer interaction, conversational computing, linguistics, natural language processing, automatic speech recognition, speech synthesis, audio engineering, digital signal processing, cloud computing, data science, ethics, law, and information security.
Voice computing has become increasingly significant in modern times, especially with the advent of smart speakers like the Amazon Echo and Google Assistant, a shift towards serverless computing, and improved accuracy of speech recognition and text-to-speech models.

History
Voice computing has a rich history. First, scientists like Wolfgang Kempelen started to build speech machines to produce the earliest synthetic speech sounds. This led to further work by Thomas Edison to record audio with dictation machines and play it back in corporate settings. In the 1950s-1960s there were primitive attempts to build automated speech recognition systems by Bell Labs, IBM, and others. However, it was not until the 1980s that Hidden Markov Models were used to recognize up to 1,000 words that speech recognition systems became relevant.

Around 2011, Siri emerged on Apple iPhones as the first voice assistant accessible to consumers. This innovation led to a dramatic shift to building voice-first computing architectures. PS4 was released by Sony in North America in 2013 (70+ million devices), Amazon released the Amazon Echo in 2014 (30+ million devices), Microsoft released Cortana (2015 - 400 million Windows 10 users), Google released Google Assistant (2016 - 2 billion active monthly users on Android phones), and Apple released HomePod (2018 - 500,000 devices sold and 1 billion devices active with iOS/Siri). These shifts, along with advancements in cloud infrastructure (e.g. Amazon Web Services) and codecs, have solidified the voice computing field and made it widely relevant to the public at large.

Hardware
A voice computer is assembled hardware and software to process voice inputs.
Note that voice computers do not necessarily need a screen, such as in the traditional Amazon Echo. In other embodiments, traditional laptop computers or mobile phones could be used as voice computers. Moreover, there has become increasingly more interfaces for voice computers with the advent of IoT-enabled devices, such as within cars or televisions.
As of September 2018, there are currently over 20,000 types of devices compatible with Amazon Alexa.

Software
Voice computing software can read/write, record, clean, encrypt/decrypt, playback, transcode, transcribe, compress, publish, featurize, model, and visualize voice files.
Here are some popular software packages related to voice computing:

Applications
Voice computing applications span many industries including voice assistants, healthcare, e-Commerce, finance, supply chain, agriculture, text-to-speech, security, marketing, customer support, recruiting, cloud computing, microphones, speakers, and podcasting. Voice technology is projected to grow at a CAGR of 19-25% by 2025, making it an attractive industry for startups and investors alike.

Legal considerations
In the United States, the states have varying telephone call recording laws. In some states, it is legal to record a conversation with the consent of only one party, in others the consent of all parties is required.
Moreover, COPPA is a significant law to protect minors using the Internet. With an increasing number of minors interacting with voice computing devices (e.g. the Amazon Alexa), on October 23, 2017 the Federal Trade Commission relaxed the COPAA rule so that children can issue voice searches and commands.Lastly, GDPR is a new European law that governs the right to be forgotten and many other clauses for EU citizens. GDPR also is clear that companies need to outline clear measures to obtain consent if audio recordings are made and define the purpose and scope as to how these recordings will be used, e.g., for training purposes. The bar for valid consent has been raised under the GDPR. Consents must be freely given, specific, informed, and unambiguous; tacit consent is no longer sufficient.

Research conferences
There are many research conferences that relate to voice computing. Some of these include:

International Conference on Acoustics, Speech, and Signal Processing
Interspeech 
AVEC 
IEEE Int'l Conf. on Automatic Face and Gesture Recognition 
ACII2019 The 8th Int'l Conf. on Affective Computing and Intelligent Interaction

Developer community
Google Assistant has roughly 2,000 actions as of January 2018.There are over 50,000 Alexa skills worldwide as of September 2018.In June 2017, Google released AudioSet, a large-scale collection of human-labeled 10-second sound clips drawn from YouTube videos. It contains 1,010,480 videos of human speech files, or 2,793.5 hours in total. It was released as part of the IEEE ICASSP 2017 Conference.In November 2017, Mozilla Foundation released the Common Voice Project, a collection of speech files to help contribute to the larger open source machine learning community. The voicebank is currently 12GB in size, with more than 500 hours of English-language voice data that have been collected from 112 countries since the project's inception in June 2017. This dataset has already resulted in creative projects like the DeepSpeech model, an open source transcription model.

See also
Speech recognition
Natural Language Processing
Voice user interface
Audio codec
Ubiquitous computing
Hands-free computing


== References ==
Voice portals are the voice equivalent of web portals, giving access to information through spoken commands and voice responses. Ideally a voice portal could be an access point for any type of information, services, or transactions found on the Internet. Common uses include movie time listings and stock trading.
In telecommunications circles, voice portals may be referred to as interactive voice response (IVR) systems, but this term also includes DTMF services.
With the emergence of conversational assistants such as Apple's Siri, Amazon Alexa, Google Assistant, Microsoft Cortana, and Samsung's Bixby, Voice Portals can now be accessed through mobile devices and Far Field voice smart speakers such as the Amazon Echo and Google Home.

Advantages
Voice portals have no dependency on the access device; even low end mobile handsets can access the service. Voice portals talk to users in their local language and there is reduced customer learning required for using voice services compared to Internet/SMS based services.A complex search query that otherwise would take multiple widgets (drop down, check box, text box filling), can easily and effortlessly be formulated by anyone who can speak without needing to be familiar with any visual interfaces.  For instance, one can say, "Find me an eyeliner, not too thick, dark brown, from Estee Lauder MAC, that's below thirty dollars" or "What is the closest liquor store from here and what time do they close?"

Limitations
Voice is the most natural communication medium, but the information that can be provided is limited compared to visual media.For example, most Internet users try a search term, scan results, then adjust the search term to eliminate irrelevant results.  They may take two or three quick iterations to get a list that they are confident will contain what they are looking for.  The equivalent approach is not practical when results are spoken, as it would take far too long. In this case, a multimodal interaction would be preferable to a voice-only interface.

Trends
Live-agent and Internet-based voice portals are converging, and the range of information they can provide is expanding.
Live-agent portals are introducing greater automation through speech recognition and text-to-speech technology, in many cases providing fully automated service, while automated Internet-based portals are adding operator fallback in premium services. The live-agent portals, which used to rely entirely on pre-structured databases holding specific types of information are expanding into more free-form Internet access, while the Internet-based portals are adding pre-structured content to improve automation of the more common types of request.Speech technology is starting to introduce Artificial Intelligence concepts that make it practical
to recognise a much broader range of utterances, learning from experience.  This promises to make it practical to greatly improve speaker recognition rates and expand the range of information that can be provided by a voice portal.

Technology Providers
A number of web-based companies are dedicated to providing voice-based access to Internet information to consumers.
Quack.com launched its service in March 2000 and has since obtained the first overall voiceportal patent.Quack.com was acquired by AOL in 2000 and relaunched as AOL By Phone later that year. Tellme Networks was acquired by Microsoft in 2007.
Nuance, the dominant provider of speech recognition and text-to-speech technology, is starting to deliver voice portal solutions. Other companies in this space include TelSurf Networks, FonGenie, Apptera and Call Genie.
Apart from public voice portal services, a number of technology companies, including Alcatel-Lucent, Avaya, and Cisco, offer commercial enterprise-grade voice portal products to be used by companies to serve their clients. Avaya also has a carrier-grade portfolio.

See also
Call avoidance
Mobile Search
Mobile local search

References
External links
Designing the Voice User Interface for Automated Directory Assistance.  Amir Mané and Esther Levin
888-TelSurf (beta) Review & Rating | PCMag.com
Start-ups dream of a Web that talks
VoiceDBC: A semi-automatic tool for writing speech applications. Honours Thesis 2002. Stephen Choularton PhD
In natural language processing a w-shingling is a set of unique shingles (therefore n-grams) each of which is composed of contiguous subsequences of tokens within a document, which can then be used to ascertain the similarity between documents.  The symbol w denotes the quantity of tokens in each shingle selected, or solved for.
The document, "a rose is a rose is a rose" can therefore be maximally tokenized as follows:

(a,rose,is,a,rose,is,a,rose)The set of all contiguous sequences of 4 tokens (Thus 4=n, thus 4-grams) is

{ (a,rose,is,a), (rose,is,a,rose), (is,a,rose,is), (a,rose,is,a), (rose,is,a,rose) } Which can then be reduced, or maximally shingled in this particular instance to { (a,rose,is,a), (rose,is,a,rose), (is,a,rose,is) }.

Resemblance
For a given shingle size, the degree to which two documents A and B resemble each other can be expressed as the ratio of the magnitudes of their shinglings' intersection and union, or

  
    
      
        r
        (
        A
        ,
        B
        )
        =
        
          
            
              
                |
              
              S
              (
              A
              )
              ∩
              S
              (
              B
              )
              
                |
              
            
            
              
                |
              
              S
              (
              A
              )
              ∪
              S
              (
              B
              )
              
                |
              
            
          
        
      
    
    {\displaystyle r(A,B)={{|S(A)\cap S(B)|} \over {|S(A)\cup S(B)|}}}
  where |A| is the size of set A.  The resemblance is a number in the range [0,1], where 1 indicates that two documents are identical. This definition is identical with the Jaccard coefficient describing similarity and diversity of sample sets.

See also
Concept mining (alternative method for document similarity calculation with more computational complexity, but where the measure more closely models a human's perception of document similarity)
N-gram
k-mer
MinHash
Rolling hash
Rabin fingerprint
Vector space model
Bag-of-words model

References
(Manber 1993) Finding Similar Files in a Large File System. Does not yet use the term "shingling".
(Broder, Glassman, Manasse, and Zweig 1997) Syntactic Clustering of the Web.  SRC Technical Note #1997-015.

External links
Manning, Christopher D.; Raghavan, Prabhakar; Schütze, Hinrich (7 July 2008). "w-shingling". Introduction to Information Retrieval. Cambridge University Press. ISBN 978-1-139-47210-4.
William Aaron Woods (born June 17, 1942), generally known as Bill Woods, is a researcher in natural language processing, continuous speech understanding, knowledge representation, and knowledge-based search technology. He is currently interested in using technology to help people organize and use information in organizations.

Education
Woods received a Bachelor's degree from Ohio Wesleyan University (1964) and a Master's (1965) and Ph.D. (1968) in Applied Mathematics from Harvard University, where he then served as an Assistant Professor and later as a Gordon McKay Professor of the Practice of Computer Science.

Research
Woods built one of the first natural language question answering systems (LUNAR) to answer questions about the Apollo 11 moon rocks for the NASA Manned Spacecraft Center while he was at Bolt Beranek and Newman (BBN) in Cambridge, Massachusetts. At BBN, he was a Principal Scientist and manager of the Artificial Intelligence Department in the '70's and early '80's. He was the principal investigator for BBN's early work in natural language processing and knowledge representation and for its first project in continuous speech understanding. Subsequently, he was Chief Scientist for Applied Expert Systems and Principal Technologist for ON Technology, Cambridge start-ups. In 1991, he joined Sun Microsystems Laboratories as a Principal Scientist and Distinguished Engineer, and in 2007, he joined ITA Software as a Distinguished Software Engineer.  ITA was acquired by Google in 2011, where he now works.
Woods' 1975 paper "What's in a Link" is a widely cited critical review of early work in semantic networks.  This paper has been cited in the context of querying and natural language processing approaches that make use of Semantic Networks and general knowledge modeling. The paper attempts to clarify notions of meaning and semantics in computational systems. Woods further elaborated on the issues and how they relate to contemporary systems in "Meaning and Links" (2007).

Awards
Woods has received many honors:

1978, Fulbright Fellowship
1990, Fellow of the American Association for Artificial Intelligence
1992, Fellow of the American Association for the Advancement of Science
2010, Association for Computational Linguistics Lifetime Achievement Award

Selected works
Woods, W. A. (1970). "Transition network grammars for natural language analysis". Communications of the ACM. 13 (10): 591–606. doi:10.1145/355598.362773.
"The Lunar Sciences Natural Language Information System: Final Report" (with R. M. Kaplan and B.L. Nash-Webber),  BBN Report No. 2378, Bolt Beranek and Newman Inc., Cambridge, MA 02138, June, 1972. (Available from NTIS as N72-28984.)
Speech-Understanding Systems:  Final Report of a Study Group, (with A. Newell, Chairman, et al. .), North-Holland/American Elsevier, 1973.
"An Experimental parsing System for Transition Network Grammars", in R. Rustin (ed.), Natural Language Processing, New York:  Algorithmics Press, 1973.
"Progress in Natural Language Understanding:  An Application to Lunar Geology,"  AFIPS Conference Proceedings 42 (1973 National Computer Conference and Exposition).
"What's in a Link:  Foundations for Semantic Networks" in D. Bobrow and A. Collins (eds.), Representation and Understanding: Studies in Cognitive Science, New York:  Academic Press, 1975.   Reprinted in R. Brachman and H. Levesque (eds.), Readings in Knowledge Representation, San Mateo: Morgan Kaufmann, 1985.  Also reprinted in Allan Collins and Edward E. Smith (eds.), Readings in Cognitive Science, San Mateo: Morgan Kaufmann, 1988.
"Procedural Semantics as a Theory of Meaning" in A. Joshi, B. L. Webber and I. Sag (eds.), Elements of Discourse Understanding, Cambridge University Press, 1981.
"Optimal Search Strategies for Speech Understanding Control", Artificial Intelligence 18:3:295-326, May 1982.
"What's Important About Knowledge Representation?," IEEE Computer 16:10, October 1983.
"Artificial Intelligence", in Lisa Taylor (ed.), The Phenomenon of Change, New York: Rizzoli, 1984.
Computer Speech Processing, (ed. with Frank Fallside), Prentice-Hall International (UK) Ltd., 1985.
"Understanding Subsumption and Taxonomy: A Framework for Progress," in John Sowa (ed.), Principles of Semantic Networks: Explorations in the Representation of Knowledge, San Mateo:Morgan Kaufmann, 1991, pp 45–94.
"Conceptual Indexing: A Better Way to Organize Knowledge," Technical Report SMLI TR-97-61, Sun Microsystems Laboratories, Mountain View, CA, April, 1997.
"Linguistic Knowledge can Improve Information Retrieval," with Lawrence A. Bookman, Ann Houston, Robert J. Kuhns, Paul Martin, and Stephen Green, Proceedings of ANLP-2000, Seattle, WA, May 1–3, 2000, (Final version with author's introduction is reprinted in Sun Labs' anniversary volume, Sun Microsystems Laboratories: The First Ten Years, 1991-2001.)
"Meaning and Links: a Semantic Odyssey," AI Magazine 28:4 (Winter 2007). full text

References
External links
Official website
Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with many dimensions per word to a continuous vector space with a much lower dimension.
Methods to generate this mapping include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, explainable knowledge base method, and explicit representation in terms of the context in which words appear.Word and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as syntactic parsing and sentiment analysis.

Development of technique
In linguistics word embeddings were discussed in the research area of distributional semantics. It aims to quantify and categorize semantic similarities between linguistic items based on their distributional properties in large samples of language data.  The underlying idea that "a word is characterized by the company it keeps" was popularized by Firth.The technique of representing words as vectors has roots in the 1960s with the development of the vector space model for information retrieval. Reducing the number of dimensions using singular value decomposition then led to the introduction of latent semantic analysis in the late 1980s. In 2000 Bengio et al. provided in a series of papers the "Neural probabilistic language models" to reduce the high dimensionality of words representations in contexts by "learning a distributed representation for words". (Bengio et al., 2003). Word embeddings come in two different styles, one in which words are expressed as vectors of co-occurring words, and another in which words are expressed as vectors of linguistic contexts in which the words occur; these different styles are studied in (Lavelli et al., 2004). Roweis and Saul published in Science how to use "locally linear embedding" (LLE) to discover representations of high dimensional data structures. The area developed gradually and really took off after 2010, partly because important advances had been made since then on the quality of vectors and the training speed of the model.
There are many branches and many research groups working on word embeddings. In 2013, a team at Google led by Tomas Mikolov created word2vec, a word embedding toolkit which can train vector space models faster than the previous approaches. Most new word embedding techniques rely on a neural network architecture instead of more traditional n-gram models and unsupervised learning.

Limitations
One of the main limitations of word embeddings (word vector space models in general) is that words with multiple meanings are conflated into a single representation (a single vector in the semantic space). In other words, polysemy and homonymy are not handled properly. For example, in the sentence “The club I tried yesterday was great!” it is not clear if the term club is related to the word sense of a  club sandwich, baseball club, clubhouse, golf club, or any other sense that club might have it. The necessity to accommodate multiple meanings per word in different vectors (multi-sense embeddings) is the motivation for several contributions in NLP to split single-sense embeddings into multi-sense ones.Most approaches that produce multi-sense embeddings can be divided into two main categories for their word sense representation, i.e., unsupervised and knowledge-based. Based on word2vec skip-gram, Multi-Sense Skip-Gram (MSSG) performs word-sense discrimination and embedding simultaneously, improving its training time, while assuming a specific number of senses for each word. In the Non-Parametric Multi-Sense Skip-Gram (NP-MSSG) this number can vary depending on each word. Combining the prior knowledge of lexical databases (e.g., WordNet, ConceptNet, BabelNet), word embeddings and word sense disambiguation, Most Suitable Sense Annotation (MSSA) labels word-senses through an unsupervised and knowledge-based approach considering a word’s context in a pre-defined sliding window. Once the words are disambiguated, they can be used in a standard word embeddings technique, so multi-sense embeddings are produced. MSSA architecture allows the disambiguation and annotation process to be performed recurrently in a self-improving manner.
The use of multi-sense embeddings is known to improve performance in several NLP tasks, such as part-of-speech tagging, semantic relation identification, and semantic relatedness. However, tasks involving named entity recognition and sentiment analysis seem not to benefit from a multiple vector representation.

For biological sequences: BioVectors
Word embeddings for n-grams in biological sequences (e.g. DNA, RNA, and Proteins) for bioinformatics applications have been proposed by Asgari and Mofrad. Named bio-vectors (BioVec) to refer to biological sequences in general with protein-vectors (ProtVec) for proteins (amino-acid sequences) and gene-vectors (GeneVec) for gene sequences, this representation can be widely used in applications of deep learning in proteomics and genomics. The results presented by Asgari and Mofrad suggest that BioVectors can characterize biological sequences in terms of biochemical and biophysical interpretations of the underlying patterns.

Thought vectors
Thought vectors are an extension of word embeddings to entire sentences or even documents. Some researchers hope that these can improve the quality of machine translation.

Software
Software for training and using word embeddings includes Tomas Mikolov's Word2vec, Stanford University's GloVe, AllenNLP's Elmo, fastText, Gensim, Indra and Deeplearning4j. Principal Component Analysis (PCA) and T-Distributed Stochastic Neighbour Embedding (t-SNE) are both  used to reduce the dimensionality of word vector spaces and visualize word embeddings and clusters.

Examples of application
For instance, the fastText is also used to calculate word embeddings for text corpora in Sketch Engine that are available online.

See also
Distributional semantics
Latent semantic analysis
Brown clustering
GloVe
word2vec
fastText
Gensim
BERT (language model)
ELMo


== References ==
In computational linguistics, word-sense disambiguation (WSD) is an open problem concerned with identifying which sense of a word is used in a sentence. The solution to this issue impacts other computer-related writing, such as discourse, improving relevance of search engines, anaphora resolution, coherence, and inference.
The human brain is quite proficient at word-sense disambiguation. That natural language is formed in a way that requires so much of it is a reflection of that neurologic reality. In other words, human language developed in a way that reflects (and also has helped to shape) the innate ability provided by the brain's neural networks. In computer science and the information technology that it enables, it has been a long-term challenge to develop the ability in computers to do natural language processing and machine learning.
A rich variety of techniques have been researched, from dictionary-based methods that use the knowledge encoded in lexical resources, to supervised machine learning methods in which a classifier is trained for each distinct word on a corpus of manually sense-annotated examples, to completely unsupervised methods that cluster occurrences of words, thereby inducing word senses. Among these, supervised learning approaches have been the most successful algorithms to date.
Accuracy of current algorithms is difficult to state without a host of caveats. In English, accuracy at the coarse-grained (homograph) level is routinely above 90%, with some methods on particular homographs achieving over 96%. On finer-grained sense distinctions, top accuracies from 59.1% to 69.0% have been reported in evaluation exercises (SemEval-2007, Senseval-2), where the baseline accuracy of the simplest possible algorithm of always choosing the most frequent sense was 51.4% and 57%, respectively.

About
Disambiguation requires two strict inputs: a dictionary to specify the senses which are to be disambiguated and a corpus of language data to be disambiguated (in some methods, a training corpus of language examples is also required). WSD task has two variants: "lexical sample" and "all words" task. The former comprises disambiguating the occurrences of a small sample of target words which were previously selected, while in the latter all the words in a piece of running text need to be disambiguated. The latter is deemed a more realistic form of evaluation, but the corpus is more expensive to produce because human annotators have to read the definitions for each word in the sequence every time they need to make a tagging judgement, rather than once for a block of instances for the same target word.
To give a hint how all this works, consider three examples of the distinct senses that exist for the (written) word "bass":

a type of fish
tones of low frequency
a type of instrumentand the sentences:

I went fishing for some sea bass.
The bass line of the song is too weak.To people who understand English, the first sentence is using the word "bass (fish)" , as in the former sense above and in the second sentence, the word "bass (instrument)"  is being used as in the latter sense below. Developing algorithms to replicate this human ability can often be a difficult task, as is further exemplified by the implicit equivocation between "bass (sound)" and "bass (instrument)".

History
WSD was first formulated into as a distinct computational task during the early days of machine translation in the 1940s, making it one of the oldest problems in computational linguistics. Warren Weaver, in his famous 1949 memorandum on translation, first introduced the problem in a computational context. Early researchers understood the significance and difficulty of WSD well. In fact, Bar-Hillel (1960) used the above example to argue that WSD could not be solved by "electronic computer" because of the need in general to model all world knowledge.
In the 1970s, WSD was a subtask of semantic interpretation systems developed within the field of artificial intelligence, starting with Wilks' preference semantics. However, since WSD systems were at the time largely rule-based and hand-coded they were prone to a knowledge acquisition bottleneck.
By the 1980s large-scale lexical resources, such as the Oxford Advanced Learner's Dictionary of Current English (OALD), became available: hand-coding was replaced with knowledge automatically extracted from these resources, but disambiguation was still knowledge-based or dictionary-based.
In the 1990s, the statistical revolution swept through computational linguistics, and WSD became a paradigm problem on which to apply supervised machine learning techniques.
The 2000s saw supervised techniques reach a plateau in accuracy, and so attention has shifted to coarser-grained senses, domain adaptation, semi-supervised and unsupervised corpus-based systems, combinations of different methods, and the return of knowledge-based systems via graph-based methods. Still, supervised systems continue to perform best.

Difficulties
Differences between dictionaries
One problem with word sense disambiguation is deciding what the senses are. In cases like the word bass above, at least some senses are obviously different. In other cases, however, the different senses can be closely related (one meaning being a metaphorical or metonymic extension of another), and in such cases division of words into senses becomes much more difficult. Different dictionaries and thesauruses will provide different divisions of words into senses. One solution some researchers have used is to choose a particular dictionary, and just use its set of senses. Generally, however, research results using broad distinctions in senses have been much better than those using narrow ones. However, given the lack of a full-fledged coarse-grained sense inventory, most researchers continue to work on fine-grained WSD.
Most research in the field of WSD is performed by using WordNet as a reference sense inventory for English. WordNet is a computational lexicon that encodes concepts as synonym sets (e.g. the concept of car is encoded as { car, auto, automobile, machine, motorcar }). Other resources used for disambiguation purposes include Roget's Thesaurus and Wikipedia. More recently, BabelNet, a multilingual encyclopedic dictionary, has been used for multilingual WSD.

Part-of-speech tagging
In any real test, part-of-speech tagging and sense tagging are very closely related with each potentially making constraints to the other. And the question whether these tasks should be kept together or decoupled is still not unanimously resolved, but recently scientists incline to test these things separately (e.g. in the Senseval/SemEval competitions parts of speech are provided as input for the text to disambiguate).
It is instructive to compare the word sense disambiguation problem with the problem of part-of-speech tagging. Both involve disambiguating or tagging with words, be it with senses or parts of speech. However, algorithms used for one do not tend to work well for the other, mainly because the part of speech of a word is primarily determined by the immediately adjacent one to three words, whereas the sense of a word may be determined by words further away. The success rate for part-of-speech tagging algorithms is at present much higher than that for WSD, state-of-the art being around 95% accuracy or better, as compared to less than 75% accuracy in word sense disambiguation with supervised learning. These figures are typical for English, and may be very different from those for other languages.

Inter-judge variance
Another problem is inter-judge variance. WSD systems are normally tested by having their results on a task compared against those of a human. However, while it is relatively easy to assign parts of speech to text, training people to tag senses is far more difficult. While users can memorize all of the possible parts of speech a word can take, it is often impossible for individuals to memorize all of the senses a word can take. Moreover, humans do not agree on the task at hand – give a list of senses and sentences, and humans will not always agree on which word belongs in which sense.As human performance serves as the standard, it is an upper bound for computer performance. Human performance, however, is much better on coarse-grained than fine-grained distinctions, so this again is why research on coarse-grained distinctions has been put to test in recent WSD evaluation exercises.

Pragmatics
Some AI researchers like Douglas Lenat argue that one cannot parse meanings from words without some form of common sense ontology. This linguistic issue is called pragmatics.
For example, comparing these two sentences:

"Jill and Mary are mothers." – (each is independently a mother).
"Jill and Mary are sisters." – (they are sisters of each other).To properly identify senses of words one must know common sense facts. Moreover, sometimes the common sense is needed to disambiguate such words like pronouns in case of having anaphoras or cataphoras in the text.

Sense inventory and algorithms' task-dependency
A task-independent sense inventory is not a coherent concept: each task requires its own division of word meaning into senses relevant to the task. For example, the ambiguity of 'mouse' (animal or device) is not relevant in English-French machine translation, but is relevant in information retrieval. The opposite is true of 'river', which requires a choice in French (fleuve 'flows into the sea', or rivière 'flows into a river').
Also, completely different algorithms might be required by different applications. In machine translation, the problem takes the form of target word selection. Here, the "senses" are words in the target language, which often correspond to significant meaning distinctions in the source language ("bank" could translate to the French "banque"—that is, 'financial bank' or "rive"—that is, 'edge of river'). In information retrieval, a sense inventory is not necessarily required, because it is enough to know that a word is used in the same sense in the query and a retrieved document; what sense that is, is unimportant.

Discreteness of senses
Finally, the very notion of "word sense" is slippery and controversial. Most people can agree in distinctions at the coarse-grained homograph level (e.g., pen as writing instrument or enclosure), but go down one level to fine-grained polysemy, and disagreements arise. For example, in Senseval-2, which used fine-grained sense distinctions, human annotators agreed in only 85% of word occurrences. Word meaning is in principle infinitely variable and context sensitive. It does not divide up easily into distinct or discrete sub-meanings. Lexicographers frequently discover in corpora loose and overlapping word meanings, and standard or conventional meanings extended, modulated, and exploited in a bewildering variety of ways. The art of lexicography is to generalize from the corpus to definitions that evoke and explain the full range of meaning of a word, making it seem like words are well-behaved semantically. However, it is not at all clear if these same meaning distinctions are applicable in computational applications, as the decisions of lexicographers are usually driven by other considerations. In 2009, a task – named lexical substitution – was proposed as a possible solution to the sense discreteness problem. The task consists of providing a substitute for a word in context that preserves the meaning of the original word (potentially, substitutes can be chosen from the full lexicon of the target language, thus overcoming discreteness).

Approaches and methods
As in all natural language processing, there are two main approaches to WSD – deep approaches and shallow approaches.
Deep approaches presume access to a comprehensive body of world knowledge. Knowledge, such as "you can go fishing for a type of fish, but not for low frequency sounds" and "songs have low frequency sounds as parts, but not types of fish", is then used to determine in which sense the word bass is used. These approaches are not very successful in practice, mainly because such a body of knowledge does not exist in a computer-readable format, outside very limited domains. However, if such knowledge did exist, then deep approaches would be much more accurate than the shallow approaches. Also, there is a long tradition in computational linguistics, of trying such approaches in terms of coded knowledge and in some cases, it is hard to say clearly whether the knowledge involved is linguistic or world knowledge. The first attempt was that by Margaret Masterman and her colleagues, at the Cambridge Language Research Unit in England, in the 1950s. This attempt used as data a punched-card version of Roget's Thesaurus and its numbered "heads", as an indicator of topics and looked for repetitions in text, using a set intersection algorithm. It was not very successful, but had strong relationships to later work, especially Yarowsky's machine learning optimisation of a thesaurus method in the 1990s.
Shallow approaches don't try to understand the text. They just consider the surrounding words, using information such as "if bass has words sea or fishing nearby, it probably is in the fish sense; if bass has the words music or song nearby, it is probably in the music sense." These rules can be automatically derived by the computer, using a training corpus of words tagged with their word senses. This approach, while theoretically not as powerful as deep approaches, gives superior results in practice, due to the computer's limited world knowledge. However, it can be confused by sentences like The dogs bark at the tree which contains the word bark near both  tree and dogs.
There are four conventional approaches to WSD:

Dictionary- and knowledge-based methods: These rely primarily on dictionaries, thesauri, and lexical knowledge bases, without using any corpus evidence.
Semi-supervised or minimally supervised methods: These make use of a secondary source of knowledge such as a small annotated corpus as seed data in a bootstrapping process, or a word-aligned bilingual corpus.
Supervised methods: These make use of sense-annotated corpora to train from.
Unsupervised methods: These eschew (almost) completely external information and work directly from raw unannotated corpora. These methods are also known under the name of word sense discrimination.Almost all these approaches normally work by defining a window of n content words around each word to be disambiguated in the corpus, and statistically analyzing those n surrounding words. Two shallow approaches used to train and then disambiguate are Naïve Bayes classifiers and decision trees. In recent research, kernel-based methods such as support vector machines have shown superior performance in supervised learning. Graph-based approaches have also gained much attention from the research community, and currently achieve performance close to the state of the art.

Dictionary- and knowledge-based methods
The Lesk algorithm is the seminal dictionary-based method. It is based on the hypothesis that words used together in text are related to each other and that the relation can be observed in the definitions of the words and their senses. Two (or more) words are disambiguated by finding the pair of dictionary senses with the greatest word overlap in their dictionary definitions. For example, when disambiguating the words in "pine cone", the definitions of the appropriate senses both include the words evergreen and tree (at least in one dictionary). A similar approach searches for the shortest path between two words: the second word is iteratively searched among the definitions of every semantic variant of the first word, then among the definitions of every semantic variant of each word in the previous definitions and so on. Finally, the first word is disambiguated by selecting the semantic variant which minimizes the distance from the first to the second word.
An alternative to the use of the definitions is to consider general word-sense relatedness and to compute the semantic similarity of each pair of word senses based on a given lexical knowledge base such as WordNet. Graph-based methods reminiscent of spreading activation research of the early days of AI research have been applied with some success. More complex graph-based approaches have been shown to perform almost as well as supervised methods or even outperforming them on specific domains. Recently, it has been reported that simple graph connectivity measures, such as degree, perform state-of-the-art WSD in the presence of a sufficiently rich lexical knowledge base. Also, automatically transferring knowledge in the form of semantic relations from Wikipedia to WordNet has been shown to boost simple knowledge-based methods, enabling them to rival the best supervised systems and even outperform them in a domain-specific setting.The use of selectional preferences (or selectional restrictions) is also useful, for example, knowing that one typically cooks food, one can disambiguate the word bass in "I am cooking basses" (i.e., it's not a musical instrument).

Supervised methods
Supervised methods are based on the assumption that the context can provide enough evidence on its own to disambiguate words (hence, common sense and reasoning are deemed unnecessary). Probably every machine learning algorithm going has been applied to WSD, including associated techniques such as feature selection, parameter optimization, and ensemble learning. Support Vector Machines and memory-based learning have been shown to be the most successful approaches, to date, probably because they can cope with the high-dimensionality of the feature space. However, these supervised methods are subject to a new knowledge acquisition bottleneck since they rely on substantial amounts of manually sense-tagged corpora for training, which are laborious and expensive to create.

Semi-supervised methods
Because of the lack of training data, many word sense disambiguation algorithms use semi-supervised learning, which allows both labeled and unlabeled data.  The Yarowsky algorithm was an early example of such an algorithm. It uses the ‘One sense per collocation’ and the ‘One sense per discourse’ properties of human languages for word sense disambiguation. From observation, words tend to exhibit only one sense in most given discourse and in a given collocation.The bootstrapping approach starts from a small amount of seed data for each word: either manually tagged training examples or a small number of surefire decision rules (e.g., 'play' in the context of 'bass' almost always indicates the musical instrument). The seeds are used to train an initial classifier, using any supervised method. This classifier is then used on the untagged portion of the corpus to extract a larger training set, in which only the most confident classifications are included. The process repeats, each new classifier being trained on a successively larger training corpus, until the whole corpus is consumed, or until a given maximum number of iterations is reached.
Other semi-supervised techniques use large quantities of untagged corpora to provide co-occurrence information that supplements the tagged corpora. These techniques have the potential to help in the adaptation of supervised models to different domains.
Also, an ambiguous word in one language is often translated into different words in a second language depending on the sense of the word. Word-aligned bilingual corpora have been used to infer cross-lingual sense distinctions, a kind of semi-supervised system.

Unsupervised methods
Unsupervised learning is the greatest challenge for WSD researchers. The underlying assumption is that similar senses occur in similar contexts, and thus senses can be induced from text by clustering word occurrences using some measure of similarity of context, a task referred to as word sense induction or discrimination. Then, new occurrences of the word can be classified into the closest induced clusters/senses. Performance has been lower than for the other methods described above, but comparisons are difficult since senses induced must be mapped to a known dictionary of word senses. If a mapping to a set of dictionary senses is not desired, cluster-based evaluations (including measures of entropy and purity) can be performed. Alternatively, word sense induction methods can be tested and compared within an application. For instance, it has been shown that word sense induction improves Web search result clustering by increasing the quality of result clusters and the degree diversification of result lists. It is hoped that unsupervised learning will overcome the knowledge acquisition bottleneck because they are not dependent on manual effort.
Representing words considering their context through fixed size dense vectors (word embeddings) has become one of the most fundamental blocks in several NLP systems. Even though most of traditional word embedding techniques conflate words with multiple meanings into a single vector representation, they still can be used to improve WSD. In addition to word embeddings techniques, lexical databases (e.g., WordNet, ConceptNet, BabelNet) can also assist unsupervised systems in mapping words and their senses as dictionaries. Some techniques that combine lexical databases and word embeddings are presented in AutoExtend and Most Suitable Sense Annotation (MSSA). In AutoExtend, they present a method that decouples an object input representation into its properties, such as words and their word senses. AutoExtend uses a graph structure to map words (e.g. text) and non-word (e.g. synsets in WordNet) objects as nodes and the relationship between nodes as edges. The relations (edges) in AutoExtend can either express the addition or similarity between its nodes. The former captures the intuition behind the offset calculus, while the latter defines the similarity between two nodes. In MSSA, an unsupervised disambiguation system uses the similarity between word senses in a fixed context window to select the most suitable word sense using a pre-trained word embedding model and WordNet. For each context window, MSSA calculates the centroid of each word sense definition by averaging the word vectors of its words in WordNet's glosses (i.e., short defining gloss and one or more usage example) using a pre-trained word embeddings model. These centroids are later used to select the word sense with the highest similarity of a target word to its immediately adjacent neighbors (i.e., predecessor and successor words). After all words are annotated and disambiguated, they can be used as a training corpus in any standard word embedding technique. In its improved version, MSSA can make use of word sense embeddings to repeat its disambiguation process iteratively.

Other approaches
Other approaches may vary differently in their methods:

Disambiguation based on operational semantics of default logic.
Domain-driven disambiguation;
Identification of dominant word senses;
WSD using Cross-Lingual Evidence.
WSD solution in John Ball's language independent NLU combining Patom Theory [1]  and RRG (Role and Reference Grammar)
Type inference in constraint-based grammars

Other languages
Hindi : Lack of lexical resources in Hindi have hindered the performance of supervised models of WSD, while the unsupervised models suffer due to extensive morphology. A possible solution to this problem is the design of a WSD model by means of parallel corpora. The creation of the Hindi WordNet has paved way for several Supervised methods which have been proven to produce a higher accuracy in disambiguating nouns.

Local impediments and summary
The knowledge acquisition bottleneck is perhaps the major impediment to solving the WSD problem. Unsupervised methods rely on knowledge about word senses, which is only sparsely formulated in dictionaries and lexical databases. Supervised methods depend crucially on the existence of manually annotated examples for every word sense, a requisite that can so far be met only for a handful of words for testing purposes, as it is done in the Senseval exercises.
One of the most promising trends in WSD research is using the largest corpus ever accessible, the World Wide Web, to acquire lexical information automatically. WSD has been traditionally understood as an intermediate language engineering technology which could improve applications such as information retrieval (IR). In this case, however, the reverse is also true: web search engines implement simple and robust IR techniques that can successfully mine the Web for information to use in WSD. The historic lack of training data has provoked the appearance of some new algorithms and techniques, as described in Automatic acquisition of sense-tagged corpora.

External knowledge sources
Knowledge is a fundamental component of WSD. Knowledge sources provide data which are essential to associate senses with words. They can vary from corpora of texts, either unlabeled or annotated with word senses, to machine-readable dictionaries, thesauri, glossaries, ontologies, etc. They can be classified as follows:
Structured:

Machine-readable dictionaries (MRDs)
Ontologies
ThesauriUnstructured:

Collocation resources
Other resources (such as word frequency lists, stoplists, domain labels, etc.)
Corpora: raw corpora and sense-annotated corpora

Evaluation
Comparing and evaluating different WSD systems is extremely difficult, because of the different test sets, sense inventories, and knowledge resources adopted. Before the organization of specific evaluation campaigns most systems were assessed on in-house, often small-scale, data sets. In order to test one's algorithm, developers should spend their time to annotate all word occurrences. And comparing methods even on the same corpus is not eligible if there is different sense inventories.
In order to define common evaluation datasets and procedures, public evaluation campaigns have been organized. Senseval (now renamed SemEval) is an international word sense disambiguation competition, held every three years since 1998: Senseval-1 (1998), Senseval-2 (2001), Senseval-3 (2004), and its successor, SemEval (2007). The objective of the competition is to organize different lectures, preparing and hand-annotating corpus for testing systems, perform a comparative evaluation of WSD systems in several kinds of tasks, including all-words and lexical sample WSD for different languages, and, more recently, new tasks such as semantic role labeling, gloss WSD, lexical substitution, etc. The systems submitted for evaluation to these competitions usually integrate different techniques and often combine supervised and knowledge-based methods (especially for avoiding bad performance in lack of training examples).
In recent years 2007-2012, the WSD evaluation task choices had grown and the criterion for evaluating WSD has changed drastically depending on the variant of the WSD evaluation task. Below enumerates the variety of WSD tasks:

Task design choices
As technology evolves, the Word Sense Disambiguation (WSD) tasks grows in different flavors towards various research directions and for more languages:

Classic monolingual WSD evaluation tasks use WordNet as the sense inventory and are largely based on supervised/semi-supervised classification with the manually sense annotated corpora:Classic English WSD uses the Princeton WordNet as it sense inventory and the primary classification input is normally based on the SemCor corpus.
Classical WSD for other languages uses their respective WordNet as sense inventories and sense annotated corpora tagged in their respective languages. Often researchers will also tapped on the SemCor corpus and aligned bitexts with English as its source language
Cross-lingual WSD evaluation task is also focused on WSD across 2 or more languages simultaneously. Unlike the Multilingual WSD tasks, instead of providing manually sense-annotated examples for each sense of a polysemous noun, the sense inventory is built up on the basis of parallel corpora, e.g. Europarl corpus.
Multilingual WSD evaluation tasks focused on WSD across 2 or more languages simultaneously, using their respective WordNets as its sense inventories or BabelNet as multilingual sense inventory. It evolved from the Translation WSD evaluation tasks that took place in Senseval-2. A popular approach is to carry out monolingual WSD and then map the source language senses into the corresponding target word translations.
Word Sense Induction and Disambiguation task is a combined task evaluation where the sense inventory is first induced from a fixed training set data, consisting of polysemous words and the sentence that they occurred in, then WSD is performed on a different testing data set.

Software
Babelfy, a unified state-of-the-art system for multilingual Word Sense Disambiguation and Entity Linking
BabelNet API, a Java API for knowledge-based multilingual Word Sense Disambiguation in 6 different languages using the BabelNet semantic network
WordNet::SenseRelate, a project that includes free, open source systems for word sense disambiguation and lexical sample sense disambiguation
UKB: Graph Base WSD, a collection of programs for performing graph-based Word Sense Disambiguation and lexical similarity/relatedness using a pre-existing Lexical Knowledge Base
pyWSD, python implementations of Word Sense Disambiguation (WSD) technologies

See also
Ambiguity
Controlled natural language
Entity linking
Lesk algorithm
Lexical substitution
Part-of-speech tagging
Polysemy
Semeval
Semantic unification
Judicial interpretation
Sentence boundary disambiguation
Syntactic ambiguity
Word sense
Word sense induction

Notes
Works cited
Agirre, E.; Lopez de Lacalle, A.; Soroa, A. (2009). "Knowledge-based WSD on Specific Domains: Performing better than Generic Supervised WSD" (PDF). Proc. of IJCAI.CS1 maint: ref=harv (link)
Agirre, E.; M. Stevenson. 2006. Knowledge sources for WSD. In Word Sense Disambiguation: Algorithms and Applications, E. Agirre and P. Edmonds, Eds. Springer, New York, NY.
Bar-Hillel, Y. (1964). Language and information. Reading, MA: Addison-Wesley.CS1 maint: ref=harv (link)
Buitelaar, P.; B. Magnini, C. Strapparava and P. Vossen. 2006. Domain-specific WSD. In Word Sense Disambiguation: Algorithms and Applications, E. Agirre and P. Edmonds, Eds. Springer, New York, NY.
Chan, Y. S.; H. T. Ng. 2005. Scaling up word sense disambiguation via parallel texts. In Proceedings of the 20th National Conference on Artificial Intelligence (AAAI, Pittsburgh, PA).
Edmonds, P. 2000. Designing a task for SENSEVAL-2. Tech. note. University of Brighton, Brighton. U.K.
Fellbaum, Christiane (1997). "Analysis of a handwriting task". Proc. of ANLP-97 Workshop on Tagging Text with Lexical Semantics: Why, What, and How? Washington D.C., USA.CS1 maint: ref=harv (link)
Gliozzo, A.; B. Magnini and C. Strapparava. 2004. Unsupervised domain relevance estimation for word sense disambiguation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP, Barcelona, Spain).
Ide, N.; T. Erjavec, D. Tufis. 2002. Sense discrimination with parallel corpora. In Proceedings of ACL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions (Philadelphia, PA).
Kilgarriff, A. 1997. I don't believe in word senses. Comput. Human. 31(2), pp. 91–113.
Kilgarriff, A.; G. Grefenstette. 2003. Introduction to the special issue on the Web as corpus. Computational Linguistics 29(3), pp. 333–347
Kilgarriff, Adam; Joseph Rosenzweig, English Senseval: Report and Results May–June, 2000, University of Brighton
Lapata, M.; and F. Keller. 2007. An information retrieval approach to sense ranking. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL, Rochester, NY).
Lenat, D. "Computers versus Common Sense". Retrieved 2008-12-10. (GoogleTachTalks on YouTube)
Lenat, D.; R. V. Guha. 1989. Building Large Knowledge-Based Systems, Addison-Wesley
Lesk; M. 1986. Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone. In Proc. of SIGDOC-86: 5th International Conference on Systems Documentation, Toronto, Canada.
Litkowski, K. C. 2005. Computational lexicons and dictionaries. In Encyclopaedia of Language and Linguistics (2nd ed.), K. R. Brown, Ed. Elsevier Publishers, Oxford, U.K.
Magnini, B; G. Cavaglià. 2000. Integrating subject field codes into WordNet. In Proceedings of the 2nd Conference on Language Resources and Evaluation (LREC, Athens, Greece).
McCarthy, D.; R. Koeling, J. Weeds, J. Carroll. 2007. Unsupervised acquisition of predominant word senses. Computational Linguistics 33(4): 553–590.
McCarthy, D.; R. Navigli. 2009. The English Lexical Substitution Task, Language Resources and Evaluation, 43(2), Springer.
Mihalcea, R. 2007. Using Wikipedia for Automatic Word Sense Disambiguation. In Proc. of the North American Chapter of the Association for Computational Linguistics (NAACL 2007), Rochester, April 2007.
Mohammad, S; G. Hirst. 2006. Determining word sense dominance using a thesaurus. In Proceedings of the 11th Conference on European chapter of the Association for Computational Linguistics (EACL, Trento, Italy).
Navigli, R. 2006. Meaningful Clustering of Senses Helps Boost Word Sense Disambiguation Performance. Proc. of the 44th Annual Meeting of the Association for Computational Linguistics joint with the 21st International Conference on Computational Linguistics (COLING-ACL 2006), Sydney, Australia.
Navigli, R.; A. Di Marco. Clustering and Diversifying Web Search Results with Graph-Based Word Sense Induction. Computational Linguistics, 39(3), MIT Press, 2013, pp. 709–754.
Navigli, R.; G. Crisafulli. Inducing Word Senses to Improve Web Search Result Clustering. Proc. of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP 2010), MIT Stata Center, Massachusetts, USA.
Navigli, R.; M. Lapata. An Experimental Study of Graph Connectivity for Unsupervised Word Sense Disambiguation. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 32(4), IEEE Press, 2010.
Navigli, R.; K. Litkowski, O. Hargraves. 2007. SemEval-2007 Task 07: Coarse-Grained English All-Words Task. Proc. of Semeval-2007 Workshop (SemEval), in the 45th Annual Meeting of the Association for Computational Linguistics (ACL 2007), Prague, Czech Republic.
Navigli, R.;P. Velardi. 2005. Structural Semantic Interconnections: a Knowledge-Based Approach to Word Sense Disambiguation. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 27(7).
Palmer, M.; O. Babko-Malaya and H. T. Dang. 2004. Different sense granularities for different applications. In Proceedings of the 2nd Workshop on Scalable Natural Language Understanding Systems in HLT/NAACL (Boston, MA).
Ponzetto, S. P.; R. Navigli. Knowledge-rich Word Sense Disambiguation rivaling supervised systems. In Proc. of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), 2010.
Pradhan, S.; E. Loper, D. Dligach, M. Palmer. 2007. SemEval-2007 Task 17: English lexical sample, SRL and all words. Proc. of Semeval-2007 Workshop (SEMEVAL), in the 45th Annual Meeting of the Association for Computational Linguistics (ACL 2007), Prague, Czech Republic.
Schütze, H. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1): 97–123.
Snow, R.; S. Prakash, D. Jurafsky, A. Y. Ng. 2007. Learning to Merge Word Senses, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).
Snyder, B.; M. Palmer. 2004. The English all-words task. In Proc. of the 3rd International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (Senseval-3), Barcelona, Spain.
Weaver, Warren (1949). "Translation" (PDF).  In Locke, W.N.; Booth, A.D. (eds.). Machine Translation of Languages: Fourteen Essays. Cambridge, MA: MIT Press.CS1 maint: ref=harv (link)
Wilks, Y.; B. Slator, L. Guthrie. 1996. Electric Words: dictionaries, computers and meanings. Cambridge, MA: MIT Press.
Yarowsky, D. Word-sense disambiguation using statistical models of Roget's categories trained on large corpora. In Proc. of the 14th conference on Computational linguistics (COLING), 1992.
Yarowsky, D. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proc. of the 33rd Annual Meeting of the Association for Computational Linguistics.

External links and suggested reading
Computational Linguistics Special Issue on Word Sense Disambiguation (1998)
Evaluation Exercises for Word Sense Disambiguation The de facto standard benchmarks for WSD systems.
Roberto Navigli. Word Sense Disambiguation: A Survey, ACM Computing Surveys, 41(2), 2009, pp. 1–69. An up-to-date state of the art of the field.
Word Sense Disambiguation as defined in Scholarpedia
Word Sense Disambiguation: The State of the Art (PDF) A comprehensive overview By Prof. Nancy Ide & Jean Véronis (1998).
Word Sense Disambiguation Tutorial, by Rada Mihalcea and Ted Pedersen (2005).
Well, well, well ... Word Sense Disambiguation with Google n-Grams, by Craig Trim (2013).
Word Sense Disambiguation: Algorithms and Applications, edited by Eneko Agirre and Philip Edmonds (2006), Springer. Covers the entire field with chapters contributed by leading researchers. www.wsdbook.org site of the book
Bar-Hillel, Yehoshua. 1964. Language and Information. New York: Addison-Wesley.
Edmonds, Philip & Adam Kilgarriff. 2002. Introduction to the special issue on evaluating word sense disambiguation systems. Journal of Natural Language Engineering, 8(4):279-291.
Edmonds, Philip. 2005. Lexical disambiguation. The Elsevier Encyclopedia of Language and Linguistics, 2nd Ed., ed. by Keith Brown, 607-23. Oxford: Elsevier.
Ide, Nancy & Jean Véronis. 1998. Word sense disambiguation: The state of the art. Computational Linguistics, 24(1):1-40.
Jurafsky, Daniel & James H. Martin. 2000. Speech and Language Processing. New Jersey, USA: Prentice Hall.
Litkowski, K. C. 2005. Computational lexicons and dictionaries. In Encyclopaedia of Language and Linguistics (2nd ed.), K. R. Brown, Ed. Elsevier Publishers, Oxford, U.K., 753–761.
Manning, Christopher D. & Hinrich Schütze. 1999. Foundations of Statistical Natural Language Processing. Cambridge, MA: MIT Press. http://nlp.stanford.edu/fsnlp/
Mihalcea, Rada. 2007. Word sense disambiguation. Encyclopedia of Machine Learning. Springer-Verlag.
Resnik, Philip and David Yarowsky. 2000. Distinguishing systems and distinguishing senses: New evaluation methods for word sense disambiguation, Natural Language Engineering, 5(2):113-133. http://www.cs.jhu.edu/~yarowsky/pubs/nle00.ps
Yarowsky, David. 2001. Word sense disambiguation. Handbook of Natural Language Processing, ed. by Dale et al., 629-654. New York: Marcel Dekker.
In computational linguistics, word-sense induction (WSI) or discrimination is an open problem of natural language processing, which concerns the automatic identification of the senses of a word (i.e. meanings). Given that the output of word-sense induction is a set of senses for the target word (sense inventory), this task is strictly related to that of word-sense disambiguation (WSD), which relies on a predefined sense inventory and aims to solve the ambiguity of words in context.

Approaches and methods
The output of a word-sense induction algorithm is a clustering of contexts in which the target word occurs or a clustering of words related to the target word. Three main methods have been proposed in the literature:
Context clustering
Word clustering
Co-occurrence graphs

Context clustering
The underlying hypothesis of this approach is that, words are semantically similar if they appear in similar documents, with in similar context windows, or in similar syntactic contexts. Each occurrence of a target word in a corpus is represented as a context vector. These context vectors can be either first-order vectors, which directly represent the context at hand, or second-order vectors, i.e., the contexts of the target word are similar if their words tend to co-occur together. The vectors are then clustered into groups, each identifying a sense of the target word. A well-known approach to context clustering is the Context-group Discrimination algorithm  based on large matrix computation methods.

Word clustering
Word clustering is a different approach to the induction of word senses. It consists of clustering words, which are semantically similar and can thus bear a specific meaning. Lin’s algorithm  is a prototypical example of word clustering, which is based on syntactic dependency statistics, which occur in a corpus to produce sets of words for each discovered sense of a target word. The Clustering By Committee (CBC)  also uses syntactic contexts, but exploits a similarity matrix to encode the similarities between words and relies on the notion of committees to output different senses of the word of interest. These approaches are hard to obtain on a large scale for many domain and languages.

Co-occurrence graphs
The main hypothesis of co-occurrence graphs assumes that the semantics of a word can be represented by means of a co-occurrence graph, whose vertices are co-occurrences and edges are co-occurrence relations. These approaches are related to word clustering methods, where co-occurrences between words can be obtained on the basis of grammatical  or collocational relations. HyperLex is the successful approaches of a graph algorithm, based on the identification of hubs in co-occurrence graphs, which have to cope with the need to tune a large number of parameters. To deal with this issue several graph-based algorithms have been proposed, which are based on simple graph patterns, namely Curvature Clustering, Squares, Triangles and Diamonds (SquaT++), and Balanced Maximum Spanning Tree Clustering (B-MST). The patterns aim at identifying meanings using the local structural properties of the co-occurrence graph. A randomized algorithm which partitions the graph vertices by iteratively transferring the mainstream message (i.e. word sense) to neighboring vertices is Chinese Whispers. By applying co-occurrence graphs approaches have been shown to achieve the state-of-the-art performance in standard evaluation tasks.

Applications
Word-sense induction has been shown to benefit Web Information Retrieval when highly ambiguous queries are employed.
Simple word-sense induction algorithms boost Web search result clustering considerably and improve the diversification of search results returned by search engines such as Yahoo!
Word-sense induction has been applied to enrich lexical resources such as WordNet.

Software
SenseClusters is a freely available open source software package that performs both context clustering and word clustering.

See also
Word Sense Disambiguation
Grammar induction
Polysemy


== References ==