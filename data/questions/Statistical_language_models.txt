[Question Start]What is the intuition behind the smoothing techniques in statistical language modeling, and how do they help with the sparsity issue of n-gram models?[Question End]

[Question Start]How does the use of caching models impact speech recognition tasks as opposed to text-based applications? [Question End]

[Question Start]How can N-gram probabilities be combined through interpolation, and why might simple linear interpolation not always provide the best results in practice?[Question End]

[Question Start]What is the primary purpose of smoothing techniques in statistical language models? 
A) To improve the probability assigned to unseen n-grams during evaluation on test sets
B) To reduce the complexity of the model for more efficient processing
C) To increase the likelihood that all words are part of the model's vocabulary
D) To specifically weight more recently used words as more likely to appear next in a text[IDE][Question End]

[Question Start]Question:In the lecture content, a bigram model is built for text from the movie *Spider-Man Homecoming*. The `bigram_probability` method in the BigramModel class calculates the probability of one word given another. Can you explain, step-by-step, how the probabilities are calculated and what data structures are used to enable quick access to these probabilities?[Question End]