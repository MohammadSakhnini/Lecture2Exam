
    1. In the context of simple text classifiers, how can manual feature engineering be used for sentiment analysis? Provide at least three examples of features that could be extracted from a review to help determine its sentiment.
    Answers:
         a) Counting positive emoticons and negative emoticons in a given document
         b) Using the length of the review as a feature
         c) Detecting negations in the text
         d) All of the above
    2. In simple feedforward neural language models, what is used to represent the meaning of words within the model?
    Answers:
         a) One-hot encoded vectors
         b) Word embeddings
         c) TF-IDF representations
         d) Context-dependent word representations
    3. Using the illustration provided, identify and label each component in the Neural Language Model architecture including input, hidden layers, output layer, embedding layer, projection layer, and mathematical notation for conditional probability.
    Answers:
         a) Input layer: The text data is fed into this layer as an input.
         b) Embedding layer: This layer converts words into dense vectors, which are used to represent the semantics of words in the model.
         c) Hidden layers: These are the layers where complex patterns are detected and learned from the text data.
         d) Projection layer: It transforms the word embeddings into a space suitable for making predictions or continuations of the input text.
         e) Output layer: This layer generates the predictions or continuations of the input text based on the learned patterns from the hidden layers.
         f) Mathematical notations like p(z|...), p(for|...), and VÃ—1 indicate the probabilistic nature of these models and their focus on statistical prediction.
    4. How can a neural language model use similarity between word embeddings to generalize and predict the next word in a sequence? Provide an example with the help of two words, "cat" and "dog".
    Answers:
         The neural language model can use the similarity between the "cat" and "dog" word embeddings to predict the word "fed" when presented with the test data "I forgot to make sure that the dog gets ___". Since "cat" and "dog" are semantically related, their embeddings would be close in the embedding space. The model can leverage this information to generalize and correctly predict the next word as "fed", even though it has not seen that specific context before during training.
    5. In the context of language modeling, why do feedforward neural language models outperform N-gram language models? Provide one reason and explain how it leads to better performance.
    Answers:
         The primary reason is that neural language models can capture complex statistical dependencies and patterns present in natural language data, whereas N-gram language models are limited to considering only local context (i.e., the preceding n-1 words). By using more sophisticated techniques like word embeddings and neural network architectures, feedforward neural language models can learn abstract relationships between words and generate coherent text or respond appropriately in natural language processing tasks.