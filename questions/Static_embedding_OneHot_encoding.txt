[Question Start]What is the main purpose of converting words into vector representations in natural language processing, and why can't machine learning models work directly with text data? [Question End]

[Question Start]Why might one-hot encoding not always be an ideal representation for word embeddings, and what are some alternative methods to capture semantic meaning in vectors?[Question End]

[Question Start]In the context of NLP, why is it important for a model to understand the similarity between words, and how does representing words as vectors help with this task?[Question End]

[Question Start]What is the primary purpose of one-hot encoding in NLP?
A) To reduce dimensionality of word embeddings
B) To convert categorical data into a usable format for machine learning algorithms
C) To quantify the semantic similarity between words in vector space
D) To create a sparse representation of text documents for improved computational efficiency[Question End]

[Question Start]**Question:**
In the `TfIdfModel` class, the method `build_index` is implemented to create a dictionary that maps each word type in the documents to an index. The same functionality was already implemented in the `OneHotModel` class and can be reused there. Should you copy that implementation over or write your own? What are the trade-offs between these two decisions, considering the need for maintaining the code DRY while also avoiding unnecessary complexity due to shared code with a different model?[Question End]