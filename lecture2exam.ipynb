{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T10:44:38.216630Z",
     "start_time": "2024-06-19T10:44:35.520184Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset import Lecture, Dataset, QuestionsGenerator\n",
    "from dotenv import load_dotenv\n",
    "from eval import Evaluator\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T10:46:20.395332Z",
     "start_time": "2024-06-19T10:46:20.376315Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assignemnts that depend on lecture content\n",
    "dependencies_map = {\n",
    "    \"1_BytePairEncoding.py\": [1],  # exercise 1 depends on lecture 1\n",
    "    \"2_N-Grams.ip.py\": [2],  # exercise 2 depends on lecture 2\n",
    "    \"3_SimpleEmbeddings.py\": [3, 4],  # exercise 3 depends on lecture 3 and 4\n",
    "    \"4_VectorSimilarity.py\": [5, 6],  # exercise 4 depends on lecture 5 and 6\n",
    "    \"5_Neural_Language_Model.py\": [7],  # exercise 5 depends on lecture 7\n",
    "    \"6_Keywords.py\": []  # 6 is a standalone exercise\n",
    "}\n",
    "if os.path.isfile(\"data/store/dataset.pkl\") is False:\n",
    "    dataset = Dataset()\n",
    "    dataset.create_dataset(dependencies_map)\n",
    "if os.path.isfile(\"data/store/questions.pkl\") is False:\n",
    "    questions_generator = QuestionsGenerator()\n",
    "    questions_generator.generate_questions(num_questions=[5], mc_questions=[1], code_questions=[1])\n",
    "\n",
    "    # data will be stored as pickle under data/store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.openai.com/v1 sk-proj-4Qvu1seP7PsGqtxoVEZUT3BlbkFJZS4aN6WSulBOYBX6BqdA gpt-3.5-turbo\n"
     ]
    }
   ],
   "source": [
    "questions_generator = QuestionsGenerator(\n",
    "    base_url=os.getenv(\"OPEANI_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=os.getenv(\"OPENAI_MODEL\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer_decoder_and_Large_Langauge_Models\n",
      "Questions for lecture 3 - Transformer_decoder_and_Large_Langauge_Models\n",
      "[Question Start]Explain the significance of the Transformer Decoder in the context of Large Language Models within Natural Language Processing based on the provided lecture content.[Question End]\n",
      "\n",
      "[Question Start]How does the training process involving human feedback contribute to improving the performance of language models like GPT-3 in tasks such as text summarization and question-answering?[Question End]\n",
      "\n",
      "[Question Start]Discuss the challenges associated with training language models to follow instructions with human feedback, as outlined in the lecture content. How can these challenges be addressed to enhance the model's alignment with user expectations?[Question End]\n",
      "\n",
      "[Question Start]Describe the process of learning to summarize from human feedback as presented in the lecture content. How does this approach contribute to enhancing the summarization capabilities of language models like GPT-2 or GPT-3?[Question End]\n",
      "\n",
      "[Question Start]### QUESTION\n",
      "\n",
      "What is the primary purpose of training language models with human feedback according to the lecture content?\n",
      "\n",
      "- A. To make the models align with their users' preferences\n",
      "- B. To reduce the cost of manual data labeling\n",
      "- C. To increase the complexity of large prompt datasets\n",
      "- D. To automate the process of fine-tuning language models[Question End]\n",
      "16704\n",
      "\n",
      "\n",
      "\n",
      "Statistical_language_models\n",
      "Questions for lecture 6 - Statistical_language_models\n",
      "[Question Start]Explain the concept of linear interpolation in the context of statistical language models. How does linear interpolation combine different N-gram models, and how are the lambda values determined for this process?[Question End]\n",
      "\n",
      "[Question Start]Discuss the techniques used for handling unknown words in statistical language models. Differentiate between open and closed vocabulary tasks and explain the significance of creating an unknown word token <UNK> in language modeling.[Question End]\n",
      "\n",
      "[Question Start]Elaborate on the concept of smoothing for web-scale N-grams in statistical language models. Describe the \"Stupid backoff\" technique introduced by Brants et al. in 2007 and how it addresses the issue of relative frequencies in large N-gram datasets.[Question End]\n",
      "\n",
      "[Question Start]### QUESTION\n",
      "\n",
      "What is the primary purpose of linear interpolation in language modeling?\n",
      "\n",
      "- To combine different order N-grams by linearly interpolating all the models\n",
      "- To remove singletons of higher-order n-grams for efficiency\n",
      "- To prune N-grams with count below a threshold\n",
      "- To store words as indexes for improved accuracy[Question End]\n",
      "\n",
      "[Question Start]```python\n",
      "import numpy as np\n",
      "from typing import List\n",
      "from collections import Counter\n",
      "from itertools import islice\n",
      "from nltk.corpus import gutenberg\n",
      "\n",
      "class BigramModel:\n",
      "    \n",
      "    def __init__(self, sentences: List[List[str]]):\n",
      "        '''\n",
      "        Takes in a list of sentences, where each sentence is a \n",
      "        list of words.\n",
      "        \n",
      "        Arguments:\n",
      "            sentences -- List of lists of words (e.g. [['I', 'have', 'a', 'dog'],\n",
      "                                                      ['a', 'dog', 'I', 'have']])\n",
      "        '''\n",
      "        # YOUR CODE HERE\n",
      "        raise NotImplementedError()\n",
      "        \n",
      "    def window(self, seq, n=2):\n",
      "        '''\n",
      "        Returns a sliding window (of width n) over data from the iterable\n",
      "        \n",
      "        Arguments:\n",
      "            seq      -- the iterable (e.g. list, set, etc) to run the window over\n",
      "            n        -- the size of the window\n",
      "        Returns:\n",
      "            iterator -- an iterator over the sliding windows\n",
      "            \n",
      "        Usage:\n",
      "            my_list = [1, 2, 3, 4]\n",
      "            for slice in self.window(my_list):\n",
      "                print(slice)\n",
      "                \n",
      "            # Output: (1, 2)\n",
      "                      (2, 3)\n",
      "                      (3, 4)\n",
      "        '''\n",
      "        \"Returns a sliding window (of width n) over data from the iterable\"\n",
      "        \"   s -> (s0,s1,...s[n-1]), (s1,s2,...,sn), ...                   \"\n",
      "        it = iter(seq)\n",
      "        result = tuple(islice(it, n))\n",
      "        if len(result) == n:\n",
      "            yield result\n",
      "        for elem in it:\n",
      "            result = result[1:] + (elem,)\n",
      "            yield result\n",
      "            \n",
      "    def unigram_count(self, word: str) -> int:\n",
      "        '''\n",
      "        Returns the unigram count for the word.\n",
      "        If the word does not exist in our corpus, return 0.\n",
      "        \n",
      "        Arguments:\n",
      "            word  -- word we want to know the count of\n",
      "        Returns:\n",
      "            count -- how often the word appears in the corpus\n",
      "        '''\n",
      "        # YOUR CODE HERE\n",
      "        raise NotImplementedError()\n",
      "        \n",
      "    def unigram_probability(self, word:str) -> float:\n",
      "        '''\n",
      "        Returns the unigram probability for the word.\n",
      "        If the word does not exist in our corpus, return 0.\n",
      "        \n",
      "        Arguments:\n",
      "            word        -- word we want to know the probability of\n",
      "        Returns:\n",
      "            probability -- how likely it is to choose the word at random\n",
      "        '''\n",
      "        # YOUR CODE HERE\n",
      "        raise NotImplementedError()\n",
      "    \n",
      "    def bigram_count(self, word1:str, word2:str) -> int:\n",
      "        '''\n",
      "        Returns the bigram count for the word1 followed by word2.\n",
      "        If either of the words does not exist in our corpus, return 0.\n",
      "        \n",
      "        Arguments:\n",
      "            word1  -- first word of the bigram\n",
      "            word2  -- second word of the bigram\n",
      "        Returns:\n",
      "            count  -- how often the bigram appears in the corpus\n",
      "        '''\n",
      "        # YOUR CODE HERE\n",
      "        raise NotImplementedError()\n",
      "        \n",
      "    def bigram_probability(self, word1:str, word2:str) -> float:\n",
      "        '''\n",
      "        Returns the bigram probability for the word1 followed by word2.\n",
      "        This is the conditional probability P(word2 | word1).\n",
      "        If either of the words does not exist in our corpus, return 0.\n",
      "        \n",
      "        Arguments:\n",
      "            word1       -- first word of the bigram\n",
      "            word2       -- second word of the bigram\n",
      "        Returns:\n",
      "            probability -- how likely it is to choose the word at random\n",
      "        '''\n",
      "        # YOUR CODE HERE\n",
      "        raise NotImplementedError()\n",
      "    \n",
      "    def sentence_probability(self, sentence:List[str]) -> float:\n",
      "        '''\n",
      "        Return the probability for the given sentence based on our\n",
      "        bigram probabilities\n",
      "        \n",
      "        Arguments:\n",
      "            sentence    -- list of tokens from the sentence \n",
      "                           (e.g. ['<s>', 'I', 'have', 'a', 'dog', '</s>'])\n",
      "        Returns:\n",
      "            probability -- probability of the sentence\n",
      "        '''\n",
      "        # YOUR CODE HERE\n",
      "        raise NotImplementedError()\n",
      "```\n",
      "\n",
      "**Question:** In the provided `BigramModel` class, what does the `window` method do and what is its purpose in the context of building a bigram language model? \n",
      "\n",
      "**Note:** You are not required to provide the actual implementation of the method, just explain its functionality and significance based on the context provided in the lecture content.[Question End]\n",
      "48603\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<dataset.Questions at 0x1e8c13ac970>, <dataset.Questions at 0x1e8c13af6d0>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_generator.generate_questions(num_questions=[5], mc_questions=[1], code_questions=[1], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(\n",
    "    \"./data/store/dataset.pkl\",\n",
    "    \"./data/store/questions.pkl\",\n",
    "    base_url=os.getenv(\"OPEANI_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=os.getenv(\"OPENAI_MODEL\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating lecture 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating question 0\n",
      "Prompting eval model | retries: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result parsed with length 4\n",
      "\n",
      "Evaluating question 1\n",
      "Prompting eval model | retries: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result parsed with length 4\n",
      "\n",
      "Evaluating question 2\n",
      "Prompting eval model | retries: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result parsed with length 4\n",
      "\n",
      "Evaluating question 3\n",
      "Prompting eval model | retries: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result parsed with length 4\n",
      "\n",
      "Evaluating question 4\n",
      "Prompting eval model | retries: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:32,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result parsed with length 4\n",
      "\n",
      "Evaluating all questions of lecture 10\n",
      "Prompting eval model | retries: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 1/2 [00:37<00:37, 37.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result parsed with length 4\n",
      "51916\n",
      "\n",
      "Evaluating lecture 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating question 0\n",
      "Prompting eval model | retries: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result parsed with length 4\n",
      "\n",
      "Evaluating question 1\n",
      "Prompting eval model | retries: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result parsed with length 4\n",
      "\n",
      "Evaluating question 2\n",
      "Prompting eval model | retries: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result parsed with length 4\n",
      "\n",
      "Evaluating question 3\n",
      "Prompting eval model | retries: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result parsed with length 4\n",
      "\n",
      "Evaluating question 4\n",
      "Prompting eval model | retries: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:29,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result parsed with length 4\n",
      "\n",
      "Evaluating all questions of lecture 2\n",
      "Prompting eval model | retries: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2/2 [01:10<00:00, 35.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result parsed with length 4\n",
      "135373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluator.evaluate_lectures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if os.path.isfile(\"data/store/evaluations.pkl\") is False:\n",
    "    questions = evaluator.get_questions()\n",
    "    with open(\"data/store/evaluations.pkl\", \"wb\") as f:\n",
    "        pickle.dump(evaluator.get_questions(), f)\n",
    "else:       \n",
    "    questions = pickle.load(open(\"data/store/evaluations.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Explain the significance of the Transformer Decoder in the context of Large Language Models within Natural Language Processing based on the provided lecture content.\n",
      "LLM Reasoning: [' The Transformer Decoder plays a crucial role in the context of Large Language Models within Natural Language Processing (NLP). It is responsible for generating text by predicting the next token in a sequence, allowing the model to understand and respond to input data effectively. In the lecture content provided, the Transformer Decoder is highlighted as a key component in models like the Generative Pre-trained Transformer (GPT), enabling tasks such as translation, summarization, and text generation. The lecture emphasizes the importance of understanding the architecture and workings of the Transformer Decoder, including components like attention mechanisms, positional encoding, and output probabilities. This knowledge is essential for students to comprehend how large language models process information and produce coherent and contextually relevant text outputs.']\n",
      "LLM Relevance Eval: ['8']\n",
      "LLM Difficulty Eval: ['6']\n",
      "LLM Answer: ['The Transformer Decoder holds significant importance in Large Language Models within Natural Language Processing (NLP) as it is responsible for generating text by predicting the next token in a sequence. This component plays a crucial role in tasks such as translation, summarization, and text generation, allowing the model to understand and respond to input data effectively. Understanding the architecture and workings of the Transformer Decoder, including attention mechanisms, positional encoding, and output probabilities, is essential for grasping how large language models process information and produce coherent and contextually relevant text outputs.']\n",
      "---Question Eval End---\n",
      "\n",
      "Question: How does the training process involving human feedback contribute to improving the performance of language models like GPT-3 in tasks such as text summarization and question-answering?\n",
      "LLM Reasoning: ['The question pertains to the training process involving human feedback and its impact on enhancing the performance of language models like GPT-3 in tasks such as text summarization and question-answering. This aligns with the content discussed in the lecture regarding the importance of incorporating human feedback to train and improve large language models, which was covered in the sections focusing on training language models with human feedback and learning to summarize from human feedback.']\n",
      "LLM Relevance Eval: ['9']\n",
      "LLM Difficulty Eval: ['6']\n",
      "LLM Answer: ['The training process involving human feedback plays a crucial role in enhancing the performance of language models like GPT-3 in tasks such as text summarization and question-answering. By collecting human feedback on model-generated outputs, these language models can learn to produce more accurate and contextually relevant summaries and answers. This feedback loop allows the model to adjust its parameters based on the desired outputs, leading to improved performance over time. Specifically, in text summarization, the model can learn to identify key information and produce concise summaries by analyzing the feedback on the generated summaries. Similarly, in question-answering tasks, the model can refine its responses by understanding and incorporating human feedback on the correctness and relevance of the answers provided.']\n",
      "---Question Eval End---\n",
      "\n",
      "Question: Discuss the challenges associated with training language models to follow instructions with human feedback, as outlined in the lecture content. How can these challenges be addressed to enhance the model's alignment with user expectations?\n",
      "LLM Reasoning: ['The question pertains to the challenges of training language models to follow instructions with human feedback, which is a crucial aspect discussed in the lecture content about large language models and transformer decoders. The challenges involve the costly and manual process of collecting labeled data for training, as well as the absence of good large prompt datasets. Addressing these challenges is essential to ensure that the models align with user expectations and produce desired outputs.']\n",
      "LLM Relevance Eval: ['10']\n",
      "LLM Difficulty Eval: ['7']\n",
      "LLM Answer: [\"The challenges associated with training language models to follow instructions with human feedback include the following:\\n1. Expensive and Manual Process: Collecting and labeling data for training language models with human feedback can be a costly and time-consuming process. Hiring contractors or experts to provide accurate annotations and feedback requires significant resources.\\n2. Lack of Good Large Prompt Datasets: Obtaining high-quality and extensive prompt datasets that cover a wide range of scenarios and contexts can be challenging. Without diverse and representative data, the model may struggle to generalize and respond effectively to various inputs.\\n\\nTo address these challenges and enhance the model's alignment with user expectations, several strategies can be implemented:\\n1. Automated Data Labeling: Utilizing automated tools and techniques for data labeling can help reduce the manual effort required to collect and annotate training data. Machine learning algorithms can assist in labeling data more efficiently, saving time and resources.\\n2. Data Augmentation: Generating synthetic data or expanding existing datasets through techniques like data augmentation can help increase the diversity and size of prompt datasets. This can improve the model's ability to handle different input scenarios and improve generalization.\\n3. Active Learning: Implementing active learning strategies can optimize the process of selecting data samples for human feedback, focusing on the most informative instances that can enhance the model's performance. This targeted approach can improve the efficiency of data collection and annotation.\\n4. Transfer Learning: Leveraging pre-trained models and transfer learning techniques can help reduce the amount of labeled data required for fine-tuning models with human feedback. By starting from a pre-trained model that has learned general patterns, the model can adapt faster to specific tasks and feedback.\\n\\nBy addressing these challenges through a combination of automated tools, data augmentation, active learning, and transfer learning, language models can be better trained to follow instructions with human feedback and align more closely with user expectations.\"]\n",
      "---Question Eval End---\n",
      "\n",
      "Question: Describe the process of learning to summarize from human feedback as presented in the lecture content. How does this approach contribute to enhancing the summarization capabilities of language models like GPT-2 or GPT-3?\n",
      "LLM Reasoning: ['The question is directly related to the lecture content on learning to summarize from human feedback, as discussed in the section focusing on transformer decoders and large language models. The question asks for a description of the process of learning to summarize from human feedback and how this approach contributes to enhancing the summarization capabilities of language models like GPT-2 or GPT-3.']\n",
      "LLM Relevance Eval: ['10']\n",
      "LLM Difficulty Eval: ['7']\n",
      "LLM Answer: ['The process of learning to summarize from human feedback involves collecting feedback from human judges on summaries generated by a language model, training a reward model on this feedback, and using the reward model to improve the performance of the language model. This approach contributes to enhancing the summarization capabilities of models like GPT-2 or GPT-3 by iteratively training the model to produce summaries that align with human preferences and quality standards. By incorporating human feedback, the model learns to generate more accurate, coherent, and contextually relevant summaries, leading to improved performance and user satisfaction in tasks requiring summarization.']\n",
      "---Question Eval End---\n",
      "\n",
      "Question: ### QUESTION\n",
      "\n",
      "\n",
      "\n",
      "What is the primary purpose of training language models with human feedback according to the lecture content?\n",
      "\n",
      "\n",
      "\n",
      "- A. To make the models align with their users' preferences\n",
      "\n",
      "- B. To reduce the cost of manual data labeling\n",
      "\n",
      "- C. To increase the complexity of large prompt datasets\n",
      "\n",
      "- D. To automate the process of fine-tuning language models\n",
      "LLM Reasoning: [\"The primary purpose of training language models with human feedback, as discussed in the lecture content, is to make the models align with their users' preferences. Training language models with human feedback helps ensure that the models produce content that is helpful, non-toxic, and aligned with what is considered desirable by users and society. This aligns the model's outputs with user expectations and requirements, improving the quality and relevance of the generated text.\"]\n",
      "LLM Relevance Eval: ['9']\n",
      "LLM Difficulty Eval: ['4']\n",
      "LLM Answer: [\"A. To make the models align with their users' preferences\"]\n",
      "---Question Eval End---\n",
      "\n",
      "Question: Explain the concept of linear interpolation in the context of statistical language models. How does linear interpolation combine different N-gram models, and how are the lambda values determined for this process?\n",
      "LLM Reasoning: [\"The question is based on the section of the lecture that covers linear interpolation in statistical language models. It specifically addresses how linear interpolation combines various N-gram models to improve the overall model's performance. The question also inquires about how lambda values are determined for this process, which is a key aspect of understanding how different models are weighted and combined.\"]\n",
      "LLM Relevance Eval: ['8']\n",
      "LLM Difficulty Eval: ['7']\n",
      "LLM Answer: ['Linear interpolation in statistical language models is a technique used to combine different N-gram models to create a more accurate and robust language model. In linear interpolation, various N-gram models, such as unigrams, bigrams, and trigrams, are weighted and combined by linearly interpolating their probabilities. \\n\\nThe formula for estimating the probability of a word given its context in linear interpolation is:\\n\\\\[ P(w_n|w_{n-2}w_{n-1}) = \\\\lambda_1 P(w_n|w_{n-2}w_{n-1}) + \\\\lambda_2 P(w_n|w_{n-1}) + \\\\lambda_3 P(w_n) \\\\]\\n\\nThe lambda values (λ) represent the weights assigned to each N-gram model, and these weights sum up to 1. The lambda values are determined by using a held-out corpus. The objective is to choose the lambda values that maximize the probability of the held-out data. By fixing the N-gram probabilities on the training data and searching for lambda values that yield the highest probability on the held-out set, the best combination of N-gram models is determined to improve language model performance.']\n",
      "---Question Eval End---\n",
      "\n",
      "Question: Discuss the techniques used for handling unknown words in statistical language models. Differentiate between open and closed vocabulary tasks and explain the significance of creating an unknown word token <UNK> in language modeling.\n",
      "LLM Reasoning: ['The question aligns with the content covered in the lecture on statistical language models by Dan Jurafsky, specifically focusing on techniques for handling unknown words in language models. The lecture touched upon strategies for dealing with out-of-vocabulary words, such as creating an unknown word token <UNK>, and discussed the differences between open and closed vocabulary tasks. Moreover, the significance of using <UNK> in language modeling was emphasized in the lecture.']\n",
      "LLM Relevance Eval: ['8']\n",
      "LLM Difficulty Eval: ['6']\n",
      "LLM Answer: ['In statistical language models, handling unknown words is crucial for accurate predictions. Techniques for addressing unknown words include creating a token like <UNK> to represent them. In closed vocabulary tasks where all words are known in advance, a fixed vocabulary (V) is used. In contrast, open vocabulary tasks involve handling out-of-vocabulary (OOV) words using <UNK>. During training, words not in a fixed lexicon are replaced with <UNK>, and their probabilities are trained like normal words. At decoding time, UNK probabilities are used for any word not seen in training data. This approach ensures that the model can generalize to new words not encountered during training, improving its performance on unseen data.']\n",
      "---Question Eval End---\n",
      "\n",
      "Question: Elaborate on the concept of smoothing for web-scale N-grams in statistical language models. Describe the \"Stupid backoff\" technique introduced by Brants et al. in 2007 and how it addresses the issue of relative frequencies in large N-gram datasets.\n",
      "LLM Reasoning: ['The question is focused on the concept of smoothing for web-scale N-grams in statistical language models, which falls under the topic of N-gram Smoothing as discussed in the lecture content. Specifically, it touches upon the \"Stupid backoff\" technique introduced by Brants et al. in 2007, which is crucial in dealing with relative frequencies in large N-gram datasets. This question aligns well with the content on language modeling techniques, smoothing methods, and handling large N-gram datasets covered in the lecture.']\n",
      "LLM Relevance Eval: ['10']\n",
      "LLM Difficulty Eval: ['7']\n",
      "LLM Answer: ['The \"Stupid backoff\" technique, introduced by Brants et al. in 2007, is a smoothing method used for web-scale N-grams in statistical language models. This approach aims to address the issue of relative frequencies in large N-gram datasets without discounting. In this technique, if the count of a specific n-gram is greater than 0, it uses the relative frequency of that n-gram. However, if the count is 0, it falls back to a less-specific n-gram, adjusting the probability accordingly to avoid zero probabilities and improve the robustness of the model. This method helps in maintaining probabilities for unseen or rare n-grams, contributing to more accurate language modeling in extensive datasets.']\n",
      "---Question Eval End---\n",
      "\n",
      "Question: ### QUESTION\n",
      "\n",
      "\n",
      "\n",
      "What is the primary purpose of linear interpolation in language modeling?\n",
      "\n",
      "\n",
      "\n",
      "- To combine different order N-grams by linearly interpolating all the models\n",
      "\n",
      "- To remove singletons of higher-order n-grams for efficiency\n",
      "\n",
      "- To prune N-grams with count below a threshold\n",
      "\n",
      "- To store words as indexes for improved accuracy\n",
      "LLM Reasoning: ['The question pertains to the concept of linear interpolation in language modeling, which was covered in the lecture under the topic of statistical language models. Linear interpolation is a method used to combine different order N-grams by linearly interpolating all the models. It is aimed at creating a more accurate language model by blending the probabilities from multiple n-grams. This technique helps in improving the prediction capabilities of the model by considering various levels of context when estimating the likelihood of a word given its preceding words.']\n",
      "LLM Relevance Eval: ['9']\n",
      "LLM Difficulty Eval: ['6']\n",
      "LLM Answer: ['- To combine different order N-grams by linearly interpolating all the models']\n",
      "---Question Eval End---\n",
      "\n",
      "Question: ```python\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "from typing import List\n",
      "\n",
      "from collections import Counter\n",
      "\n",
      "from itertools import islice\n",
      "\n",
      "from nltk.corpus import gutenberg\n",
      "\n",
      "\n",
      "\n",
      "class BigramModel:\n",
      "\n",
      "    \n",
      "\n",
      "    def __init__(self, sentences: List[List[str]]):\n",
      "\n",
      "        '''\n",
      "\n",
      "        Takes in a list of sentences, where each sentence is a \n",
      "\n",
      "        list of words.\n",
      "\n",
      "        \n",
      "\n",
      "        Arguments:\n",
      "\n",
      "            sentences -- List of lists of words (e.g. [['I', 'have', 'a', 'dog'],\n",
      "\n",
      "                                                      ['a', 'dog', 'I', 'have']])\n",
      "\n",
      "        '''\n",
      "\n",
      "        # YOUR CODE HERE\n",
      "\n",
      "        raise NotImplementedError()\n",
      "\n",
      "        \n",
      "\n",
      "    def window(self, seq, n=2):\n",
      "\n",
      "        '''\n",
      "\n",
      "        Returns a sliding window (of width n) over data from the iterable\n",
      "\n",
      "        \n",
      "\n",
      "        Arguments:\n",
      "\n",
      "            seq      -- the iterable (e.g. list, set, etc) to run the window over\n",
      "\n",
      "            n        -- the size of the window\n",
      "\n",
      "        Returns:\n",
      "\n",
      "            iterator -- an iterator over the sliding windows\n",
      "\n",
      "            \n",
      "\n",
      "        Usage:\n",
      "\n",
      "            my_list = [1, 2, 3, 4]\n",
      "\n",
      "            for slice in self.window(my_list):\n",
      "\n",
      "                print(slice)\n",
      "\n",
      "                \n",
      "\n",
      "            # Output: (1, 2)\n",
      "\n",
      "                      (2, 3)\n",
      "\n",
      "                      (3, 4)\n",
      "\n",
      "        '''\n",
      "\n",
      "        \"Returns a sliding window (of width n) over data from the iterable\"\n",
      "\n",
      "        \"   s -> (s0,s1,...s[n-1]), (s1,s2,...,sn), ...                   \"\n",
      "\n",
      "        it = iter(seq)\n",
      "\n",
      "        result = tuple(islice(it, n))\n",
      "\n",
      "        if len(result) == n:\n",
      "\n",
      "            yield result\n",
      "\n",
      "        for elem in it:\n",
      "\n",
      "            result = result[1:] + (elem,)\n",
      "\n",
      "            yield result\n",
      "\n",
      "            \n",
      "\n",
      "    def unigram_count(self, word: str) -> int:\n",
      "\n",
      "        '''\n",
      "\n",
      "        Returns the unigram count for the word.\n",
      "\n",
      "        If the word does not exist in our corpus, return 0.\n",
      "\n",
      "        \n",
      "\n",
      "        Arguments:\n",
      "\n",
      "            word  -- word we want to know the count of\n",
      "\n",
      "        Returns:\n",
      "\n",
      "            count -- how often the word appears in the corpus\n",
      "\n",
      "        '''\n",
      "\n",
      "        # YOUR CODE HERE\n",
      "\n",
      "        raise NotImplementedError()\n",
      "\n",
      "        \n",
      "\n",
      "    def unigram_probability(self, word:str) -> float:\n",
      "\n",
      "        '''\n",
      "\n",
      "        Returns the unigram probability for the word.\n",
      "\n",
      "        If the word does not exist in our corpus, return 0.\n",
      "\n",
      "        \n",
      "\n",
      "        Arguments:\n",
      "\n",
      "            word        -- word we want to know the probability of\n",
      "\n",
      "        Returns:\n",
      "\n",
      "            probability -- how likely it is to choose the word at random\n",
      "\n",
      "        '''\n",
      "\n",
      "        # YOUR CODE HERE\n",
      "\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    \n",
      "\n",
      "    def bigram_count(self, word1:str, word2:str) -> int:\n",
      "\n",
      "        '''\n",
      "\n",
      "        Returns the bigram count for the word1 followed by word2.\n",
      "\n",
      "        If either of the words does not exist in our corpus, return 0.\n",
      "\n",
      "        \n",
      "\n",
      "        Arguments:\n",
      "\n",
      "            word1  -- first word of the bigram\n",
      "\n",
      "            word2  -- second word of the bigram\n",
      "\n",
      "        Returns:\n",
      "\n",
      "            count  -- how often the bigram appears in the corpus\n",
      "\n",
      "        '''\n",
      "\n",
      "        # YOUR CODE HERE\n",
      "\n",
      "        raise NotImplementedError()\n",
      "\n",
      "        \n",
      "\n",
      "    def bigram_probability(self, word1:str, word2:str) -> float:\n",
      "\n",
      "        '''\n",
      "\n",
      "        Returns the bigram probability for the word1 followed by word2.\n",
      "\n",
      "        This is the conditional probability P(word2 | word1).\n",
      "\n",
      "        If either of the words does not exist in our corpus, return 0.\n",
      "\n",
      "        \n",
      "\n",
      "        Arguments:\n",
      "\n",
      "            word1       -- first word of the bigram\n",
      "\n",
      "            word2       -- second word of the bigram\n",
      "\n",
      "        Returns:\n",
      "\n",
      "            probability -- how likely it is to choose the word at random\n",
      "\n",
      "        '''\n",
      "\n",
      "        # YOUR CODE HERE\n",
      "\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    \n",
      "\n",
      "    def sentence_probability(self, sentence:List[str]) -> float:\n",
      "\n",
      "        '''\n",
      "\n",
      "        Return the probability for the given sentence based on our\n",
      "\n",
      "        bigram probabilities\n",
      "\n",
      "        \n",
      "\n",
      "        Arguments:\n",
      "\n",
      "            sentence    -- list of tokens from the sentence \n",
      "\n",
      "                           (e.g. ['<s>', 'I', 'have', 'a', 'dog', '</s>'])\n",
      "\n",
      "        Returns:\n",
      "\n",
      "            probability -- probability of the sentence\n",
      "\n",
      "        '''\n",
      "\n",
      "        # YOUR CODE HERE\n",
      "\n",
      "        raise NotImplementedError()\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "**Question:** In the provided `BigramModel` class, what does the `window` method do and what is its purpose in the context of building a bigram language model? \n",
      "\n",
      "\n",
      "\n",
      "**Note:** You are not required to provide the actual implementation of the method, just explain its functionality and significance based on the context provided in the lecture content.\n",
      "LLM Reasoning: ['The `window` method in the `BigramModel` class is designed to create a sliding window over a sequence of data, specifically over a given iterable object like a list or set. This method allows for iterating through the data in a sequence of fixed window sizes, where each window represents a subset of elements. In the context of building a bigram language model, the `window` method is crucial for extracting pairs of adjacent words in sentences. By sliding a window of size 2 over the words in a sentence, the method helps in generating all possible bigrams (pairs of consecutive words) from the input data. This is essential for calculating bigram counts and probabilities, which are key components in constructing a statistical language model based on word sequences.']\n",
      "LLM Relevance Eval: ['9']\n",
      "LLM Difficulty Eval: ['5']\n",
      "LLM Answer: ['The `window` method in the `BigramModel` class creates a sliding window over a sequence of data, allowing for the extraction of pairs of adjacent elements like words in sentences. This functionality is crucial for generating bigrams and calculating counts and probabilities in a bigram language model, aiding in the construction of statistical language models based on word sequences.']\n",
      "---Question Eval End---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lecture_index in questions:\n",
    "    for question_index in range(len(questions[lecture_index].questions)):\n",
    "        print(f\"Question: {questions[lecture_index].questions[question_index]}\")\n",
    "        print(f\"LLM Reasoning: {questions[lecture_index].evaluations[question_index].get('reasoning')}\")\n",
    "        print(f\"LLM Relevance Eval: {questions[lecture_index].evaluations[question_index].get('relevance')}\")\n",
    "        print(f\"LLM Difficulty Eval: {questions[lecture_index].evaluations[question_index].get('difficulty')}\")\n",
    "        print(f\"LLM Answer: {questions[lecture_index].evaluations[question_index].get('answer')}\")\n",
    "        print(f\"---Question Eval End---\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
